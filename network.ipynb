{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 27\n",
    "#num = 3\n",
    "\n",
    "train_model = True\n",
    "load_weights = False\n",
    "make_labels = True\n",
    "\n",
    "\n",
    "import datetime\n",
    "num_keypoints = 2\n",
    "save_path = rf'F:\\ET_Tube\\CheXpert-v1.0\\weights\\test_weights_{datetime.datetime.now().strftime(\"%H_%M_%S\")}.pth'\n",
    "save_dir =  r\"D:\\ET_Tube\\CheXpert-v1.0\\checkpoints\" #checkpoint directory\n",
    "workbook_path = rf'F:\\ET_Tube\\CheXpert-v1.0\\predictions_{datetime.datetime.now().strftime(\"%H_%M_%S\")}.xlsx'\n",
    "\n",
    "batch_size = 1\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYPOINTS_FOLDER_TRAIN = fr\"C:\\Users\\nprim\\Downloads\\ET_TUBE.v{num}i.yolov7pytorch\\train\"\n",
    "KEYPOINTS_FOLDER_VALID = fr\"C:\\Users\\nprim\\Downloads\\ET_TUBE.v{num}i.yolov7pytorch\\valid\"\n",
    "#KEYPOINTS_FOLDER_TRAIN = fr\"C:\\Users\\nprim\\Downloads\\ET_TUBE.v23i.yolov7pytorch\\train\"\n",
    "#KEYPOINTS_FOLDER_TEST =  fr\"C:\\Users\\nprim\\Downloads\\ET_TUBE.v{num}i.yolov7pytorch\\test\"\n",
    "KEYPOINTS_FOLDER_TEST =  fr\"C:\\Users\\nprim\\Downloads\\ET_TUBE.v{num}i.yolov7pytorch\\test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import json, cv2, numpy as np, matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "import torchvision\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision.models.detection import KeypointRCNN\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import transforms, utils, engine, train\n",
    "from utils import collate_fn\n",
    "from engine import train_one_epoch, evaluate\n",
    "import random\n",
    "import base64\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from IPython.display import HTML\n",
    "from torchsummary import summary\n",
    "import math\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.drawing.image import Image as ExcelImage\n",
    "from openpyxl.utils import get_column_letter\n",
    "import glob\n",
    "from natsort import natsorted\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "print(\"Using torch\", torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if make_labels:\n",
    "    import json\n",
    "    import zipfile\n",
    "    path_to_zip_file = fr\"C:\\Users\\nprim\\Downloads\\ET_TUBE.v{num}i.yolov7pytorch.zip\"\n",
    "    \n",
    "    BASE = fr\"C:\\Users\\nprim\\Downloads\\ET_TUBE.v{num}i.yolov7pytorch\"\n",
    "    with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(BASE)\n",
    "\n",
    "    SET = [\"/train\", \"/valid\", \"/test\"]\n",
    "    #SET = [\"/train\",  \"/test\"]\n",
    "    #SET = [\"/train\", \"/valid\"]\n",
    "    #SET = [\"/train\"]\n",
    "    #SET = [\"/test\"]\n",
    "  \n",
    "    keypoint_names =  ['Carina', 'ET']\n",
    "\n",
    "    def dump2json(bboxes, keypoints_sorted, file_json):\n",
    "        annotations = {}\n",
    "        annotations['bboxes'], annotations['keypoints'] = bboxes, keypoints_sorted\n",
    "\n",
    "        with open(file_json, \"w\") as f:\n",
    "            json.dump(annotations, f)\n",
    "\n",
    "    def converter(file_labels, file_image, keypoint_names):\n",
    "\n",
    "        img = cv2.imread(file_image)\n",
    "        img_w, img_h = img.shape[1], img.shape[0]\n",
    "        \n",
    "        with open(file_labels) as f:\n",
    "            lines_txt = f.readlines()\n",
    "            lines = []\n",
    "            for line in lines_txt:\n",
    "                lines.append([int(line.split()[0])] + [round(float(el), 5) for el in line.split()[1:]])\n",
    "        \n",
    "        bboxes = []\n",
    "        keypoints = []\n",
    "\n",
    "        # Convert normalized coordinates to absolute coordinates\n",
    "        for line in lines:\n",
    "            # Number 0 is a class of rectangles related to bounding boxes.\n",
    "            if line[0] == 2:\n",
    "                x_c, y_c, w, h = round(line[1] * img_w), round(line[2] * img_h), round(line[3] * img_w), round(line[4] * img_h)\n",
    "                bboxes.append([round(x_c - w/2), round(y_c - h/2), round(x_c + w/2), round(y_c + h/2)])\n",
    "\n",
    "            elif line[0] == 0 or line[0] == 1: #append all other keypoints without class change\n",
    "                kp_id, x_c, y_c = line[0], round(line[1] * img_w), round(line[2] * img_h) \n",
    "                keypoints.append([kp_id, x_c, y_c])\n",
    "                \n",
    "\n",
    "        # iterating over each keypoint and looking to which bounding box it matches, dont need this for patellas\n",
    "        keypoints_sorted = [[[] for _ in keypoint_names] for _ in bboxes]\n",
    "\n",
    "        for kp in keypoints:\n",
    "            kp_id, kp_x, kp_y = kp[0], kp[1], kp[2]\n",
    "            for bbox_idx, bbox in enumerate(bboxes):\n",
    "                x1, y1, x2, y2 = bbox[0], bbox[1], bbox[2], bbox[3]\n",
    "                if x1 < kp_x < x2 and y1 < kp_y < y2:\n",
    "                    keypoints_sorted[bbox_idx][kp_id] = [kp_x, kp_y, 1] # All keypoints are visible\n",
    "                    \n",
    "        return bboxes, keypoints_sorted\n",
    "\n",
    "\n",
    "    for i in range(len(SET)):\n",
    "        IMAGES = BASE + SET[i] + \"/images\"\n",
    "        LABELS = BASE + SET[i] + \"/labels\"\n",
    "        ANNOTATIONS = BASE +  SET[i] + \"/annotations\"\n",
    "        \n",
    "        files_names = [file.split('.jpg')[0] for file in os.listdir(IMAGES)]\n",
    "\n",
    "        for file in tqdm((files_names), desc =f\"Set: {SET[i]}\"):\n",
    "        #for file in (files_names):\n",
    "            file_labels = os.path.join(LABELS, file + \".txt\")\n",
    "            file_image = os.path.join(IMAGES, file + \".jpg\")\n",
    "\n",
    "            #img = cv2.imread(file_image)\n",
    "            #if img.shape[0] != img.shape[1]:\n",
    "                #print(\"Non square image:\", file_image)\n",
    "                \n",
    "            bboxes, keypoints_sorted = converter(file_labels, file_image, keypoint_names)\n",
    "\n",
    "            for i in keypoints_sorted:\n",
    "                a,b = i\n",
    "                if (len(a) != 3 or len (b) != 3) :\n",
    "                    print(\"Error in file\", file)\n",
    "\n",
    "            if not os.path.exists(ANNOTATIONS):\n",
    "                os.makedirs(ANNOTATIONS)\n",
    "\n",
    "            dump2json(bboxes, keypoints_sorted, os.path.join(ANNOTATIONS, file + '.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMG_SIZE = 512\n",
    "\n",
    "def train_transform():\n",
    "    return A.Compose([\n",
    "        A.Sequential([\n",
    "            A.GaussNoise(var_limit=(50, 100.0), mean=0, per_channel=True, always_apply=False, p=0.5),\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n",
    "            A.Rotate(p=0.5, limit=20, border_mode=cv2.BORDER_CONSTANT, value=0),\n",
    "            A.CLAHE(clip_limit=(1,4), p=1)\n",
    "            #A.Resize(IMG_SIZE, IMG_SIZE, p=1)\n",
    "        ], p=1)\n",
    "    ],\n",
    "    keypoint_params=A.KeypointParams(format='xy'), \n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bboxes_labels'])\n",
    "    )\n",
    "\n",
    "def test_transform():\n",
    "    return A.Compose([\n",
    "        A.Sequential([\n",
    "            A.CLAHE(clip_limit=(1,4), p=1)\n",
    "            #A.Resize(IMG_SIZE, IMG_SIZE, p=1)\n",
    "        ], p=1)\n",
    "    ],\n",
    "    keypoint_params=A.KeypointParams(format='xy'), \n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bboxes_labels'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transform():\n",
    "    return A.Compose([\n",
    "        A.GaussNoise(var_limit=(10, 50), mean=0, per_channel=True, p=0.5), # Simulate sensor noise\n",
    "        A.ISONoise(intensity=(0.1, 0.5), p=0.5), # Additional noise type\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5), # Adjust brightness & contrast\n",
    "        A.Rotate(limit=15, border_mode=cv2.BORDER_CONSTANT, value=0, p=0.5), # Small rotations\n",
    "        A.HorizontalFlip(p=0.5), # Horizontal flip (use with caution, depending on orientation-specific structures)\n",
    "        #A.ElasticTransform(alpha=1, sigma=50, p=0.5), # Elastic deformation\n",
    "        A.CLAHE(clip_limit=(1,4), p=1) # Enhance local contrast\n",
    "        #A.Resize(IMG_SIZE, IMG_SIZE, p=1) if IMG_SIZE else None # Resize if needed\n",
    "    ],\n",
    "    keypoint_params=A.KeypointParams(format='xy'),\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bboxes_labels'])\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, demo=False):                \n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.demo = demo # Use demo=True if you need transformed and original images (for example, for visualization purposes)\n",
    "        self.imgs_files = sorted(os.listdir(os.path.join(root, \"images\")))\n",
    "        self.annotations_files = sorted(os.listdir(os.path.join(root, \"annotations\")))\n",
    "\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root, \"images\", self.imgs_files[idx])\n",
    "        annotations_path = os.path.join(self.root, \"annotations\", self.annotations_files[idx])\n",
    "\n",
    "        img_original = cv2.imread(img_path)\n",
    "        img_original = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)        \n",
    "        \n",
    "        with open(annotations_path) as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "            bboxes_original = data['bboxes']\n",
    "            keypoints_original = data['keypoints']\n",
    "                          \n",
    "            # All objects are patellas, so we can use the same label for all objects\n",
    "            bboxes_labels_original = ['ROI' for _ in bboxes_original]  \n",
    "\n",
    "\n",
    "\n",
    "            if self.transform:   \n",
    "                # Converting keypoints from [x,y,visibility]-format to [x, y]-format + Flattening nested list of keypoints            \n",
    "                # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "                # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]], where each keypoint is in [x, y]-format            \n",
    "                # Then we need to convert it to the following list:\n",
    "                # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2]\n",
    "                \n",
    "                keypoints_original_flattened = [el[0:2] for kp in keypoints_original for el in kp]\n",
    "\n",
    "                # Apply augmentations\n",
    "                transformed = self.transform(image=img_original, bboxes=bboxes_original, bboxes_labels=bboxes_labels_original, keypoints=keypoints_original_flattened)\n",
    "                img = transformed['image']\n",
    "                bboxes = transformed['bboxes']\n",
    "                \n",
    "                # Unflattening list transformed['keypoints']\n",
    "                # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "                # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2], where each keypoint is in [x, y]-format\n",
    "                # Then we need to convert it to the following list:\n",
    "                # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]]\n",
    "                \n",
    "                #keypoints_transformed_unflattened = np.reshape(np.array(transformed['keypoints']), (-1,3,2)).tolist()\n",
    "\n",
    "                keypoints_transformed_unflattened = [transformed['keypoints']]\n",
    "\n",
    "\n",
    "                #keypoints_transformed_unflattened = np.reshape(np.array(transformed['keypoints']), (-1,3,2)).tolist()\n",
    "                \n",
    "                # Converting transformed keypoints from [x, y]-format to [x,y,visibility]-format by appending original visibilities to transformed coordinates of keypoints\n",
    "                keypoints = []\n",
    "                for o_idx, obj in enumerate(keypoints_transformed_unflattened): # Iterating over objects\n",
    "                    obj_keypoints = []\n",
    "\n",
    "                    for k_idx, kp in enumerate(obj): # Iterating over keypoints in each object\n",
    "                        # kp - coordinates of keypoint\n",
    "                        # keypoints_original[o_idx][k_idx][2] - original visibility of keypoint\n",
    "                        obj_keypoints.append(np.array(kp).tolist() + [keypoints_original[o_idx][k_idx][2]])\n",
    "                    keypoints.append(obj_keypoints)\n",
    "        \n",
    "            else:\n",
    "                img, bboxes, keypoints = img_original, bboxes_original, keypoints_original\n",
    "                 \n",
    "            # Convert everything into a torch tensor        \n",
    "            bboxes = torch.as_tensor(bboxes, dtype=torch.float32)       \n",
    "            target = {}\n",
    "            target[\"boxes\"] = bboxes\n",
    "            target[\"labels\"] = torch.as_tensor([1 for _ in bboxes], dtype=torch.int64)\n",
    "            target[\"image_id\"] = torch.tensor([idx])\n",
    "            target[\"area\"] = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])\n",
    "            target[\"iscrowd\"] = torch.zeros(len(bboxes), dtype=torch.int64)\n",
    "\n",
    "            target[\"keypoints\"] = torch.as_tensor(keypoints, dtype=torch.float32) \n",
    "            \n",
    "            img = F.to_tensor(img)\n",
    "            \n",
    "            bboxes_original = torch.as_tensor(bboxes_original, dtype=torch.float32)\n",
    "            target_original = {}\n",
    "            target_original[\"boxes\"] = bboxes_original\n",
    "            target_original[\"labels\"] = torch.as_tensor([1 for _ in bboxes_original], dtype=torch.int64) # all objects are patellas\n",
    "            target_original[\"image_id\"] = torch.tensor([idx])\n",
    "            target_original[\"area\"] = (bboxes_original[:, 3] - bboxes_original[:, 1]) * (bboxes_original[:, 2] - bboxes_original[:, 0])\n",
    "            target_original[\"iscrowd\"] = torch.zeros(len(bboxes_original), dtype=torch.int64)\n",
    "            target_original[\"keypoints\"] = torch.as_tensor(keypoints_original, dtype=torch.float32)        \n",
    "            img_original = F.to_tensor(img_original)\n",
    "\n",
    "            if self.demo:\n",
    "                return img, target, img_original, target_original\n",
    "            else:\n",
    "                return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ClassDataset(KEYPOINTS_FOLDER_TEST, transform=train_transform(), demo=True)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "iterator = iter(data_loader)\n",
    "batch = next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints_classes_ids2names = {0: 'Carina' , 1: 'ET'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(image, bboxes, keypoints, image_original=None, bboxes_original=None, keypoints_original=None, save = False, save_path = None, display=True):\n",
    "    fontsize = 12\n",
    "\n",
    "    for bbox in bboxes:\n",
    "        start_point = (bbox[0], bbox[1])\n",
    "        end_point = (bbox[2], bbox[3])\n",
    "        image = cv2.rectangle(image.copy(), start_point, end_point, (0,255,0), 2)\n",
    "    \n",
    "    for kps in keypoints:\n",
    "        for idx, kp in enumerate(kps):\n",
    "            image = cv2.circle(image.copy(), tuple(kp), 1, (255,0,0), 10)\n",
    "            image = cv2.putText(image.copy(), \" \" + keypoints_classes_ids2names[idx], tuple(kp), cv2.FONT_HERSHEY_PLAIN, 1, (255,0,0), 1, cv2.LINE_AA)\n",
    "\n",
    "    if image_original is None and keypoints_original is None:\n",
    "        plt.figure(figsize=(20,20))\n",
    "        plt.imshow(image)\n",
    "        \n",
    "\n",
    "    else:\n",
    "        for bbox in bboxes_original:\n",
    "            start_point = (bbox[0], bbox[1])\n",
    "            end_point = (bbox[2], bbox[3])\n",
    "            image_original = cv2.rectangle(image_original.copy(), start_point, end_point, (255,255,0), 2)\n",
    "        \n",
    "        for kps in keypoints_original:\n",
    "            for idx, kp in enumerate(kps):\n",
    "                image_original = cv2.circle(image_original, tuple(kp), 1, (255,255,0), 10)\n",
    "                image_original = cv2.putText(image_original, \" \" + keypoints_classes_ids2names[idx], tuple(kp), cv2.FONT_HERSHEY_PLAIN, 1, (255,0,0), 1, cv2.LINE_AA)\n",
    "        \n",
    "\n",
    "        f, ax = plt.subplots(1, 2, figsize=(40, 20))\n",
    "        ax[0].set_title('Original image', fontsize=fontsize)\n",
    "        ax[1].set_title('Predicted image', fontsize=fontsize) # also augmented image\n",
    "    \n",
    "        ax[0].imshow(image_original)\n",
    "        ax[1].imshow(image)\n",
    "        \n",
    "        if(save):\n",
    "            #print(\"Saving image to: \", save_path)\n",
    "            plt.gcf().set_size_inches(5, 10)\n",
    "            plt.savefig(save_path, dpi=100)\n",
    "            plt.close()\n",
    "            \n",
    "        if(display == False):\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = (batch[0][0].permute(1,2,0).numpy() * 255).astype(np.uint8)\n",
    "bboxes = batch[1][0]['boxes'].detach().cpu().numpy().astype(np.int32).tolist()\n",
    "\n",
    "keypoints = []\n",
    "for kps in batch[1][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist():\n",
    "    keypoints.append([kp[:2] for kp in kps])\n",
    "\n",
    "image_original = (batch[2][0].permute(1,2,0).numpy() * 255).astype(np.uint8)\n",
    "bboxes_original = batch[3][0]['boxes'].detach().cpu().numpy().astype(np.int32).tolist()\n",
    "\n",
    "keypoints_original = []\n",
    "for kps in batch[3][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist():\n",
    "    keypoints_original.append([kp[:2] for kp in kps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(image, bboxes, keypoints, image_original, bboxes_original, keypoints_original, display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_keypoints=2, anchor_sizes = (64, 128, 256) , anchor_ratios= (0.5, 0.83, 1.2, 2), weights_path=None):\n",
    "    \n",
    "    backbone = torchvision.models.convnext_large(weights='DEFAULT').features\n",
    "    backbone.out_channels = 1536 # 1536 for convnext_large, 1024 for resnet50\n",
    "    #backbone = torchvision.models.resnet101(weights='DEFAULT').features\n",
    "    #backbone.out_channels = 1024 \n",
    "\n",
    "    anchor_generator = AnchorGenerator(sizes=(anchor_sizes,), aspect_ratios=(anchor_ratios,))\n",
    "    \n",
    "    roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
    "                                                    output_size=7,\n",
    "                                                    sampling_ratio=2)\n",
    "    keypoint_roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
    "                                                             output_size=14,\n",
    "                                                             sampling_ratio=2)\n",
    "\n",
    "    model = KeypointRCNN(backbone,\n",
    "                          num_classes=2,\n",
    "                          rpn_anchor_generator=anchor_generator,\n",
    "                          box_roi_pool=roi_pooler,\n",
    "                          keypoint_roi_pool=keypoint_roi_pooler,\n",
    "                          num_keypoints=num_keypoints)\n",
    "    \n",
    "    #if weights_path:\n",
    "    #    print(\"loading weights from: \", weights_path)\n",
    "    #    state_dict = torch.load(weights_path)\n",
    "    #    model.load_state_dict(state_dict)        \n",
    "        \n",
    "    return model\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = ClassDataset(KEYPOINTS_FOLDER_TRAIN, transform=train_transform(), demo=False)\n",
    "dataset_test = ClassDataset(KEYPOINTS_FOLDER_TEST, transform=test_transform(), demo=False)\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_distance(kp1, kp2):\n",
    "    x1, y1 = kp1\n",
    "    x2, y2 = kp2\n",
    "    return math.sqrt((x2 - x1)**2 + (y2 - y1)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_epoch_error(model, data_loader_test, device):\n",
    "    with torch.inference_mode():\n",
    "        model.eval()\n",
    "        running_epoch_dist_error = 0\n",
    "        num_keypoint_predictions = 0\n",
    "        avg_dist_error = 0\n",
    "        for batch_idx, (images, targets) in enumerate(data_loader_test):\n",
    "            #print(f\"Processing batch {batch_idx}\")\n",
    "            #print(f\"Number of images: {len(images)}, Number of targets: {len(targets)}\")\n",
    "            \n",
    "            if not targets:\n",
    "                #print(\"No targets available for the current batch.\")\n",
    "                continue\n",
    "\n",
    "            images = list(image.to(device) for image in images)\n",
    "            predictions = model(images)\n",
    "            for i in range(len(predictions)):\n",
    "                for idx in range(num_keypoints):\n",
    "                    try:\n",
    "                        pred = predictions[i]['keypoints'][0][idx].detach().cpu().numpy().astype(np.int32)\n",
    "                        x1, y1, _ = predictions[i]['keypoints'][0][idx].detach().cpu().numpy().astype(np.int32)\n",
    "                        kp1 = (x1, y1)\n",
    "                        x2, y2, _ = targets[i]['keypoints'][0][idx].detach().cpu().numpy().astype(np.int32)\n",
    "                        kp2 = (x2, y2)\n",
    "                        num_keypoint_predictions += 1\n",
    "                        running_epoch_dist_error += calc_distance(kp1, kp2)\n",
    "                        avg_dist_error = running_epoch_dist_error / num_keypoint_predictions\n",
    "                    except:\n",
    "                        print(\"No prediction for val image.\")\n",
    "        print(\"Running Epoch Error: \", avg_dist_error)\n",
    "        score = -avg_dist_error\n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, fold, save_dir):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'epoch': fold,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, os.path.join(save_dir, f'checkpoint__{fold}_{epoch}.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def train_and_evaluate(model, train_data, val_data, device, hp_combination):\n",
    "    if train_model == True:\n",
    "        #print(\"Training model...\")\n",
    "\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        optimizer = torch.optim.SGD(params, lr=0.0005, momentum=0.8, weight_decay=0.0005)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.3)\n",
    "\n",
    "\n",
    "        #save_dir = 'checkpoints'\n",
    "        #os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch {epoch}\")\n",
    "            train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=100)\n",
    "            lr_scheduler.step()\n",
    "            #save_checkpoint(model, optimizer, epoch, save_dir)\n",
    "            #evaluate(model, data_loader_test, device)\n",
    "            \n",
    "            #model.eval()\n",
    "            \n",
    "        #torch.save(model.state_dict(), save_path)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(model):\n",
    "    for name, module in model.named_modules():\n",
    "        if not name.startswith(\"backbone\"):\n",
    "            if isinstance(model, nn.Conv2d) or isinstance(model, nn.Linear) or isinstance(model, nn.ConvTranspose2d):\n",
    "                model.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import itertools\n",
    "\n",
    "def grid_search(train_data, val_data, device, num_keypoints, hyperparams):\n",
    "    print(\"Started at: \", datetime.datetime.now().strftime(\"%H:%M:%S\"))\n",
    "    best_score = 0\n",
    "    best_hyperparams = None\n",
    "\n",
    "    for hp_combination in itertools.product(*hyperparams.values()):\n",
    "        print(f\"Running training with hyperparameters: {hp_combination}\")\n",
    "\n",
    "        anchor_size, anchor_ratios = hp_combination\n",
    "\n",
    "        model = get_model(2, anchor_size, anchor_ratios)\n",
    "        model.to(device)\n",
    "\n",
    "\n",
    "        #checkpoint_path = \"D:/ET_Tube/CheXpert-v1.0/checkpoints/checkpoint_9.pt\"\n",
    "        #checkpoint = torch.load(checkpoint_path)\n",
    "        #model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "        # Train and evaluate the model with the current hyperparameters\n",
    "        train_and_evaluate(model, train_data, val_data, device, hp_combination)\n",
    "        score = calc_epoch_error(model, data_loader_test, device)\n",
    "        \n",
    "\n",
    "        # Update the best score and hyperparameters if necessary\n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_hyperparams = hp_combination\n",
    "            print(f\"Saving best model at{save_path}\")\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "\n",
    "    print(f\"Best hyperparameters: {best_hyperparams}\")\n",
    "    print(\"Ended at: \", datetime.datetime.now().strftime(\"%H:%M:%S\"))\n",
    "    return best_hyperparams\n",
    "\n",
    "hyperparams = {\n",
    "    'anchor_sizes': [(64, 128, 256)],\n",
    "    'anchor_ratios': [(0.5, 0.83, 1.2, 2)]\n",
    "}\n",
    "\n",
    "#if train_model == True:\n",
    "#    best_hyperparams = grid_search(data_loader_train, data_loader_test, device, num_keypoints, hyperparams)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_distance(kp1, kp2):\n",
    "    x1, y1 = kp1\n",
    "    x2, y2 = kp2\n",
    "    return math.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "\n",
    "def calc_mean_distance_error(real_keypoints, pred_keypoints):\n",
    "    mean_distance = 0\n",
    "    point_error = [0,0,0]\n",
    "    assert len(real_keypoints) == len(pred_keypoints)\n",
    "    for i in range(len(real_keypoints)):\n",
    "        for j in range(len(real_keypoints[i])):\n",
    "            point_error[j] = calc_distance(real_keypoints[i][j], pred_keypoints[i][j])\n",
    "            mean_distance += point_error[j]\n",
    "    return mean_distance/len(keypoints), point_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_keypoints(scores):\n",
    "    avg_scores = []\n",
    "    for score in scores:\n",
    "        avg_scores.append(torch.mean(score))\n",
    "\n",
    "    try:\n",
    "        if len(avg_scores) > 0:\n",
    "            return avg_scores.index(max(avg_scores))\n",
    "        else:\n",
    "            raise ValueError(\"avg_scores is empty. Make sure you have calculated the average scores before finding the maximum.\")\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        return []  # or some other default value\n",
    "   \n",
    "    #return avg_scores.index(max(avg_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Proportion of Correct Keypoints (PCK)\n",
    "# Threshold is the maximum distance between the predicted and ground truth keypoints\n",
    "# Here it is written (default param) as 10 pixels\n",
    "# Perhaps this should be a function of the image size?\n",
    "def calculate_example_pck(total_point_error_list, threshold=10):\n",
    "    num_correct = 0\n",
    "    flat_list = list(itertools.chain(*total_point_error_list))\n",
    "\n",
    "    for point in flat_list:\n",
    "        if point <= threshold:\n",
    "            num_correct += 1\n",
    "            \n",
    "# Calculate the PCK\n",
    "    pck = num_correct / len(flat_list)\n",
    "    return pck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "k fold cross\n",
    "def train_and_evaluate(device, k_folds=5):\n",
    "    with open('output.txt', 'a') as output_file:\n",
    "        print(\"Starting at: \", datetime.datetime.now().strftime(\"%H:%M:%S\"))\n",
    "        \n",
    "\n",
    "        \n",
    "        if train_model == True:\n",
    "            # Combine the train and test datasets\n",
    "            combined_dataset = ConcatDataset([dataset_train, dataset_test])\n",
    "\n",
    "            # K-fold cross-validation\n",
    "            kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "            \n",
    "            fold = 0\n",
    "            for train_idx, val_idx in kf.split(combined_dataset):\n",
    "\n",
    "                example_error_list = []\n",
    "                pred_image_file_list = []\n",
    "                total_mean_distance_error = 0\n",
    "                total_mean_ETT_distance_error = 0\n",
    "                total_number_of_predictions = 0\n",
    "                no_preds_count = 0\n",
    "                total_attempts = 0\n",
    "                batch_count = 0\n",
    "                correct_predictions = 0\n",
    "                incorrect_predictions = 0\n",
    "                correct_real = 0\n",
    "                incorrect_real = 0\n",
    "                TP = 0\n",
    "                FP = 0\n",
    "                TN = 0\n",
    "                FN = 0\n",
    "                fold += 1\n",
    "                print(f\"Fold {fold}\")\n",
    "                \n",
    "                # Split the data into training and validation sets for this fold\n",
    "                train_data = [combined_dataset[i] for i in tqdm(train_idx)]\n",
    "                val_data = [combined_dataset[i] for i in tqdm(val_idx)]\n",
    "                \n",
    "                # Initialize the dataloaders\n",
    "                data_loader_train = DataLoader(train_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "                data_loader_val = DataLoader(val_data, batch_size=10, shuffle=False, collate_fn=collate_fn)\n",
    "                \n",
    "                model = get_model(2)\n",
    "                params = [p for p in model.parameters() if p.requires_grad]\n",
    "                optimizer = torch.optim.SGD(params, lr=0.0005, momentum=0.8, weight_decay=0.0005)\n",
    "                lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.3)\n",
    "\n",
    "                for epoch in range(num_epochs):\n",
    "                    model.to(device)\n",
    "                    train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=100)\n",
    "                    lr_scheduler.step()\n",
    "                    save_checkpoint(model, optimizer, epoch, fold, save_dir)\n",
    "                \n",
    "                #Evaluate the model on the validation set\n",
    "                with torch.inference_mode():\n",
    "\n",
    "                    #print(\"Loading model...\")\n",
    "                    #anchor_sizes, anchor_ratios = best_hyperparams        \n",
    "                    #model = get_model(2, anchor_sizes=anchor_sizes, anchor_ratios=anchor_ratios)\n",
    "                    #model.to(device)\n",
    "\n",
    "                    #checkpoint_path = r\"D:\\ET_Tube\\CheXpert-v1.0\\checkpoints\\checkpoint_3.pt\"\n",
    "                    #checkpoint = torch.load(checkpoint_path)\n",
    "                    #model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                    \n",
    "                    #test_batch_size = 1\n",
    "                    #data_loader_test = DataLoader(val_data, batch_size=test_batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "                    \n",
    "      \n",
    "                    #dataset_val = ClassDataset(KEYPOINTS_FOLDER_TEST, transform=test_transform(), demo=False)\n",
    "                    #data_loader_test = DataLoader(dataset_val, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "                    iterator = iter(data_loader_val)\n",
    "                    for item in iterator:\n",
    "                        try:\n",
    "                            images, targets = item\n",
    "                            images = list(image.to(device) for image in images)\n",
    "                        except StopIteration as e:\n",
    "                            print(\"StopIteration exception handled at batch: \", batch_count)\n",
    "                            \n",
    "                        model.to(device)            \n",
    "                        model.eval()\n",
    "                        output = model(images)   \n",
    "                        #print(\"output: \", output)\n",
    "\n",
    "                        batch_count += 1 \n",
    "\n",
    "                        for prediction_number in range(len(images)):\n",
    "                            \n",
    "                            real_keypoints = [] #list of keypoints for each image in batch\n",
    "\n",
    "                            #unpacking the targets, this is a pain but works to remove the 0/1 visibility dim (which we do not need because all keypoints are visible)\n",
    "                            for kps in targets[prediction_number]['keypoints']:\n",
    "                                real_keypoints.append([list(map(int, kp[:2])) for kp in kps])\n",
    "                            distance_real = calc_distance(real_keypoints[0][0], real_keypoints[0][1])\n",
    "                            #print(\"Real keypoints: \", real_keypoints)\n",
    "                        \n",
    "                            \n",
    "                            real_bboxes = targets[prediction_number]['boxes'].int().tolist()\n",
    "                            \n",
    "                            #permute(1,2,0) converts the tensor to numpy array. The tensor is in the format (C, H, W) and numpy array is in the format (H, W, C).\n",
    "                            #detach().cpu().numpy() detaches the tensor from the graph and converts it to numpy array and moves it to CPU.\n",
    "                            image = (images[prediction_number].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)  \n",
    "                            \n",
    "                            scores = output[prediction_number]['scores'].detach().cpu().numpy()\n",
    "                            if len(scores) == 0:\n",
    "                                print(\"No keypoints found at image: \", prediction_number)\n",
    "                                no_preds_count += 1\n",
    "                                break\n",
    "\n",
    "\n",
    "                            high_scores_idxs = np.where(scores > 0.1)[0].tolist() # Indexes of boxes with scores > 0.1\n",
    "                            #print(\"High Score idxs: \", high_scores_idxs)\n",
    "                            #print(\"Raw NMS Boxes len: \", len(output[0]['boxes'][high_scores_idxs]))\n",
    "                            #print(\"Raw NMS scores len: \", len(output[0]['scores'][high_scores_idxs]))\n",
    "                            try:\n",
    "                                #print(\"Box idx:\", output[0]['boxes'][high_scores_idxs])\n",
    "                                #print(\"Box scores:\", output[0]['scores'][high_scores_idxs])\n",
    "                                post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n",
    "                            except:\n",
    "                                print(\"NMS exception handled at batch: \", batch_count)\n",
    "                                post_nms_idxs = 0\n",
    "                                continue\n",
    "                            \n",
    "                            #print(\"-----------Post NMS idxs:-----------\", post_nms_idxs)\n",
    "\n",
    "                            #Making images based on keypoints_scores, instead of bbox scores now\n",
    "                            #print(\"Raw Keypoint scores: \", output[prediction_number]['keypoints_scores'])\n",
    "                            keypoint_scores = output[prediction_number]['keypoints_scores'][post_nms_idxs]\n",
    "                            #print(\"Keypoint scores: \", keypoint_scores)\n",
    "                            #print(\"Keypoint scores: \", keypoint_scores)\n",
    "                            score_idx = get_best_keypoints(keypoint_scores)\n",
    "                            #print(\"Best Keypoints IDX: \", score_idx)\n",
    "                            #score_idx = 0\n",
    "                            pred_keypoints = []\n",
    "                            keypoints = output[prediction_number]['keypoints'][score_idx].detach().cpu().numpy()\n",
    "                        \n",
    "                            for kp in keypoints:\n",
    "                                kp = list(map(int, kp)) #convert (x,y) coords in each keypoint to int\n",
    "                                pred_keypoints.append(kp[:2])\n",
    "\n",
    "\n",
    "                            if len(pred_keypoints) != 0:\n",
    "                                pred_keypoints = [pred_keypoints] #convert to list of lists to match real_keypoints format\n",
    "                                #print(\"Pred keypoints: \", pred_keypoints)\n",
    "                            \n",
    "                                distance_pred = calc_distance(pred_keypoints[0][0], pred_keypoints[0][1])\n",
    "                                distance_error = abs(distance_pred - distance_real)\n",
    "                            \n",
    "                            #print(\"Real ETT to Carina:\", distance_real/50)\n",
    "                            #print(\"Prediencted ETT to Carina:\", distance_pred/50)\n",
    "                            #print(\"ETT to Carina Distance error:\" , distance_error/50)\n",
    "\n",
    "                                if ((distance_real/50) <= 3) or ((distance_real/50) >= 7):\n",
    "                                    incorrect_real += 1\n",
    "                                else:\n",
    "                                    correct_real += 1\n",
    "\n",
    "                                if ((distance_pred/50) <= 3) or ((distance_pred/50) >= 7):\n",
    "                                    incorrect_predictions += 1\n",
    "                                else:\n",
    "                                    correct_predictions += 1\n",
    "\n",
    "                                if (((distance_real/50) <= 3) or ((distance_real/50) >= 7)) and (((distance_pred/50) <= 3) or ((distance_pred/50) >= 7)):\n",
    "                                    TN +=1\n",
    "                                    #print(\"TN\")\n",
    "                                elif (((distance_real/50) <= 3) or ((distance_real/50) >= 7)) and (((distance_pred/50) > 3) or ((distance_pred/50) < 7)):\n",
    "                                    FN +=1\n",
    "                                    #print(\"FN\")\n",
    "                                elif (((distance_real/50) > 3) or ((distance_real/50) < 7)) and (((distance_pred/50) <= 3) or ((distance_pred/50) >= 7)):\n",
    "                                    FP +=1\n",
    "                                    #print(\"FP\")\n",
    "                                elif (((distance_real/50) > 3) or ((distance_real/50) < 7)) and (((distance_pred/50) > 3) or ((distance_pred/50) < 7)):\n",
    "                                    TP +=1\n",
    "                                    #print(\"TP\")\n",
    "\n",
    "                                bboxes = []\n",
    "                                    \n",
    "                                #print(\"Boxes:\", output[prediction_number]['boxes'][[0]][[0]])\n",
    "                                #print(\"Scores:\", output[prediction_number]['scores'])\n",
    "                                for bbox in  output[prediction_number]['boxes'][[score_idx]][[0]].detach().cpu().numpy():\n",
    "                                    bboxes.append(list(map(int, bbox.tolist())))\n",
    "                                \n",
    "                                \n",
    "                                if (len(bboxes) == 0):\n",
    "                                    print(\"No bounding boxes found at image: \", prediction_number)\n",
    "                                    no_preds_count += 1 #count preds with no bounding box at given threshold  \n",
    "                                else:\n",
    "                                    total_number_of_predictions += 1    \n",
    "                                    example_error, point_error = calc_mean_distance_error(real_keypoints, pred_keypoints)\n",
    "                                    total_mean_distance_error += example_error\n",
    "                                \n",
    "                            \n",
    "                            \n",
    "                                example_point_error = [pt for pt in point_error]\n",
    "                                total_point_error_list.append(example_point_error)\n",
    "                                example_error_list.append(example_error)\n",
    "                                ETT_distance_error_list.append(distance_error)\n",
    "                                #print(\"Example error: \", example_error)\n",
    "                                \n",
    "                            \n",
    "                                #save_img_path = f\"F:\\ET_Tube\\saved_img_preds\\prediction_{str(prediction_number)}_{str(batch_count)}.jpg\"\n",
    "                                save_img_path = f\"F:\\ET_Tube\\saved_img_preds\\prediction_{str(prediction_number)}_{str(batch_count)}.jpg\"\n",
    "                                pred_image_file_list.append(save_img_path)\n",
    "                                visualize(image, bboxes, pred_keypoints, image_original=image, keypoints_original=real_keypoints, bboxes_original=real_bboxes, save=False, save_path = save_img_path, display=False)\n",
    "                                plt.savefig(save_img_path)\n",
    "\n",
    "                            total_attempts += 1\n",
    "\n",
    "                    print(\"Placement Correct predictions: \", correct_predictions)\n",
    "                    print(\"Placement Incorrect predictions: \", incorrect_predictions)\n",
    "                    print(\"Placement Correct real: \", correct_real)\n",
    "                    print(\"Placement Incorrect real: \", incorrect_real)\n",
    "\n",
    "                    if total_number_of_predictions != 0:\n",
    "                        total_mean_distance_error /= total_number_of_predictions\n",
    "                    print(\"Total mean Euclidean distance error: \", total_mean_distance_error)\n",
    "                    if len(ETT_distance_error_list) != 0:\n",
    "                        total_mean_ETT_distance_error = sum(ETT_distance_error_list)/len(ETT_distance_error_list)\n",
    "                    print(\"Total ETT distance error: \", total_mean_ETT_distance_error)\n",
    "                    print(\"No predictions count: \", no_preds_count)\n",
    "                    print(\"Total attempts: \", total_attempts)\n",
    "\n",
    "                    pck_threshold_large = 50\n",
    "                    pck = calculate_example_pck(total_point_error_list, threshold=pck_threshold_large)\n",
    "                    print(f\"Fraction of Correct Keypoints: {pck:0.3f}, at a threshold of {pck_threshold_large} pixels.\")\n",
    "                    pck_list.append(pck)\n",
    "\n",
    "                    print(\"TP: \", TP)\n",
    "                    print(\"FP: \", FP)\n",
    "                    print(\"TN: \", TN)\n",
    "                    print(\"FN: \", FN)\n",
    "\n",
    "                    epsilon = 1e-8 #to avoid division by 0 errors \n",
    "\n",
    "                    precision = TP / (TP + FP + epsilon)\n",
    "                    precision_list.append(precision)\n",
    "                    print(\"Precision/PPV: \", precision)\n",
    "                    recall = TP / (TP + FN + epsilon)\n",
    "                    recall_list.append(recall)\n",
    "                    print (\"Recall/Sensitivity: \", recall)\n",
    "                    NPV = TN / (TN + FN + epsilon)\n",
    "                    NPV_list.append(NPV)\n",
    "                    print(\"NPV: \", NPV)\n",
    "                    accuracy = (TP + TN) / (TP + TN + FP + FN + epsilon)\n",
    "                    accuracy_list.append(accuracy)\n",
    "                    print(\"Accuracy: \", accuracy)\n",
    "                    specificity = TN / (TN + FP + epsilon)\n",
    "                    specificity_list.append(specificity)\n",
    "                    print(\"Specificity: \", specificity)\n",
    "                    f1_score = 2 * (precision * recall) / (precision + recall + epsilon)\n",
    "                    f1_score_list.append(f1_score)\n",
    "                    print(\"F1 Score: \", f1_score)\n",
    "\n",
    "\n",
    "                    print(\"Completed at: \", datetime.datetime.now().strftime(\"%H:%M:%S\"))\n",
    "                # Reset the model weights for the next fold\n",
    "                print(\"Resetting model weights\")\n",
    "                model.apply(reset_weights)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "def train_and_evaluate(device, data_loader_train, data_loader_test):\n",
    "\n",
    "    print(\"Starting at: \", datetime.datetime.now())\n",
    "    \n",
    "    if train_model == True:\n",
    "        # Initialize the dataloaders\n",
    "        # You need to define these according to your application\n",
    "        data_loader_train = data_loader_train \n",
    "        data_loader_val = data_loader_test\n",
    "\n",
    "        example_error_list = []\n",
    "        pred_image_file_list = []\n",
    "        total_mean_distance_error = 0\n",
    "        total_mean_ETT_distance_error = 0\n",
    "        total_number_of_predictions = 0\n",
    "        no_preds_count = 0\n",
    "        total_attempts = 0\n",
    "        batch_count = 0\n",
    "        correct_predictions = 0\n",
    "        incorrect_predictions = 0\n",
    "        correct_real = 0\n",
    "        incorrect_real = 0\n",
    "        TP = 0\n",
    "        FP = 0\n",
    "        TN = 0\n",
    "        FN = 0\n",
    "        \n",
    "        model = get_model(2)\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        optimizer = torch.optim.SGD(params, lr=0.0005, momentum=0.8, weight_decay=0.0005)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.3)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.to(device)\n",
    "            train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=100)\n",
    "            lr_scheduler.step()\n",
    "            save_checkpoint(model, optimizer, epoch, 1, save_dir)\n",
    "        #Evaluate the model on the validation set\n",
    "        with torch.inference_mode():\n",
    "\n",
    "            batch_count = 0\n",
    "            iterator = iter(data_loader_val)\n",
    "            for item in iterator:\n",
    "                try:\n",
    "                    images, targets = item\n",
    "                    images = list(image.to(device) for image in images)\n",
    "                except StopIteration as e:\n",
    "                    print(\"StopIteration exception handled at batch: \", batch_count)\n",
    "                    \n",
    "                #model.to(device)            \n",
    "                #model.eval()\n",
    "                output = model(images)   \n",
    "                #print(\"output: \", output)\n",
    "\n",
    "                batch_count += 1 \n",
    "\n",
    "                for prediction_number in range(len(images)):\n",
    "                    \n",
    "                    real_keypoints = [] #list of keypoints for each image in batch\n",
    "\n",
    "                    #unpacking the targets, this is a pain but works to remove the 0/1 visibility dim (which we do not need because all keypoints are visible)\n",
    "                    for kps in targets[prediction_number]['keypoints']:\n",
    "                        real_keypoints.append([list(map(int, kp[:2])) for kp in kps])\n",
    "                    distance_real = calc_distance(real_keypoints[0][0], real_keypoints[0][1])\n",
    "                    #print(\"Real keypoints: \", real_keypoints)\n",
    "                \n",
    "                    \n",
    "                    real_bboxes = targets[prediction_number]['boxes'].int().tolist()\n",
    "                    \n",
    "                    #permute(1,2,0) converts the tensor to numpy array. The tensor is in the format (C, H, W) and numpy array is in the format (H, W, C).\n",
    "                    #detach().cpu().numpy() detaches the tensor from the graph and converts it to numpy array and moves it to CPU.\n",
    "                    image = (images[prediction_number].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)  \n",
    "                    \n",
    "                    scores = output[prediction_number]['scores'].detach().cpu().numpy()\n",
    "                    if len(scores) == 0:\n",
    "                        print(\"No keypoints found at image: \", prediction_number)\n",
    "                        no_preds_count += 1\n",
    "                        break\n",
    "\n",
    "\n",
    "                    high_scores_idxs = np.where(scores > 0)[0].tolist() # Indexes of boxes with scores > 0.1\n",
    "                    #print(\"High Score idxs: \", high_scores_idxs)\n",
    "                    #print(\"Raw NMS Boxes len: \", len(output[0]['boxes'][high_scores_idxs]))\n",
    "                    #print(\"Raw NMS scores len: \", len(output[0]['scores'][high_scores_idxs]))\n",
    "                    try:\n",
    "                        #print(\"Box idx:\", output[0]['boxes'][high_scores_idxs])\n",
    "                        #print(\"Box scores:\", output[0]['scores'][high_scores_idxs])\n",
    "                        post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n",
    "                    except:\n",
    "                        print(\"NMS exception handled at batch: \", batch_count)\n",
    "                        post_nms_idxs = 0\n",
    "                        continue\n",
    "                    \n",
    "                    #print(\"-----------Post NMS idxs:-----------\", post_nms_idxs)\n",
    "\n",
    "                    #Making images based on keypoints_scores, instead of bbox scores now\n",
    "                    #print(\"Raw Keypoint scores: \", output[prediction_number]['keypoints_scores'])\n",
    "                    keypoint_scores = output[prediction_number]['keypoints_scores'][post_nms_idxs]\n",
    "                    #print(\"Keypoint scores: \", keypoint_scores)\n",
    "                    #print(\"Keypoint scores: \", keypoint_scores)\n",
    "                    score_idx = get_best_keypoints(keypoint_scores)\n",
    "                    #print(\"Best Keypoints IDX: \", score_idx)\n",
    "                    #score_idx = 0\n",
    "                    pred_keypoints = []\n",
    "                    keypoints = output[prediction_number]['keypoints'][score_idx].detach().cpu().numpy()\n",
    "                \n",
    "                    for kp in keypoints:\n",
    "                        kp = list(map(int, kp)) #convert (x,y) coords in each keypoint to int\n",
    "                        pred_keypoints.append(kp[:2])\n",
    "\n",
    "\n",
    "                    if len(pred_keypoints) != 0:\n",
    "                        pred_keypoints = [pred_keypoints] #convert to list of lists to match real_keypoints format\n",
    "                        #print(\"Pred keypoints: \", pred_keypoints)\n",
    "                    \n",
    "                        distance_pred = calc_distance(pred_keypoints[0][0], pred_keypoints[0][1])\n",
    "                        distance_error = abs(distance_pred - distance_real)\n",
    "                    \n",
    "                    #print(\"Real ETT to Carina:\", distance_real/50)\n",
    "                    #print(\"Prediencted ETT to Carina:\", distance_pred/50)\n",
    "                    #print(\"ETT to Carina Distance error:\" , distance_error/50)\n",
    "\n",
    "                        if ((distance_real/50) <= 3) or ((distance_real/50) >= 7):\n",
    "                            incorrect_real += 1\n",
    "                        else:\n",
    "                            correct_real += 1\n",
    "\n",
    "                        if ((distance_pred/50) <= 3) or ((distance_pred/50) >= 7):\n",
    "                            incorrect_predictions += 1\n",
    "                        else:\n",
    "                            correct_predictions += 1\n",
    "\n",
    "                        if (((distance_real/50) <= 3) or ((distance_real/50) >= 7)) and (((distance_pred/50) <= 3) or ((distance_pred/50) >= 7)):\n",
    "                            TN +=1\n",
    "                            #print(\"TN\")\n",
    "                        elif (((distance_real/50) <= 3) or ((distance_real/50) >= 7)) and (((distance_pred/50) > 3) or ((distance_pred/50) < 7)):\n",
    "                            FN +=1\n",
    "                            #print(\"FN\")\n",
    "                        elif (((distance_real/50) > 3) or ((distance_real/50) < 7)) and (((distance_pred/50) <= 3) or ((distance_pred/50) >= 7)):\n",
    "                            FP +=1\n",
    "                            #print(\"FP\")\n",
    "                        elif (((distance_real/50) > 3) or ((distance_real/50) < 7)) and (((distance_pred/50) > 3) or ((distance_pred/50) < 7)):\n",
    "                            TP +=1\n",
    "                            #print(\"TP\")\n",
    "\n",
    "                        bboxes = []\n",
    "                            \n",
    "                        #print(\"Boxes:\", output[prediction_number]['boxes'][[0]][[0]])\n",
    "                        #print(\"Scores:\", output[prediction_number]['scores'])\n",
    "                        for bbox in  output[prediction_number]['boxes'][[score_idx]][[0]].detach().cpu().numpy():\n",
    "                            bboxes.append(list(map(int, bbox.tolist())))\n",
    "                        \n",
    "                        \n",
    "                        if (len(bboxes) == 0):\n",
    "                            print(\"No bounding boxes found at image: \", prediction_number)\n",
    "                            no_preds_count += 1 #count preds with no bounding box at given threshold  \n",
    "                        else:\n",
    "                            total_number_of_predictions += 1    \n",
    "                            example_error, point_error = calc_mean_distance_error(real_keypoints, pred_keypoints)\n",
    "                            total_mean_distance_error += example_error\n",
    "                        \n",
    "                    \n",
    "                    \n",
    "                        example_point_error = [pt for pt in point_error]\n",
    "                        total_point_error_list.append(example_point_error)\n",
    "                        example_error_list.append(example_error)\n",
    "                        ETT_distance_error_list.append(distance_error)\n",
    "                        #print(\"Example error: \", example_error)\n",
    "                        \n",
    "                    \n",
    "                        #save_img_path = f\"F:\\ET_Tube\\saved_img_preds\\prediction_{str(prediction_number)}_{str(batch_count)}.jpg\"\n",
    "                        save_img_path = f\"F:\\ET_Tube\\saved_img_preds\\prediction_{str(prediction_number)}_{str(batch_count)}.jpg\"\n",
    "                        pred_image_file_list.append(save_img_path)\n",
    "                        visualize(image, bboxes, pred_keypoints, image_original=image, keypoints_original=real_keypoints, bboxes_original=real_bboxes, save=False, save_path = save_img_path, display=False)\n",
    "                        plt.savefig(save_img_path)\n",
    "\n",
    "                    total_attempts += 1\n",
    "\n",
    "            print(\"Placement Correct predictions: \", correct_predictions)\n",
    "            print(\"Placement Incorrect predictions: \", incorrect_predictions)\n",
    "            print(\"Placement Correct real: \", correct_real)\n",
    "            print(\"Placement Incorrect real: \", incorrect_real)\n",
    "\n",
    "            if total_number_of_predictions != 0:\n",
    "                total_mean_distance_error /= total_number_of_predictions\n",
    "            print(\"Total mean Euclidean distance error: \", total_mean_distance_error)\n",
    "            if len(ETT_distance_error_list) != 0:\n",
    "                total_mean_ETT_distance_error = sum(ETT_distance_error_list)/len(ETT_distance_error_list)\n",
    "            print(\"Total ETT distance error: \", total_mean_ETT_distance_error)\n",
    "            print(\"No predictions count: \", no_preds_count)\n",
    "            print(\"Total attempts: \", total_attempts)\n",
    "\n",
    "            pck_threshold_large = 50\n",
    "            pck = calculate_example_pck(total_point_error_list, threshold=pck_threshold_large)\n",
    "            print(f\"Fraction of Correct Keypoints: {pck:0.3f}, at a threshold of {pck_threshold_large} pixels.\")\n",
    "            pck_list.append(pck)\n",
    "\n",
    "            print(\"TP: \", TP)\n",
    "            print(\"FP: \", FP)\n",
    "            print(\"TN: \", TN)\n",
    "            print(\"FN: \", FN)\n",
    "\n",
    "            epsilon = 1e-8 #to avoid division by 0 errors \n",
    "\n",
    "            precision = TP / (TP + FP + epsilon)\n",
    "            precision_list.append(precision)\n",
    "            print(\"Precision/PPV: \", precision)\n",
    "            recall = TP / (TP + FN + epsilon)\n",
    "            recall_list.append(recall)\n",
    "            print (\"Recall/Sensitivity: \", recall)\n",
    "            NPV = TN / (TN + FN + epsilon)\n",
    "            NPV_list.append(NPV)\n",
    "            print(\"NPV: \", NPV)\n",
    "            accuracy = (TP + TN) / (TP + TN + FP + FN + epsilon)\n",
    "            accuracy_list.append(accuracy)\n",
    "            print(\"Accuracy: \", accuracy)\n",
    "            specificity = TN / (TN + FP + epsilon)\n",
    "            specificity_list.append(specificity)\n",
    "            print(\"Specificity: \", specificity)\n",
    "            f1_score = 2 * (precision * recall) / (precision + recall + epsilon)\n",
    "            f1_score_list.append(f1_score)\n",
    "            print(\"F1 Score: \", f1_score)\n",
    "\n",
    "\n",
    "            print(\"Completed at: \", datetime.datetime.now().strftime(\"%H:%M:%S\"))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_one_epoch(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    criterion = torch.nn.MSELoss() # Loss function\n",
    "    example_distance_error_list = []\n",
    "    example_distance_error_list_MSE = []\n",
    "    pred_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_count, batch  in enumerate(data_loader):\n",
    "            images, kp = batch\n",
    "            assert len(images) == len(kp)\n",
    "            for i in range(len(images)):\n",
    "                image = images[i].to(device)\n",
    "\n",
    "                # Get ground truth keypoints, remove visibility column\n",
    "                kps = torch.squeeze(kp[i]['keypoints'][:, :, :2])\n",
    "\n",
    "                keypoints_gt = kps.to(device)\n",
    "                \n",
    "                # Get predictions\n",
    "                output = model(image.unsqueeze(0))\n",
    "                \n",
    "                # Check if predictions are available\n",
    "                if len(output) > 0 and 'keypoints' in output[0] and len(output[0]['keypoints']) > 0:\n",
    "                    #get highest score keypoint preds, again without visibility column\n",
    "                    keypoints_pred = output[0]['keypoints'][0][:, :-1]\n",
    "\n",
    "                    # Compute loss\n",
    "                    loss = criterion(keypoints_pred, keypoints_gt)\n",
    "                    total_loss += loss.item()\n",
    "                    pred_count += 1\n",
    "\n",
    "                    diff = keypoints_pred - keypoints_gt\n",
    "                    squared_diff = diff ** 2\n",
    "                    example_distance_error = torch.sqrt(torch.sum(squared_diff))\n",
    "                    example_distance_error_list.append(example_distance_error)\n",
    "\n",
    "                else:\n",
    "                    print(f\"No keypoints detected for image: {i}, batch: {batch_count}\")\n",
    "\n",
    "    # Compute average loss and accuracy\n",
    "    if pred_count > 0:\n",
    "        avg_loss = total_loss / pred_count\n",
    "        avg_distance_error = torch.mean(torch.stack(example_distance_error_list))\n",
    "\n",
    "        print(f\"Average Validation Loss: {avg_loss}, Average Validation Distance Error: {avg_distance_error}\")\n",
    "        return avg_loss, avg_distance_error\n",
    "    else:\n",
    "        print(\"No keypoints detected in the entire batch.\")\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_one_epoch(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = ClassDataset(KEYPOINTS_FOLDER_VALID, transform=test_transform(), demo=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=5, shuffle=False, collate_fn=collate_fn)\n",
    "model = get_model()\n",
    "model.to(device)\n",
    "\n",
    "validate_one_epoch(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model == True:\n",
    "    print(\"Starting at: \", datetime.datetime.now())\n",
    "\n",
    "    model = get_model()\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.0005, momentum=0.8, weight_decay=0.0005)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.3)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.to(device)\n",
    "        train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=100)\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # Validation phase\n",
    "        validate_one_epoch(model, val_loader, device)\n",
    "\n",
    "        save_checkpoint(model, optimizer, epoch, 1, save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"num_epochs = 11\n",
    "if train_model == True:\n",
    "    print(\"Starting at: \", datetime.datetime.now())\n",
    "\n",
    "    model = get_model()\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.0005, momentum=0.8, weight_decay=0.0005)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.3)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.to(device)\n",
    "        train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=100)\n",
    "        lr_scheduler.step()\n",
    "        validate_one_epoch(model, data_loader_val, device)\n",
    "        save_checkpoint(model, optimizer, epoch, 1, save_dir)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "checkpoint_path = r\"D:\\ET_Tube\\CheXpert-v1.0\\checkpoints\\checkpoint__1_8.pt\"\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_size = 1\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=test_batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_list = []\n",
    "recall_list = []\n",
    "NPV_list = []\n",
    "accuracy_list = []\n",
    "specificity_list = []\n",
    "f1_score_list = []\n",
    "ETT_distance_error_list = []\n",
    "total_point_error_list = []\n",
    "pck_list = []\n",
    "\n",
    "\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "NPV_list = []\n",
    "accuracy_list = []\n",
    "specificity_list = []\n",
    "f1_score_list = []\n",
    "ETT_distance_error_list = []\n",
    "total_point_error_list = []\n",
    "pck_list = []\n",
    "\n",
    "example_error_list = []\n",
    "pred_image_file_list = []\n",
    "total_mean_distance_error = 0\n",
    "total_mean_ETT_distance_error = 0\n",
    "total_number_of_predictions = 0\n",
    "no_preds_count = 0\n",
    "total_attempts = 0\n",
    "batch_count = 0\n",
    "correct_predictions = 0\n",
    "incorrect_predictions = 0\n",
    "correct_real = 0\n",
    "incorrect_real = 0\n",
    "TP = 0\n",
    "FP = 0\n",
    "TN = 0\n",
    "FN = 0\n",
    "\n",
    "\n",
    "#Evaluate the model on the validation set\n",
    "\n",
    "model.to(device)            \n",
    "model.eval()\n",
    "\n",
    "batch_count = 0\n",
    "iterator = iter(data_loader_test)\n",
    "for item in iterator:\n",
    "    try:\n",
    "        images, targets = item\n",
    "        images = list(image.to(device) for image in images)\n",
    "    except StopIteration as e:\n",
    "        print(\"StopIteration exception handled at batch: \", batch_count)\n",
    "        \n",
    "\n",
    "    output = model(images)   \n",
    "    #print(\"output: \", output)\n",
    "\n",
    "    batch_count += 1 \n",
    "\n",
    "    for prediction_number in range(len(images)):\n",
    "        \n",
    "        real_keypoints = [] #list of keypoints for each image in batch\n",
    "\n",
    "        #unpacking the targets, this is a pain but works to remove the 0/1 visibility dim (which we do not need because all keypoints are visible)\n",
    "        for kps in targets[prediction_number]['keypoints']:\n",
    "            real_keypoints.append([list(map(int, kp[:2])) for kp in kps])\n",
    "        distance_real = calc_distance(real_keypoints[0][0], real_keypoints[0][1])\n",
    "        #print(\"Real keypoints: \", real_keypoints)\n",
    "    \n",
    "        \n",
    "        real_bboxes = targets[prediction_number]['boxes'].int().tolist()\n",
    "        \n",
    "        #permute(1,2,0) converts the tensor to numpy array. The tensor is in the format (C, H, W) and numpy array is in the format (H, W, C).\n",
    "        #detach().cpu().numpy() detaches the tensor from the graph and converts it to numpy array and moves it to CPU.\n",
    "        image = (images[prediction_number].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)  \n",
    "        \n",
    "        scores = output[prediction_number]['scores'].detach().cpu().numpy()\n",
    "        print(\"Scores: \", scores)\n",
    "        if len(scores) == 0:\n",
    "            print(\"No keypoints found at image: \", prediction_number)\n",
    "            no_preds_count += 1\n",
    "            break\n",
    "\n",
    "\n",
    "        high_scores_idxs = np.where(scores > 0)[0].tolist() # Indexes of boxes with scores > 0.1\n",
    "        #print(\"High Score idxs: \", high_scores_idxs)\n",
    "        #print(\"Raw NMS Boxes len: \", len(output[0]['boxes'][high_scores_idxs]))\n",
    "        #print(\"Raw NMS scores len: \", len(output[0]['scores'][high_scores_idxs]))\n",
    "        #print(\"Raw NMS Boxes len: \", len(output[0]['boxes'][high_scores_idxs]))\n",
    "        #print(\"High Score idxs: \", high_scores_idxs)\n",
    "        #print(\"Output: \", output)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #print(\"Raw NMS scores len: \", len(output[0]['scores'][high_scores_idxs]))\n",
    "        \n",
    "        #print(\"******************\")\n",
    "        post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n",
    "\n",
    "        #print(\"-----------Post NMS idxs:-----------\", post_nms_idxs)\n",
    "\n",
    "        #Making images based on keypoints_scores, instead of bbox scores now\n",
    "        #print(\"Raw Keypoint scores: \", output[prediction_number]['keypoints_scores'])\n",
    "        keypoint_scores = output[prediction_number]['keypoints_scores'][post_nms_idxs]\n",
    "        #print(\"Keypoint scores: \", keypoint_scores)\n",
    "        #print(\"Keypoint scores: \", keypoint_scores)\n",
    "        score_idx = get_best_keypoints(keypoint_scores)\n",
    "        #print(\"Best Keypoints IDX: \", score_idx)\n",
    "        #score_idx = 0\n",
    "        pred_keypoints = []\n",
    "        keypoints = output[prediction_number]['keypoints'][score_idx].detach().cpu().numpy()\n",
    "    \n",
    "        for kp in keypoints:\n",
    "            kp = list(map(int, kp)) #convert (x,y) coords in each keypoint to int\n",
    "            pred_keypoints.append(kp[:2])\n",
    "\n",
    "\n",
    "        if len(pred_keypoints) != 0:\n",
    "            pred_keypoints = [pred_keypoints] #convert to list of lists to match real_keypoints format\n",
    "            #print(\"Pred keypoints: \", pred_keypoints)\n",
    "        \n",
    "            distance_pred = calc_distance(pred_keypoints[0][0], pred_keypoints[0][1])\n",
    "            distance_error = abs(distance_pred - distance_real)\n",
    "        \n",
    "        #print(\"Real ETT to Carina:\", distance_real/50)\n",
    "        #print(\"Prediencted ETT to Carina:\", distance_pred/50)\n",
    "        #print(\"ETT to Carina Distance error:\" , distance_error/50)\n",
    "\n",
    "            if ((distance_real/50) <= 3) or ((distance_real/50) >= 7):\n",
    "                incorrect_real += 1\n",
    "            else:\n",
    "                correct_real += 1\n",
    "\n",
    "            if ((distance_pred/50) <= 3) or ((distance_pred/50) >= 7):\n",
    "                incorrect_predictions += 1\n",
    "            else:\n",
    "                correct_predictions += 1\n",
    "\n",
    "            if (((distance_real/50) <= 3) or ((distance_real/50) >= 7)) and (((distance_pred/50) <= 3) or ((distance_pred/50) >= 7)):\n",
    "                TN +=1\n",
    "                #print(\"TN\")\n",
    "            elif (((distance_real/50) <= 3) or ((distance_real/50) >= 7)) and (((distance_pred/50) > 3) or ((distance_pred/50) < 7)):\n",
    "                FN +=1\n",
    "                #print(\"FN\")\n",
    "            elif (((distance_real/50) > 3) or ((distance_real/50) < 7)) and (((distance_pred/50) <= 3) or ((distance_pred/50) >= 7)):\n",
    "                FP +=1\n",
    "                #print(\"FP\")\n",
    "            elif (((distance_real/50) > 3) or ((distance_real/50) < 7)) and (((distance_pred/50) > 3) or ((distance_pred/50) < 7)):\n",
    "                TP +=1\n",
    "                #print(\"TP\")\n",
    "\n",
    "            bboxes = []\n",
    "                \n",
    "            #print(\"Boxes:\", output[prediction_number]['boxes'][[0]][[0]])\n",
    "            #print(\"Scores:\", output[prediction_number]['scores'])\n",
    "            for bbox in  output[prediction_number]['boxes'][[score_idx]][[0]].detach().cpu().numpy():\n",
    "                bboxes.append(list(map(int, bbox.tolist())))\n",
    "            \n",
    "            \n",
    "            if (len(bboxes) == 0):\n",
    "                print(\"No bounding boxes found at image: \", prediction_number)\n",
    "                no_preds_count += 1 #count preds with no bounding box at given threshold  \n",
    "            else:\n",
    "                total_number_of_predictions += 1    \n",
    "                example_error, point_error = calc_mean_distance_error(real_keypoints, pred_keypoints)\n",
    "                total_mean_distance_error += example_error\n",
    "            \n",
    "        \n",
    "        \n",
    "            example_point_error = [pt for pt in point_error]\n",
    "            total_point_error_list.append(example_point_error)\n",
    "            example_error_list.append(example_error)\n",
    "            ETT_distance_error_list.append(distance_error)\n",
    "            #print(\"Example error: \", example_error)\n",
    "            \n",
    "        \n",
    "            #save_img_path = f\"F:\\ET_Tube\\saved_img_preds\\prediction_{str(prediction_number)}_{str(batch_count)}.jpg\"\n",
    "            save_img_path = f\"F:\\ET_Tube\\saved_img_preds\\prediction_{str(prediction_number)}_{str(batch_count)}.jpg\"\n",
    "            pred_image_file_list.append(save_img_path)\n",
    "            visualize(image, bboxes, pred_keypoints, image_original=image, keypoints_original=real_keypoints, bboxes_original=real_bboxes, save=False, save_path = save_img_path, display=False)\n",
    "            plt.savefig(save_img_path)\n",
    "\n",
    "        total_attempts += 1\n",
    "\n",
    "print(\"Placement Correct predictions: \", correct_predictions)\n",
    "print(\"Placement Incorrect predictions: \", incorrect_predictions)\n",
    "print(\"Placement Correct real: \", correct_real)\n",
    "print(\"Placement Incorrect real: \", incorrect_real)\n",
    "\n",
    "if total_number_of_predictions != 0:\n",
    "    total_mean_distance_error /= total_number_of_predictions\n",
    "print(\"Total mean Euclidean distance error: \", total_mean_distance_error)\n",
    "if len(ETT_distance_error_list) != 0:\n",
    "    total_mean_ETT_distance_error = sum(ETT_distance_error_list)/len(ETT_distance_error_list)\n",
    "print(\"Total ETT distance error: \", total_mean_ETT_distance_error)\n",
    "print(\"No predictions count: \", no_preds_count)\n",
    "print(\"Total attempts: \", total_attempts)\n",
    "\n",
    "pck_threshold_large = 50\n",
    "pck = calculate_example_pck(total_point_error_list, threshold=pck_threshold_large)\n",
    "print(f\"Fraction of Correct Keypoints: {pck:0.3f}, at a threshold of {pck_threshold_large} pixels.\")\n",
    "pck_list.append(pck)\n",
    "\n",
    "print(\"TP: \", TP)\n",
    "print(\"FP: \", FP)\n",
    "print(\"TN: \", TN)\n",
    "print(\"FN: \", FN)\n",
    "\n",
    "epsilon = 1e-8 #to avoid division by 0 errors \n",
    "\n",
    "precision = TP / (TP + FP + epsilon)\n",
    "precision_list.append(precision)\n",
    "print(\"Precision/PPV: \", precision)\n",
    "recall = TP / (TP + FN + epsilon)\n",
    "recall_list.append(recall)\n",
    "print (\"Recall/Sensitivity: \", recall)\n",
    "NPV = TN / (TN + FN + epsilon)\n",
    "NPV_list.append(NPV)\n",
    "print(\"NPV: \", NPV)\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN + epsilon)\n",
    "accuracy_list.append(accuracy)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "specificity = TN / (TN + FP + epsilon)\n",
    "specificity_list.append(specificity)\n",
    "print(\"Specificity: \", specificity)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall + epsilon)\n",
    "f1_score_list.append(f1_score)\n",
    "print(\"F1 Score: \", f1_score)\n",
    "\n",
    "\n",
    "print(\"Completed at: \", datetime.datetime.now().strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "eucl_err = [29.192664806593392,30.417407798028098,54.13088229772116, 30.169575240273996]\n",
    "ETT_err = [16.936977627484584, 17.40249039641634, 17.95239513280611, 18.22531862299226]\n",
    "precision_list = [0.963414634028852,  0.9390243901293873, 0.926829268179655, 0.9878048779283165]\n",
    "recall_list = [0.963414634028852, 0.9624999998796876,  0.9743589742340566, 0.9204545453499484]\n",
    "NPV_list = [0.8888888885596707, 0.8888888885596707, 0.9259259255829904, 0.7407407404663923]\n",
    "accuracy_list = [0.944954128353674, 0.9266055045021463, 0.9266055045021463, 0.9266055045021463]\n",
    "specificity_list = [ 0.8888888885596707, 0.8275862066111771, 0.8064516126430801, 0.9523809519274377]\n",
    "f1_score_list = [ 0.963414629028852,0.9506172788340193, 0.949999994884375, 0.952941171364706 ]\n",
    "pck_list = [0.991,0.991,0.987, 0.989]\n",
    "\n",
    "stat_list = [precision_list, recall_list, NPV_list, accuracy_list, specificity_list, f1_score_list, pck_list, eucl_err , ETT_err]\n",
    "\n",
    "for list_ in stat_list:\n",
    "    std_dev = statistics.stdev(list_)\n",
    "    avg = sum(list_)/len(list_)\n",
    "    print(f\"The standard deviation of list is: {std_dev:.3f}\")\n",
    "    print(f\"The average of list is: {avg:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset = ConcatDataset([dataset_train, dataset_test])\n",
    "\n",
    "# K-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for train_idx, val_idx in kf.split(combined_dataset):\n",
    "\n",
    "    val_data = [combined_dataset[i] for i in tqdm(val_idx)]\n",
    "\n",
    "    global data_loader_val\n",
    "    data_loader_val = DataLoader(val_data, batch_size=1, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(2)\n",
    "checkpoint_path = r\"D:\\ET_Tube\\CheXpert-v1.0\\checkpoints\\checkpoint__5_8.pt\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "\n",
    "    print(\"Loading model...\")\n",
    " \n",
    "    test_batch_size = 1\n",
    "    #data_loader_test = DataLoader(dataset_test, batch_size=test_batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    iterator = iter(data_loader_test)\n",
    "\n",
    "    total_mean_distance_error = 0\n",
    "    total_mean_ETT_distance_error = 0\n",
    "    ETT_distance_error_list = []\n",
    "    total_point_error_list = []\n",
    "    example_error_list = []\n",
    "    pred_image_file_list = []\n",
    "    total_number_of_predictions = 0\n",
    "    no_preds_count = 0\n",
    "    total_attempts = 0\n",
    "    batch_count = 0\n",
    "    correct_predictions = 0\n",
    "    incorrect_predictions = 0\n",
    "    correct_real = 0\n",
    "    incorrect_real = 0\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for item in iterator:\n",
    "        \n",
    "        try:\n",
    "            images, targets = item\n",
    "            \n",
    "            images = [image.to(device) for image in images]\n",
    "        except StopIteration as e:\n",
    "            print(\"StopIteration exception handled at batch: \", batch_count)\n",
    "            \n",
    "        #model.to(device)            \n",
    "        #model.eval()\n",
    "        output = model(images)   \n",
    "        #print(\"output: \", output)\n",
    "\n",
    "        batch_count += 1 \n",
    "\n",
    "        for prediction_number in range(len(images)):\n",
    "            \n",
    "            real_keypoints = [] #list of keypoints for each image in batch\n",
    "\n",
    "            #unpacking the targets, this is a pain but works to remove the 0/1 visibility dim (which we do not need because all keypoints are visible)\n",
    "            for kps in targets[prediction_number]['keypoints']:\n",
    "                real_keypoints.append([list(map(int, kp[:2])) for kp in kps])\n",
    "            \n",
    "            distance_real = calc_distance(real_keypoints[0][0], real_keypoints[0][1])\n",
    "            \n",
    "        \n",
    "            \n",
    "            real_bboxes = targets[prediction_number]['boxes'].int().tolist()\n",
    "            \n",
    "            #permute(1,2,0) converts the tensor to numpy array. The tensor is in the format (C, H, W) and numpy array is in the format (H, W, C).\n",
    "            #detach().cpu().numpy() detaches the tensor from the graph and converts it to numpy array and moves it to CPU.\n",
    "            image = (images[prediction_number].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)  \n",
    "            \n",
    "            scores = output[prediction_number]['scores'].detach().cpu().numpy()\n",
    "            if len(scores) == 0:\n",
    "                print(\"No keypoints found at image: \", prediction_number)\n",
    "                no_preds_count += 1\n",
    "                break\n",
    "\n",
    "\n",
    "            high_scores_idxs = np.where(scores > 0.1)[0].tolist() # Indexes of boxes with scores > 0.1\n",
    "            #print(\"High Score idxs: \", high_scores_idxs)\n",
    "            #print(\"Raw NMS Boxes len: \", len(output[0]['boxes'][high_scores_idxs]))\n",
    "            #print(\"Raw NMS scores len: \", len(output[0]['scores'][high_scores_idxs]))\n",
    "            post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n",
    "            #post_nms_idxs = 0\n",
    "            #print(\"-----------Post NMS idxs:-----------\", post_nms_idxs)\n",
    "\n",
    "            #Making images based on keypoints_scores, instead of bbox scores now\n",
    "            #print(\"Raw Keypoint scores: \", output[prediction_number]['keypoints_scores'])\n",
    "            keypoint_scores = output[prediction_number]['keypoints_scores'][post_nms_idxs]\n",
    "            #print(\"Keypoint scores: \", keypoint_scores)\n",
    "            #print(\"Keypoint scores: \", keypoint_scores)\n",
    "            score_idx = get_best_keypoints(keypoint_scores)\n",
    "            #print(\"Best Keypoints IDX: \", score_idx)\n",
    "            #score_idx = 0\n",
    "            pred_keypoints = []\n",
    "            keypoints = output[prediction_number]['keypoints'][score_idx].detach().cpu().numpy()\n",
    "        \n",
    "            for kp in keypoints:\n",
    "                kp = list(map(int, kp)) #convert (x,y) coords in each keypoint to int\n",
    "                pred_keypoints.append(kp[:2])\n",
    "            pred_keypoints = [pred_keypoints] #convert to list of lists to match real_keypoints format\n",
    "            #print(\"Pred keypoints: \", pred_keypoints)\n",
    "\n",
    "            distance_pred = calc_distance(pred_keypoints[0][0], pred_keypoints[0][1])\n",
    "            distance_error = abs(distance_pred - distance_real)\n",
    "            \n",
    "            #print(\"Real ETT to Carina:\", distance_real/50)\n",
    "            #print(\"Prediencted ETT to Carina:\", distance_pred/50)\n",
    "            #print(\"ETT to Carina Distance error:\" , distance_error/50)\n",
    "\n",
    "            if ((distance_real/50) <= 3) or ((distance_real/50) >= 7):\n",
    "                incorrect_real += 1\n",
    "            else:\n",
    "                correct_real += 1\n",
    "\n",
    "            if ((distance_pred/50) <= 3) or ((distance_pred/50) >= 7):\n",
    "                incorrect_predictions += 1\n",
    "            else:\n",
    "                correct_predictions += 1\n",
    "\n",
    "            if (((distance_real/50) <= 3) or ((distance_real/50) >= 7)) and (((distance_pred/50) <= 3) or ((distance_pred/50) >= 7)):\n",
    "                TN +=1\n",
    "                #print(\"TN\")\n",
    "            elif (((distance_real/50) <= 3) or ((distance_real/50) >= 7)) and (((distance_pred/50) > 3) or ((distance_pred/50) < 7)):\n",
    "                FN +=1\n",
    "                #print(\"FN\")\n",
    "            elif (((distance_real/50) > 3) or ((distance_real/50) < 7)) and (((distance_pred/50) <= 3) or ((distance_pred/50) >= 7)):\n",
    "                FP +=1\n",
    "                #print(\"FP\")\n",
    "            elif (((distance_real/50) > 3) or ((distance_real/50) < 7)) and (((distance_pred/50) > 3) or ((distance_pred/50) < 7)):\n",
    "                TP +=1\n",
    "                #print(\"TP\")\n",
    "\n",
    "            bboxes = []\n",
    "            \n",
    "            #print(\"Boxes:\", output[prediction_number]['boxes'][[0]][[0]])\n",
    "            #print(\"Scores:\", output[prediction_number]['scores'])\n",
    "            for bbox in  output[prediction_number]['boxes'][[score_idx]][[0]].detach().cpu().numpy():\n",
    "                bboxes.append(list(map(int, bbox.tolist())))\n",
    "            \n",
    "            \n",
    "            if (len(bboxes) == 0):\n",
    "                print(\"No bounding boxes found at image: \", prediction_number)\n",
    "                no_preds_count += 1 #count preds with no bounding box at given threshold  \n",
    "            else:\n",
    "                total_number_of_predictions += 1    \n",
    "                example_error, point_error = calc_mean_distance_error(real_keypoints, pred_keypoints)\n",
    "                total_mean_distance_error += example_error\n",
    "                \n",
    "            \n",
    "            total_attempts += 1\n",
    "            \n",
    "            example_point_error = [pt for pt in point_error]\n",
    "            total_point_error_list.append(example_point_error)\n",
    "            example_error_list.append(example_error)\n",
    "            ETT_distance_error_list.append(distance_error)\n",
    "            #print(\"Example error: \", example_error)\n",
    "            \n",
    "            save_img_path = f\"F:\\ET_Tube\\saved_img_preds\\prediction_{str(prediction_number)}_{str(batch_count)}.jpg\"\n",
    "            pred_image_file_list.append(save_img_path)\n",
    "            visualize(image, bboxes, pred_keypoints, image_original=image, keypoints_original=real_keypoints, bboxes_original=real_bboxes, save=False, save_path = save_img_path, display=False)\n",
    "            plt.savefig(save_img_path)\n",
    "            #break\n",
    "        #break\n",
    "\n",
    "    print(\"Placement Correct predictions: \", correct_predictions)\n",
    "    print(\"Placement Incorrect predictions: \", incorrect_predictions)\n",
    "    print(\"Placement Correct real: \", correct_real)\n",
    "    print(\"Placement Incorrect real: \", incorrect_real)\n",
    "\n",
    "\n",
    "    total_mean_distance_error /= total_number_of_predictions\n",
    "    print(\"Total mean Euclidean distance error: \", total_mean_distance_error)\n",
    "    total_mean_ETT_distance_error = sum(ETT_distance_error_list)/len(ETT_distance_error_list)\n",
    "    print(\"Total ETT distance error: \", total_mean_ETT_distance_error)\n",
    "    print(\"No predictions count: \", no_preds_count)\n",
    "    print(\"Total attempts: \", total_attempts)\n",
    "    pck_threshold_large = 50\n",
    "    pck_large = calculate_example_pck(total_point_error_list, threshold=pck_threshold_large)\n",
    "    print(f\"Fraction of Correct Keypoints: {calculate_example_pck(total_point_error_list, threshold=pck_threshold_large):0.3f}, at a threshold of {pck_threshold_large} pixels.\")\n",
    "\n",
    "    print(\"TP: \", TP)\n",
    "    print(\"FP: \", FP)\n",
    "    print(\"TN: \", TN)\n",
    "    print(\"FN: \", FN)\n",
    "\n",
    "\n",
    "    precision = TP / (TP + FP)\n",
    "    print(\"Precision/PPV: \", precision)\n",
    "    recall = TP / (TP + FN)\n",
    "    print (\"Recall/Sensitivity: \", recall)\n",
    "    NPV = TN / (TN + FN)\n",
    "    NPV_list.append(NPV)\n",
    "    print(\"NPV: \", NPV)\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    accuracy_list.append(accuracy)\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    specificity = TN / (TN + FP)\n",
    "    specificity_list.append(specificity)\n",
    "    print(\"Specificity: \", specificity)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    f1_score_list.append(f1_score)\n",
    "    print(\"F1 Score: \", f1_score)\n",
    "\n",
    "\n",
    "    print(\"Completed at: \", datetime.datetime.now().strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Placement Correct predictions:  32\n",
      "Placement Incorrect predictions:  11\n",
      "Placement Correct real:  29\n",
      "Placement Incorrect real:  14\n",
      "Total mean Euclidean distance error:  31.582414769047688\n",
      "Total ETT distance error:  36.72768436557167\n",
      "No predictions count:  0\n",
      "Total attempts:  43\n",
      "Fraction of Correct Keypoints: 0.915, at a threshold of 50 pixels.\n",
      "TP:  27\n",
      "FP:  2\n",
      "TN:  9\n",
      "FN:  5\n",
      "Precision/PPV:  0.9310344824375743\n",
      "Recall/Sensitivity:  0.8437499997363281\n",
      "NPV:  0.6428571423979591\n",
      "Accuracy:  0.8372093021308815\n",
      "Specificity:  0.8181818174380164\n",
      "F1 Score:  0.8852458963611932\n",
      "Completed at:  09:50:18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "\n",
    "    print(\"Loading model...\")\n",
    " \n",
    "    test_batch_size = 1\n",
    "    #data_loader_test = DataLoader(dataset_test, batch_size=test_batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    iterator = iter(data_loader_test)\n",
    "\n",
    "    total_mean_distance_error = 0\n",
    "    total_mean_ETT_distance_error = 0\n",
    "    ETT_distance_error_list = []\n",
    "    total_point_error_list = []\n",
    "    example_error_list = []\n",
    "    pred_image_file_list = []\n",
    "    total_number_of_predictions = 0\n",
    "    no_preds_count = 0\n",
    "    total_attempts = 0\n",
    "    batch_count = 0\n",
    "    correct_predictions = 0\n",
    "    incorrect_predictions = 0\n",
    "    correct_real = 0\n",
    "    incorrect_real = 0\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for item in iterator:\n",
    "        try:\n",
    "            images, targets = item\n",
    "            \n",
    "            images = [image.to(device) for image in images]\n",
    "        except StopIteration as e:\n",
    "            print(\"StopIteration exception handled at batch: \", batch_count)\n",
    "            \n",
    "        #model.to(device)            \n",
    "        #model.eval()\n",
    "        output = model(images)   \n",
    "        batch_count += 1 \n",
    "\n",
    "        for prediction_number in range(len(images)):\n",
    "            \n",
    "            real_keypoints = [] #list of keypoints for each image in batch\n",
    "\n",
    "            #unpacking the targets, this is a pain but works to remove the 0/1 visibility dim (which we do not need because all keypoints are visible)\n",
    "            for kps in targets[prediction_number]['keypoints']:\n",
    "                real_keypoints.append([list(map(int, kp[:2])) for kp in kps])\n",
    "            distance_real = calc_distance(real_keypoints[0][0], real_keypoints[0][1])\n",
    "            #print(\"Real keypoints: \", real_keypoints)\n",
    "        \n",
    "            \n",
    "            real_bboxes = targets[prediction_number]['boxes'].int().tolist()\n",
    "            \n",
    "            #permute(1,2,0) converts the tensor to numpy array. The tensor is in the format (C, H, W) and numpy array is in the format (H, W, C).\n",
    "            #detach().cpu().numpy() detaches the tensor from the graph and converts it to numpy array and moves it to CPU.\n",
    "            image = (images[prediction_number].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)  \n",
    "            \n",
    "            scores = output[prediction_number]['scores'].detach().cpu().numpy()\n",
    "            if len(scores) == 0:\n",
    "                print(\"No keypoints found at image: \", prediction_number)\n",
    "                no_preds_count += 1\n",
    "                break\n",
    "\n",
    "\n",
    "            high_scores_idxs = np.where(scores > 0.1)[0].tolist() # Indexes of boxes with scores > 0.1\n",
    "            #print(\"High Score idxs: \", high_scores_idxs)\n",
    "            #print(\"Raw NMS Boxes len: \", len(output[0]['boxes'][high_scores_idxs]))\n",
    "            #print(\"Raw NMS scores len: \", len(output[0]['scores'][high_scores_idxs]))\n",
    "            try:\n",
    "                #print(\"Box idx:\", output[0]['boxes'][high_scores_idxs])\n",
    "                #print(\"Box scores:\", output[0]['scores'][high_scores_idxs])\n",
    "                post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n",
    "            except:\n",
    "                print(\"NMS exception handled at batch: \", batch_count)\n",
    "                post_nms_idxs = 0\n",
    "                continue\n",
    "            \n",
    "            #print(\"-----------Post NMS idxs:-----------\", post_nms_idxs)\n",
    "\n",
    "            #Making images based on keypoints_scores, instead of bbox scores now\n",
    "            #print(\"Raw Keypoint scores: \", output[prediction_number]['keypoints_scores'])\n",
    "            keypoint_scores = output[prediction_number]['keypoints_scores'][post_nms_idxs]\n",
    "            #print(\"Keypoint scores: \", keypoint_scores)\n",
    "            #print(\"Keypoint scores: \", keypoint_scores)\n",
    "            score_idx = get_best_keypoints(keypoint_scores)\n",
    "            #print(\"Best Keypoints IDX: \", score_idx)\n",
    "            #score_idx = 0\n",
    "            pred_keypoints = []\n",
    "            keypoints = output[prediction_number]['keypoints'][score_idx].detach().cpu().numpy()\n",
    "        \n",
    "            for kp in keypoints:\n",
    "                kp = list(map(int, kp)) #convert (x,y) coords in each keypoint to int\n",
    "                pred_keypoints.append(kp[:2])\n",
    "\n",
    "\n",
    "            if len(pred_keypoints) != 0:\n",
    "                pred_keypoints = [pred_keypoints] #convert to list of lists to match real_keypoints format\n",
    "                #print(\"Pred keypoints: \", pred_keypoints)\n",
    "            \n",
    "                distance_pred = calc_distance(pred_keypoints[0][0], pred_keypoints[0][1])\n",
    "                distance_error = abs(distance_pred - distance_real)\n",
    "            \n",
    "            #print(\"Real ETT to Carina:\", distance_real/50)\n",
    "            #print(\"Prediencted ETT to Carina:\", distance_pred/50)\n",
    "            #print(\"ETT to Carina Distance error:\" , distance_error/50)\n",
    "\n",
    "                if ((distance_real/50) <= 3) or ((distance_real/50) >= 7):\n",
    "                    incorrect_real += 1\n",
    "                else:\n",
    "                    correct_real += 1\n",
    "\n",
    "                if ((distance_pred/50) <= 3) or ((distance_pred/50) >= 7):\n",
    "                    incorrect_predictions += 1\n",
    "                else:\n",
    "                    correct_predictions += 1\n",
    "\n",
    "                if (((distance_real/50) <= 3) or ((distance_real/50) >= 7)) and (((distance_pred/50) <= 3) or ((distance_pred/50) >= 7)):\n",
    "                    TN +=1\n",
    "                    #print(\"TN\")\n",
    "                elif (((distance_real/50) <= 3) or ((distance_real/50) >= 7)) and (((distance_pred/50) > 3) or ((distance_pred/50) < 7)):\n",
    "                    FN +=1\n",
    "                    #print(\"FN\")\n",
    "                elif (((distance_real/50) > 3) or ((distance_real/50) < 7)) and (((distance_pred/50) <= 3) or ((distance_pred/50) >= 7)):\n",
    "                    FP +=1\n",
    "                    #print(\"FP\")\n",
    "                elif (((distance_real/50) > 3) or ((distance_real/50) < 7)) and (((distance_pred/50) > 3) or ((distance_pred/50) < 7)):\n",
    "                    TP +=1\n",
    "                    #print(\"TP\")\n",
    "\n",
    "                bboxes = []\n",
    "                    \n",
    "                #print(\"Boxes:\", output[prediction_number]['boxes'][[0]][[0]])\n",
    "                #print(\"Scores:\", output[prediction_number]['scores'])\n",
    "                for bbox in  output[prediction_number]['boxes'][[score_idx]][[0]].detach().cpu().numpy():\n",
    "                    bboxes.append(list(map(int, bbox.tolist())))\n",
    "                \n",
    "                \n",
    "                if (len(bboxes) == 0):\n",
    "                    print(\"No bounding boxes found at image: \", prediction_number)\n",
    "                    no_preds_count += 1 #count preds with no bounding box at given threshold  \n",
    "                else:\n",
    "                    total_number_of_predictions += 1    \n",
    "                    example_error, point_error = calc_mean_distance_error(real_keypoints, pred_keypoints)\n",
    "                    total_mean_distance_error += example_error\n",
    "                \n",
    "            \n",
    "            \n",
    "                example_point_error = [pt for pt in point_error]\n",
    "                total_point_error_list.append(example_point_error)\n",
    "                example_error_list.append(example_error)\n",
    "                ETT_distance_error_list.append(distance_error)\n",
    "                #print(\"Example error: \", example_error)\n",
    "                \n",
    "            \n",
    "                #save_img_path = f\"F:\\ET_Tube\\saved_img_preds\\prediction_{str(prediction_number)}_{str(batch_count)}.jpg\"\n",
    "                save_img_path = f\"F:\\ET_Tube\\saved_img_preds\\prediction_{str(prediction_number)}_{str(batch_count)}.jpg\"\n",
    "                pred_image_file_list.append(save_img_path)\n",
    "                visualize(image, bboxes, pred_keypoints, image_original=image, keypoints_original=real_keypoints, bboxes_original=real_bboxes, save=False, save_path = save_img_path, display=False)\n",
    "                plt.savefig(save_img_path)\n",
    "\n",
    "            total_attempts += 1\n",
    "\n",
    "    print(\"Placement Correct predictions: \", correct_predictions)\n",
    "    print(\"Placement Incorrect predictions: \", incorrect_predictions)\n",
    "    print(\"Placement Correct real: \", correct_real)\n",
    "    print(\"Placement Incorrect real: \", incorrect_real)\n",
    "\n",
    "    if total_number_of_predictions != 0:\n",
    "        total_mean_distance_error /= total_number_of_predictions\n",
    "    print(\"Total mean Euclidean distance error: \", total_mean_distance_error)\n",
    "    if len(ETT_distance_error_list) != 0:\n",
    "        total_mean_ETT_distance_error = sum(ETT_distance_error_list)/len(ETT_distance_error_list)\n",
    "    print(\"Total ETT distance error: \", total_mean_ETT_distance_error)\n",
    "    print(\"No predictions count: \", no_preds_count)\n",
    "    print(\"Total attempts: \", total_attempts)\n",
    "\n",
    "    pck_threshold_large = 50\n",
    "    pck = calculate_example_pck(total_point_error_list, threshold=pck_threshold_large)\n",
    "    print(f\"Fraction of Correct Keypoints: {pck:0.3f}, at a threshold of {pck_threshold_large} pixels.\")\n",
    "    pck_list.append(pck)\n",
    "\n",
    "    print(\"TP: \", TP)\n",
    "    print(\"FP: \", FP)\n",
    "    print(\"TN: \", TN)\n",
    "    print(\"FN: \", FN)\n",
    "\n",
    "    epsilon = 1e-8 #to avoid division by 0 errors \n",
    "\n",
    "    precision = TP / (TP + FP + epsilon)\n",
    "    precision_list.append(precision)\n",
    "    print(\"Precision/PPV: \", precision)\n",
    "    recall = TP / (TP + FN + epsilon)\n",
    "    recall_list.append(recall)\n",
    "    print (\"Recall/Sensitivity: \", recall)\n",
    "    NPV = TN / (TN + FN + epsilon)\n",
    "    NPV_list.append(NPV)\n",
    "    print(\"NPV: \", NPV)\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN + epsilon)\n",
    "    accuracy_list.append(accuracy)\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    specificity = TN / (TN + FP + epsilon)\n",
    "    specificity_list.append(specificity)\n",
    "    print(\"Specificity: \", specificity)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall + epsilon)\n",
    "    f1_score_list.append(f1_score)\n",
    "    print(\"F1 Score: \", f1_score)\n",
    "\n",
    "\n",
    "    print(\"Completed at: \", datetime.datetime.now().strftime(\"%H:%M:%S\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "TP, FP, FN, TN = 0, 0, 0, 0\n",
    "\n",
    "confusion_matrix = [\n",
    "    [TP, FP],\n",
    "    [FN, TN]\n",
    "]\n",
    "\n",
    "# Create a heatmap using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.set(font_scale=1.4)  # Increase font size\n",
    "ax = sns.heatmap(confusion_matrix, annot=True, cmap=\"YlGnBu\", fmt=\"d\", annot_kws={\"size\": 16})\n",
    "ax.set_ylim(sorted(ax.get_xlim(), reverse=True))\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Predicted labels')\n",
    "ax.set_ylabel('True labels')\n",
    "ax.set_title('Confusion Matrix for ETT Placement')\n",
    "\n",
    "# Set tick labels\n",
    "ax.xaxis.set_ticklabels(['Correct ETT Placement', 'Incorrect ETT Placement'])\n",
    "ax.yaxis.set_ticklabels(['Correct ETT Placement', 'Incorrect ETT Placement'])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"precision = TP / (TP + FP)\n",
    "print(\"Precision/PPV: \", precision)\n",
    "recall = TP / (TP + FN)\n",
    "print (\"Recall/Sensitivity: \", recall)\n",
    "NPV = TN / (TN + FN)\n",
    "print(\"NPV: \", NPV)\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "specificity = TN / (TN + FP)\n",
    "print(\"Specificity: \", specificity)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "print(\"F1 Score: \", f1_score)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pck_large' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[141], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(example_error_list, columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mExample Error\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     17\u001b[0m \u001b[39m#df['pck_small'] = pck_small\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mpck_large\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m pck_large\n\u001b[0;32m     19\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m pred_image_file_list\n\u001b[0;32m     20\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mfile\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m f: get_thumbnail(f))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pck_large' is not defined"
     ]
    }
   ],
   "source": [
    "def get_thumbnail(path):\n",
    "    i = Image.open(path)\n",
    "    i.thumbnail((500, 250), Image.Resampling.LANCZOS)\n",
    "    return i\n",
    "\n",
    "def image_base64(im):\n",
    "    if isinstance(im, str):\n",
    "        im = get_thumbnail(im)\n",
    "    with BytesIO() as buffer:\n",
    "        im.save(buffer, 'jpeg')\n",
    "        return base64.b64encode(buffer.getvalue()).decode()\n",
    "\n",
    "def image_formatter(im):\n",
    "    return f'<img src=\"data:image/jpeg;base64,{image_base64(im)}\">'\n",
    "\n",
    "df = pd.DataFrame(example_error_list, columns=['Example Error'])\n",
    "df['pck_small'] = pck_small\n",
    "df['pck_large'] = pck_large\n",
    "df['file'] = pred_image_file_list\n",
    "df['image'] = df.file.map(lambda f: get_thumbnail(f))\n",
    "df.head()\n",
    "\n",
    "HTML(df[['Example Error', 'image']].to_html(formatters={'image': image_formatter}, escape=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workbook = Workbook()\n",
    "worksheet = workbook.active\n",
    "\n",
    "# resize cells\n",
    "for row in range(2, len(df['file'])+2):\n",
    "    for col in range(1,2):\n",
    "        worksheet.row_dimensions[row].height = 400\n",
    "        col_letter = get_column_letter(col)\n",
    "        worksheet.column_dimensions[col_letter].width = 150\n",
    "        \n",
    "for col in range(2,50):\n",
    "    col_letter = get_column_letter(col)\n",
    "    worksheet.column_dimensions[col_letter].width = 30\n",
    "         \n",
    "# insert images\n",
    "for index, image in enumerate(df['file']):\n",
    "    if index == 0:\n",
    "        worksheet.cell(row=index+1, column=1, value=\"Image\")\n",
    "        worksheet.add_image(ExcelImage(image), anchor='A'+str(index+2))\n",
    "    else:\n",
    "        worksheet.add_image(ExcelImage(image), anchor='A'+str(index+2))\n",
    "\n",
    "excel_cell_values = []\n",
    "for excel_cell_value in df['Example Error']:\n",
    "    excel_cell_values.append(excel_cell_value)\n",
    "\n",
    "for index, excel_cell_value in enumerate(excel_cell_values):\n",
    "    if index == 0:\n",
    "        worksheet.cell(row=index+1, column=2, value=\"Example Error\")\n",
    "        worksheet.cell(row=index+2, column=2, value=excel_cell_value)\n",
    "    else:\n",
    "        worksheet.cell(row=index+2, column=2, value=excel_cell_value)\n",
    "\n",
    "# Find the last column\n",
    "last_column = worksheet.max_column\n",
    "\n",
    "# Add values to the last column\n",
    "worksheet.cell(row=1, column=last_column+1).value = f'Total Mean Euclidean Distance Error: {total_mean_distance_error}'\n",
    "worksheet.cell(row=2, column=last_column+1).value = f'No predictions count: {no_preds_count}'\n",
    "worksheet.cell(row=6, column=last_column+1).value = f'Fraction of Correct Keypoints at threshold of {pck_threshold_small} pixels: {pck_small}'\n",
    "worksheet.cell(row=7, column=last_column+1).value = f'Fraction of Correct Keypoints at threshold of {pck_threshold_large} pixels: {pck_large}'\n",
    "\n",
    "# save workbook\n",
    "workbook.save(workbook_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_predict(image_file):\n",
    "    raw_img = cv2.imread(image_file)\n",
    "    aspect_ratio = raw_img.shape[0]/raw_img.shape[1]\n",
    "\n",
    "    if aspect_ratio < 1.5 and aspect_ratio > 0.5:\n",
    "        raw_img = cv2.resize(raw_img, (456, 456))\n",
    "        test_img = np.moveaxis(raw_img, -1, 0)\n",
    "        test_img= np.expand_dims(test_img, axis=0)\n",
    "        test_img = torch.from_numpy(test_img).float().to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.to(device)            \n",
    "            model.eval()\n",
    "            output = model(test_img)\n",
    "        \n",
    "        scores = output[0]['keypoints_scores']\n",
    "        score_idx = get_best_keypoints(scores)\n",
    "\n",
    "        keypoints = output[0]['keypoints'][score_idx].detach().cpu().numpy()\n",
    "        for idx, kp in enumerate(keypoints):\n",
    "            current_keypoint = kp[:2].astype(int)\n",
    "            raw_img = cv2.circle(raw_img, current_keypoint, 1, (255,255,0), 10)\n",
    "            image_original = cv2.putText(raw_img, \" \" + keypoints_classes_ids2names[idx], current_keypoint, cv2.FONT_HERSHEY_PLAIN, 2, (255,0,0), 2, cv2.LINE_AA)\n",
    "\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.imshow(raw_img)\n",
    "    else:\n",
    "        print(f\"Image aspect ratio is {aspect_ratio:0.2F} it but be between 0.5 - 1.5\")\n",
    "\n",
    "        \n",
    "#test_img = \"/mnt/c/Users/nprim/Downloads/F1.jpg\"\n",
    "#process_and_predict(test_img) # random picture from the internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
