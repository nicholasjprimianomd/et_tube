{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 22\n",
    "\n",
    "train_model = True\n",
    "load_weights =False\n",
    "make_labels = False\n",
    "\n",
    "\n",
    "import datetime\n",
    "num_keypoints = 2\n",
    "save_path = rf'F:\\ET_Tube\\CheXpert-v1.0\\weights\\test_weights_{datetime.datetime.now().strftime(\"%H_%M_%S\")}.pth'\n",
    "workbook_path = rf'F:\\ET_Tube\\CheXpert-v1.0\\predictions_{datetime.datetime.now().strftime(\"%H_%M_%S\")}.xlsx'\n",
    "\n",
    "batch_size = 1\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYPOINTS_FOLDER_TRAIN = fr\"C:\\Users\\nprim\\Downloads\\ET_TUBE.v{num}i.yolov7pytorch\\train\"\n",
    "KEYPOINTS_FOLDER_TEST =  fr\"C:\\Users\\nprim\\Downloads\\ET_TUBE.v{num}i.yolov7pytorch\\test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import json, cv2, numpy as np, matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision.models.detection import KeypointRCNN\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import transforms, utils, engine, train\n",
    "from utils import collate_fn\n",
    "from engine import train_one_epoch, evaluate\n",
    "import random\n",
    "import base64\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from IPython.display import HTML\n",
    "from torchsummary import summary\n",
    "import math\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.drawing.image import Image as ExcelImage\n",
    "from openpyxl.utils import get_column_letter\n",
    "import glob\n",
    "from natsort import natsorted\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "print(\"Using torch\", torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if make_labels:\n",
    "    import json\n",
    "    import zipfile\n",
    "    path_to_zip_file = fr\"C:\\Users\\nprim\\Downloads\\ET_TUBE.v{num}i.yolov7pytorch.zip\"\n",
    "    BASE = fr\"C:\\Users\\nprim\\Downloads\\ET_TUBE.v{num}i.yolov7pytorch\"\n",
    "    with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(BASE)\n",
    "\n",
    "\n",
    "    #SET = [\"/train\", \"/valid\", \"/test\"]\n",
    "    SET = [\"/train\",  \"/test\"]\n",
    "    #SET = [\"/train\", \"/valid\"]\n",
    "    #SET = [\"/train\"]\n",
    "    #SET = [\"/test\"]\n",
    "\n",
    "        \n",
    "    keypoint_names =  ['Carina', 'ET']\n",
    "\n",
    "    def dump2json(bboxes, keypoints_sorted, file_json):\n",
    "        annotations = {}\n",
    "        annotations['bboxes'], annotations['keypoints'] = bboxes, keypoints_sorted\n",
    "\n",
    "        with open(file_json, \"w\") as f:\n",
    "            json.dump(annotations, f)\n",
    "\n",
    "    def converter(file_labels, file_image, keypoint_names):\n",
    "\n",
    "        img = cv2.imread(file_image)\n",
    "        img_w, img_h = img.shape[1], img.shape[0]\n",
    "        \n",
    "        with open(file_labels) as f:\n",
    "            lines_txt = f.readlines()\n",
    "            lines = []\n",
    "            for line in lines_txt:\n",
    "                lines.append([int(line.split()[0])] + [round(float(el), 5) for el in line.split()[1:]])\n",
    "        \n",
    "        bboxes = []\n",
    "        keypoints = []\n",
    "\n",
    "        # Convert normalized coordinates to absolute coordinates\n",
    "        for line in lines:\n",
    "            # Number 0 is a class of rectangles related to bounding boxes.\n",
    "            if line[0] == 2:\n",
    "                x_c, y_c, w, h = round(line[1] * img_w), round(line[2] * img_h), round(line[3] * img_w), round(line[4] * img_h)\n",
    "                bboxes.append([round(x_c - w/2), round(y_c - h/2), round(x_c + w/2), round(y_c + h/2)])\n",
    "\n",
    "            elif line[0] == 0 or line[0] == 1: #append all other keypoints without class change\n",
    "                kp_id, x_c, y_c = line[0], round(line[1] * img_w), round(line[2] * img_h) \n",
    "                keypoints.append([kp_id, x_c, y_c])\n",
    "                \n",
    "\n",
    "        # iterating over each keypoint and looking to which bounding box it matches, dont need this for patellas\n",
    "        keypoints_sorted = [[[] for _ in keypoint_names] for _ in bboxes]\n",
    "\n",
    "        for kp in keypoints:\n",
    "            kp_id, kp_x, kp_y = kp[0], kp[1], kp[2]\n",
    "            for bbox_idx, bbox in enumerate(bboxes):\n",
    "                x1, y1, x2, y2 = bbox[0], bbox[1], bbox[2], bbox[3]\n",
    "                if x1 < kp_x < x2 and y1 < kp_y < y2:\n",
    "                    keypoints_sorted[bbox_idx][kp_id] = [kp_x, kp_y, 1] # All keypoints are visible\n",
    "                    \n",
    "        return bboxes, keypoints_sorted\n",
    "\n",
    "\n",
    "    for i in range(len(SET)):\n",
    "        IMAGES = BASE + SET[i] + \"/images\"\n",
    "        LABELS = BASE + SET[i] + \"/labels\"\n",
    "        ANNOTATIONS = BASE +  SET[i] + \"/annotations\"\n",
    "        \n",
    "        files_names = [file.split('.jpg')[0] for file in os.listdir(IMAGES)]\n",
    "\n",
    "        for file in tqdm((files_names), desc =f\"Set: {SET[i]}\"):\n",
    "        #for file in (files_names):\n",
    "            file_labels = os.path.join(LABELS, file + \".txt\")\n",
    "            file_image = os.path.join(IMAGES, file + \".jpg\")\n",
    "\n",
    "            #img = cv2.imread(file_image)\n",
    "            #if img.shape[0] != img.shape[1]:\n",
    "                #print(\"Non square image:\", file_image)\n",
    "                \n",
    "            bboxes, keypoints_sorted = converter(file_labels, file_image, keypoint_names)\n",
    "\n",
    "            for i in keypoints_sorted:\n",
    "                a,b = i\n",
    "                if (len(a) != 3 or len (b) != 3) :\n",
    "                    print(\"Error in file\", file)\n",
    "\n",
    "            if not os.path.exists(ANNOTATIONS):\n",
    "                os.makedirs(ANNOTATIONS)\n",
    "\n",
    "            dump2json(bboxes, keypoints_sorted, os.path.join(ANNOTATIONS, file + '.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 512\n",
    "\n",
    "def train_transform():\n",
    "    return A.Compose([\n",
    "        A.Sequential([\n",
    "            A.GaussNoise(var_limit=(50, 100.0), mean=0, per_channel=True, always_apply=False, p=0.5),\n",
    "            A.ISONoise(intensity=(1, 10), always_apply=False, p=0.5),\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.4, contrast_limit=0.4, p=0.5),\n",
    "            A.Rotate(p=0.5, limit=30, border_mode=cv2.BORDER_CONSTANT, value=0),\n",
    "            A.CLAHE(clip_limit=(1,4), p=1)\n",
    "            #A.Resize(IMG_SIZE, IMG_SIZE, p=1)\n",
    "        ], p=1)\n",
    "    ],\n",
    "    keypoint_params=A.KeypointParams(format='xy'), \n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bboxes_labels'])\n",
    "    )\n",
    "\n",
    "def test_transform():\n",
    "    return A.Compose([\n",
    "        A.Sequential([\n",
    "            A.CLAHE(clip_limit=(1,4), p=1)\n",
    "            #A.Resize(IMG_SIZE, IMG_SIZE, p=1)\n",
    "        ], p=1)\n",
    "    ],\n",
    "    keypoint_params=A.KeypointParams(format='xy'), \n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bboxes_labels'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, demo=False):                \n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.demo = demo # Use demo=True if you need transformed and original images (for example, for visualization purposes)\n",
    "        self.imgs_files = sorted(os.listdir(os.path.join(root, \"images\")))\n",
    "        self.annotations_files = sorted(os.listdir(os.path.join(root, \"annotations\")))\n",
    "\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root, \"images\", self.imgs_files[idx])\n",
    "        annotations_path = os.path.join(self.root, \"annotations\", self.annotations_files[idx])\n",
    "\n",
    "        img_original = cv2.imread(img_path)\n",
    "        img_original = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)        \n",
    "        \n",
    "        with open(annotations_path) as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "            bboxes_original = data['bboxes']\n",
    "            keypoints_original = data['keypoints']\n",
    "                          \n",
    "            # All objects are patellas, so we can use the same label for all objects\n",
    "            bboxes_labels_original = ['ROI' for _ in bboxes_original]  \n",
    "\n",
    "\n",
    "\n",
    "            if self.transform:   \n",
    "                # Converting keypoints from [x,y,visibility]-format to [x, y]-format + Flattening nested list of keypoints            \n",
    "                # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "                # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]], where each keypoint is in [x, y]-format            \n",
    "                # Then we need to convert it to the following list:\n",
    "                # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2]\n",
    "                \n",
    "                keypoints_original_flattened = [el[0:2] for kp in keypoints_original for el in kp]\n",
    "\n",
    "                # Apply augmentations\n",
    "                transformed = self.transform(image=img_original, bboxes=bboxes_original, bboxes_labels=bboxes_labels_original, keypoints=keypoints_original_flattened)\n",
    "                img = transformed['image']\n",
    "                bboxes = transformed['bboxes']\n",
    "                \n",
    "                # Unflattening list transformed['keypoints']\n",
    "                # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "                # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2], where each keypoint is in [x, y]-format\n",
    "                # Then we need to convert it to the following list:\n",
    "                # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]]\n",
    "                \n",
    "                #keypoints_transformed_unflattened = np.reshape(np.array(transformed['keypoints']), (-1,3,2)).tolist()\n",
    "\n",
    "                keypoints_transformed_unflattened = [transformed['keypoints']]\n",
    "\n",
    "\n",
    "                #keypoints_transformed_unflattened = np.reshape(np.array(transformed['keypoints']), (-1,3,2)).tolist()\n",
    "                \n",
    "                # Converting transformed keypoints from [x, y]-format to [x,y,visibility]-format by appending original visibilities to transformed coordinates of keypoints\n",
    "                keypoints = []\n",
    "                for o_idx, obj in enumerate(keypoints_transformed_unflattened): # Iterating over objects\n",
    "                    obj_keypoints = []\n",
    "\n",
    "                    for k_idx, kp in enumerate(obj): # Iterating over keypoints in each object\n",
    "                        # kp - coordinates of keypoint\n",
    "                        # keypoints_original[o_idx][k_idx][2] - original visibility of keypoint\n",
    "                        obj_keypoints.append(np.array(kp).tolist() + [keypoints_original[o_idx][k_idx][2]])\n",
    "                    keypoints.append(obj_keypoints)\n",
    "        \n",
    "            else:\n",
    "                img, bboxes, keypoints = img_original, bboxes_original, keypoints_original\n",
    "                 \n",
    "            # Convert everything into a torch tensor        \n",
    "            bboxes = torch.as_tensor(bboxes, dtype=torch.float32)       \n",
    "            target = {}\n",
    "            target[\"boxes\"] = bboxes\n",
    "            target[\"labels\"] = torch.as_tensor([1 for _ in bboxes], dtype=torch.int64)\n",
    "            target[\"image_id\"] = torch.tensor([idx])\n",
    "            target[\"area\"] = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])\n",
    "            target[\"iscrowd\"] = torch.zeros(len(bboxes), dtype=torch.int64)\n",
    "\n",
    "            target[\"keypoints\"] = torch.as_tensor(keypoints, dtype=torch.float32) \n",
    "            \n",
    "            img = F.to_tensor(img)\n",
    "            \n",
    "            bboxes_original = torch.as_tensor(bboxes_original, dtype=torch.float32)\n",
    "            target_original = {}\n",
    "            target_original[\"boxes\"] = bboxes_original\n",
    "            target_original[\"labels\"] = torch.as_tensor([1 for _ in bboxes_original], dtype=torch.int64) # all objects are patellas\n",
    "            target_original[\"image_id\"] = torch.tensor([idx])\n",
    "            target_original[\"area\"] = (bboxes_original[:, 3] - bboxes_original[:, 1]) * (bboxes_original[:, 2] - bboxes_original[:, 0])\n",
    "            target_original[\"iscrowd\"] = torch.zeros(len(bboxes_original), dtype=torch.int64)\n",
    "            target_original[\"keypoints\"] = torch.as_tensor(keypoints_original, dtype=torch.float32)        \n",
    "            img_original = F.to_tensor(img_original)\n",
    "\n",
    "            if self.demo:\n",
    "                return img, target, img_original, target_original\n",
    "            else:\n",
    "                return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.imgs_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ClassDataset(KEYPOINTS_FOLDER_TEST, transform=train_transform(), demo=True)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "iterator = iter(data_loader)\n",
    "batch = next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints_classes_ids2names = {0: 'Carina' , 1: 'ET'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(image, bboxes, keypoints, image_original=None, bboxes_original=None, keypoints_original=None, save = False, save_path = None, display=True):\n",
    "    fontsize = 12\n",
    "\n",
    "    for bbox in bboxes:\n",
    "        start_point = (bbox[0], bbox[1])\n",
    "        end_point = (bbox[2], bbox[3])\n",
    "        image = cv2.rectangle(image.copy(), start_point, end_point, (0,255,0), 2)\n",
    "    \n",
    "    for kps in keypoints:\n",
    "        for idx, kp in enumerate(kps):\n",
    "            image = cv2.circle(image.copy(), tuple(kp), 1, (255,0,0), 10)\n",
    "            image = cv2.putText(image.copy(), \" \" + keypoints_classes_ids2names[idx], tuple(kp), cv2.FONT_HERSHEY_PLAIN, 1, (255,0,0), 1, cv2.LINE_AA)\n",
    "\n",
    "    if image_original is None and keypoints_original is None:\n",
    "        plt.figure(figsize=(20,20))\n",
    "        plt.imshow(image)\n",
    "        \n",
    "\n",
    "    else:\n",
    "        for bbox in bboxes_original:\n",
    "            start_point = (bbox[0], bbox[1])\n",
    "            end_point = (bbox[2], bbox[3])\n",
    "            image_original = cv2.rectangle(image_original.copy(), start_point, end_point, (255,255,0), 2)\n",
    "        \n",
    "        for kps in keypoints_original:\n",
    "            for idx, kp in enumerate(kps):\n",
    "                image_original = cv2.circle(image_original, tuple(kp), 1, (255,255,0), 10)\n",
    "                image_original = cv2.putText(image_original, \" \" + keypoints_classes_ids2names[idx], tuple(kp), cv2.FONT_HERSHEY_PLAIN, 1, (255,0,0), 1, cv2.LINE_AA)\n",
    "        \n",
    "\n",
    "        f, ax = plt.subplots(1, 2, figsize=(40, 20))\n",
    "        ax[0].set_title('Original image', fontsize=fontsize)\n",
    "        ax[1].set_title('Predicted image', fontsize=fontsize) # also augmented image\n",
    "    \n",
    "        ax[0].imshow(image_original)\n",
    "        ax[1].imshow(image)\n",
    "        \n",
    "        if(save):\n",
    "            #print(\"Saving image to: \", save_path)\n",
    "            plt.gcf().set_size_inches(5, 10)\n",
    "            plt.savefig(save_path, dpi=100)\n",
    "            plt.close()\n",
    "            \n",
    "        if(display == False):\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = (batch[0][0].permute(1,2,0).numpy() * 255).astype(np.uint8)\n",
    "bboxes = batch[1][0]['boxes'].detach().cpu().numpy().astype(np.int32).tolist()\n",
    "\n",
    "keypoints = []\n",
    "for kps in batch[1][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist():\n",
    "    keypoints.append([kp[:2] for kp in kps])\n",
    "\n",
    "image_original = (batch[2][0].permute(1,2,0).numpy() * 255).astype(np.uint8)\n",
    "bboxes_original = batch[3][0]['boxes'].detach().cpu().numpy().astype(np.int32).tolist()\n",
    "\n",
    "keypoints_original = []\n",
    "for kps in batch[3][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist():\n",
    "    keypoints_original.append([kp[:2] for kp in kps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(image, bboxes, keypoints, image_original, bboxes_original, keypoints_original, display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_keypoints, anchor_sizes, anchor_ratios, weights_path=None):\n",
    "    \n",
    "    backbone = torchvision.models.convnext_large(weights='DEFAULT').features\n",
    "    backbone.out_channels = 1536 # 1536 for convnext_large, 1024 for resnet50\n",
    "\n",
    "    \n",
    "    #anchor_generator = AnchorGenerator(sizes=((64, 128, 256),), aspect_ratios=((0.5, 0.83, 2, 1.2),)) # these need to be tweaked for the very large dataset\n",
    "    anchor_generator = AnchorGenerator(sizes=(anchor_sizes,), aspect_ratios=(anchor_ratios,))\n",
    "    \n",
    "    roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
    "                                                    output_size=7,\n",
    "                                                    sampling_ratio=2)\n",
    "    keypoint_roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
    "                                                             output_size=14,\n",
    "                                                             sampling_ratio=2)\n",
    "\n",
    "    model = KeypointRCNN(backbone,\n",
    "                          num_classes=2,\n",
    "                          rpn_anchor_generator=anchor_generator,\n",
    "                          box_roi_pool=roi_pooler,\n",
    "                          keypoint_roi_pool=keypoint_roi_pooler,\n",
    "                          num_keypoints=num_keypoints)\n",
    "    \n",
    "    if weights_path:\n",
    "        print(\"loading weights from: \", weights_path)\n",
    "        state_dict = torch.load(weights_path)\n",
    "        model.load_state_dict(state_dict)        \n",
    "        \n",
    "    return model\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = ClassDataset(KEYPOINTS_FOLDER_TRAIN, transform=train_transform(), demo=False)\n",
    "dataset_test = ClassDataset(KEYPOINTS_FOLDER_TEST, transform=test_transform(), demo=False)\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=10, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_epoch_error(model, data_loader_test, device):\n",
    "    with torch.inference_mode():\n",
    "        model.eval()\n",
    "        running_epoch_dist_error = 0\n",
    "        num_keypoint_predictions = 0\n",
    "        avg_dist_error = 0\n",
    "        for batch_idx, (images, targets) in enumerate(data_loader_test):\n",
    "            #print(f\"Processing batch {batch_idx}\")\n",
    "            #print(f\"Number of images: {len(images)}, Number of targets: {len(targets)}\")\n",
    "            \n",
    "            if not targets:\n",
    "                #print(\"No targets available for the current batch.\")\n",
    "                continue\n",
    "\n",
    "            images = list(image.to(device) for image in images)\n",
    "            predictions = model(images)\n",
    "            for i in range(len(predictions)):\n",
    "                for idx in range(num_keypoints):\n",
    "                    try:\n",
    "                        pred = predictions[i]['keypoints'][0][idx].detach().cpu().numpy().astype(np.int32)\n",
    "                        x1, y1, _ = predictions[i]['keypoints'][0][idx].detach().cpu().numpy().astype(np.int32)\n",
    "                        kp1 = (x1, y1)\n",
    "                        x2, y2, _ = targets[i]['keypoints'][0][idx].detach().cpu().numpy().astype(np.int32)\n",
    "                        kp2 = (x2, y2)\n",
    "                        num_keypoint_predictions += 1\n",
    "                        running_epoch_dist_error += calc_distance(kp1, kp2)\n",
    "                        avg_dist_error = running_epoch_dist_error / num_keypoint_predictions\n",
    "                    except:\n",
    "                        print(\"No prediction for val image.\")\n",
    "        print(\"Running Epoch Error: \", avg_dist_error)\n",
    "        score = -avg_dist_error\n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"%%capture\n",
    "\n",
    "if load_weights == True:    \n",
    "    model = get_model(num_keypoints = 2, anchor_sizes= (64, 128, 256), anchor_ratios=(0.5, 0.83, 2, 1.2) , weights_path=save_path) # load saved weights\n",
    "    print(\"Loaded weights from: \", save_path)\n",
    "else:\n",
    "    model = get_model(num_keypoints = 2)\n",
    "    \n",
    "model.to(device)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_distance(kp1, kp2):\n",
    "    x1, y1 = kp1\n",
    "    x2, y2 = kp2\n",
    "    return math.sqrt((x2 - x1)**2 + (y2 - y1)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def calc_epoch_error(model, data_loader_test, device):\n",
    "    with torch.inference_mode():\n",
    "        running_epoch_dist_error = 0\n",
    "        num_keypoint_predictions = 0\n",
    "        avg_dist_error = 0\n",
    "        for images, targets in data_loader_test:\n",
    "            images = list(image.to(device) for image in images)\n",
    "            predictions = model(images)\n",
    "            for i in range(len(predictions)):\n",
    "                    for idx in range(num_keypoints):\n",
    "                        try:\n",
    "                            pred = predictions[i]['keypoints'][0][idx].detach().cpu().numpy().astype(np.int32)\n",
    "                            x1,y1,_ = predictions[i]['keypoints'][0][idx].detach().cpu().numpy().astype(np.int32)\n",
    "                            kp1 = (x1,y1)\n",
    "                            x2,y2,_ = targets[i]['keypoints'][0][idx].detach().cpu().numpy().astype(np.int32)\n",
    "                            kp2 = (x2,y2)\n",
    "                            num_keypoint_predictions += 1\n",
    "                            running_epoch_dist_error += calc_distance(kp1, kp2)\n",
    "                            avg_dist_error = running_epoch_dist_error / num_keypoint_predictions\n",
    "                        except:\n",
    "                            print(\"No prediction for val image.\")\n",
    "        print(\"Running Epoch Error: \", avg_dist_error)\n",
    "        score = -avg_dist_error\n",
    "        return score\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, save_dir):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, os.path.join(save_dir, f'checkpoint_{epoch}.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_data, val_data, device, hp_combination):\n",
    "    if train_model == True:\n",
    "        print(\"Training model...\")\n",
    "\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        optimizer = torch.optim.SGD(params, lr=0.0005, momentum=0.8, weight_decay=0.0005)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.3)\n",
    "\n",
    "\n",
    "        save_dir = 'checkpoints'\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=100)\n",
    "            lr_scheduler.step()\n",
    "            save_checkpoint(model, optimizer, epoch, save_dir)\n",
    "            #evaluate(model, data_loader_test, device)\n",
    "            \n",
    "            #model.eval()\n",
    "            \n",
    "        #torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def grid_search(train_data, val_data, device, num_keypoints, hyperparams):\n",
    "    print(\"Started at: \", datetime.datetime.now().strftime(\"%H:%M:%S\"))\n",
    "    best_score = 0\n",
    "    best_hyperparams = None\n",
    "\n",
    "    for hp_combination in itertools.product(*hyperparams.values()):\n",
    "        print(f\"Running training with hyperparameters: {hp_combination}\")\n",
    "\n",
    "        anchor_size, anchor_ratios = hp_combination\n",
    "\n",
    "        model = get_model(2, anchor_size, anchor_ratios)\n",
    "        model.to(device)\n",
    "\n",
    "        # Train and evaluate the model with the current hyperparameters\n",
    "        train_and_evaluate(model, train_data, val_data, device, hp_combination)\n",
    "        score = calc_epoch_error(model, data_loader_test, device)\n",
    "        \n",
    "\n",
    "        # Update the best score and hyperparameters if necessary\n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_hyperparams = hp_combination\n",
    "            print(f\"Saving best model at{save_path}\")\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "\n",
    "    print(f\"Best hyperparameters: {best_hyperparams}\")\n",
    "    print(\"Ended at: \", datetime.datetime.now().strftime(\"%H:%M:%S\"))\n",
    "    return best_hyperparams\n",
    "\n",
    "hyperparams = {\n",
    "    'anchor_sizes': [(32, 64, 128, 256)],\n",
    "    'anchor_ratios': [(0.5, 0.83, 1.2, 2)]\n",
    "}\n",
    "\n",
    "best_hyperparams = grid_search(data_loader_train, data_loader_test, device, num_keypoints, hyperparams)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "if train_model == True:\n",
    "    print(\"Training model...\")\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.0005, momentum=0.8, weight_decay=0.0005)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.3)\n",
    "\n",
    "    # Define the number of splits for K-Fold Cross Validation\n",
    "    num_splits = 5\n",
    "    kfold = KFold(n_splits=num_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Assuming your dataset is a torch.utils.data.Dataset object\n",
    "    dataset_size = len(dataset_train)\n",
    "\n",
    "    # Convert dataset indices to a NumPy array\n",
    "    indices = np.arange(dataset_size)\n",
    "\n",
    "    # Initialize the fold counter\n",
    "    fold = 0\n",
    "\n",
    "    for train_indices, val_indices in kfold.split(indices):\n",
    "        fold += 1\n",
    "        print(f\"Training fold {fold}/{num_splits}\")\n",
    "\n",
    "        # Create data loaders for the current fold\n",
    "        train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "        val_sampler = torch.utils.data.SubsetRandomSampler(val_indices)\n",
    "\n",
    "        data_loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, sampler=train_sampler, num_workers=4)\n",
    "        data_loader_val = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, sampler=val_sampler, num_workers=4)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=100)\n",
    "            lr_scheduler.step()\n",
    "            evaluate(model, data_loader_val, device)\n",
    "\n",
    "        # Would Need to calculate metrics here\n",
    "\n",
    "        # Reset the model and optimizer for the next fold\n",
    "        model.apply(reset_weights)\n",
    "        optimizer = torch.optim.SGD(params, lr=0.0005, momentum=0.8, weight_decay=0.0005)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.3)\n",
    "\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_distance(kp1, kp2):\n",
    "    x1, y1 = kp1\n",
    "    x2, y2 = kp2\n",
    "    return math.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "\n",
    "def calc_mean_distance_error(real_keypoints, pred_keypoints):\n",
    "    mean_distance = 0\n",
    "    point_error = [0,0,0]\n",
    "    assert len(real_keypoints) == len(pred_keypoints)\n",
    "    for i in range(len(real_keypoints)):\n",
    "        for j in range(len(real_keypoints[i])):\n",
    "            point_error[j] = calc_distance(real_keypoints[i][j], pred_keypoints[i][j])\n",
    "            mean_distance += point_error[j]\n",
    "    return mean_distance/len(keypoints), point_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_keypoints(scores):\n",
    "    avg_scores = []\n",
    "    for score in scores:\n",
    "        avg_scores.append(torch.mean(score))\n",
    "    return avg_scores.index(max(avg_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "\n",
    "    print(\"Loading model...\")\n",
    "    anchor_sizes, anchor_ratios = best_hyperparams        \n",
    "    model = get_model(2, anchor_sizes=anchor_sizes, anchor_ratios=anchor_ratios)\n",
    "    model.to(device)\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    test_batch_size = 1\n",
    "    data_loader_test = DataLoader(dataset_test, batch_size=test_batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    iterator = iter(data_loader_test)\n",
    "\n",
    "    total_mean_distance_error = 0\n",
    "    total_mean_ETT_distance_error = 0\n",
    "    ETT_distance_error_list = []\n",
    "    total_point_error_list = []\n",
    "    example_error_list = []\n",
    "    pred_image_file_list = []\n",
    "    total_number_of_predictions = 0\n",
    "    no_preds_count = 0\n",
    "    total_attempts = 0\n",
    "    batch_count = 0\n",
    "    correct_predictions = 0\n",
    "    incorrect_predictions = 0\n",
    "    correct_real = 0\n",
    "    incorrect_real = 0\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for item in iterator:\n",
    "        \n",
    "        try:\n",
    "            images, targets = item\n",
    "            images = list(image.to(device) for image in images)\n",
    "            \n",
    "\n",
    "        except StopIteration as e:\n",
    "            print(\"StopIteration exception handled at batch: \", batch_count)\n",
    "            \n",
    "        \n",
    "        model.to(device)            \n",
    "        model.eval()\n",
    "        output = model(images)   \n",
    "        #print(\"output: \", output)\n",
    "\n",
    "        batch_count += 1 \n",
    "\n",
    "        for prediction_number in range(len(images)):\n",
    "            \n",
    "            real_keypoints = [] #list of keypoints for each image in batch\n",
    "\n",
    "            #unpacking the targets, this is a pain but works to remove the 0/1 visibility dim (which we do not need because all keypoints are visible)\n",
    "            for kps in targets[prediction_number]['keypoints']:\n",
    "                real_keypoints.append([list(map(int, kp[:2])) for kp in kps])\n",
    "            distance_real = calc_distance(real_keypoints[0][0], real_keypoints[0][1])\n",
    "            #print(\"Real keypoints: \", real_keypoints)\n",
    "        \n",
    "            \n",
    "            real_bboxes = targets[prediction_number]['boxes'].int().tolist()\n",
    "            \n",
    "            #permute(1,2,0) converts the tensor to numpy array. The tensor is in the format (C, H, W) and numpy array is in the format (H, W, C).\n",
    "            #detach().cpu().numpy() detaches the tensor from the graph and converts it to numpy array and moves it to CPU.\n",
    "            image = (images[prediction_number].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)  \n",
    "            \n",
    "            scores = output[prediction_number]['scores'].detach().cpu().numpy()\n",
    "            if len(scores) == 0:\n",
    "                print(\"No keypoints found at image: \", prediction_number)\n",
    "                no_preds_count += 1\n",
    "                break\n",
    "\n",
    "\n",
    "            high_scores_idxs = np.where(scores > 0.1)[0].tolist() # Indexes of boxes with scores > 0.1\n",
    "            #print(\"High Score idxs: \", high_scores_idxs)\n",
    "            #print(\"Raw NMS Boxes len: \", len(output[0]['boxes'][high_scores_idxs]))\n",
    "            #print(\"Raw NMS scores len: \", len(output[0]['scores'][high_scores_idxs]))\n",
    "            post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n",
    "            #post_nms_idxs = 0\n",
    "            #print(\"-----------Post NMS idxs:-----------\", post_nms_idxs)\n",
    "\n",
    "            #Making images based on keypoints_scores, instead of bbox scores now\n",
    "            #print(\"Raw Keypoint scores: \", output[prediction_number]['keypoints_scores'])\n",
    "            keypoint_scores = output[prediction_number]['keypoints_scores'][post_nms_idxs]\n",
    "            #print(\"Keypoint scores: \", keypoint_scores)\n",
    "            #print(\"Keypoint scores: \", keypoint_scores)\n",
    "            score_idx = get_best_keypoints(keypoint_scores)\n",
    "            #print(\"Best Keypoints IDX: \", score_idx)\n",
    "            #score_idx = 0\n",
    "            pred_keypoints = []\n",
    "            keypoints = output[prediction_number]['keypoints'][score_idx].detach().cpu().numpy()\n",
    "        \n",
    "            for kp in keypoints:\n",
    "                kp = list(map(int, kp)) #convert (x,y) coords in each keypoint to int\n",
    "                pred_keypoints.append(kp[:2])\n",
    "            pred_keypoints = [pred_keypoints] #convert to list of lists to match real_keypoints format\n",
    "            #print(\"Pred keypoints: \", pred_keypoints)\n",
    "\n",
    "            distance_pred = calc_distance(pred_keypoints[0][0], pred_keypoints[0][1])\n",
    "            distance_error = abs(distance_pred - distance_real)\n",
    "            \n",
    "            #print(\"Real ETT to Carina:\", distance_real/50)\n",
    "            #print(\"Prediencted ETT to Carina:\", distance_pred/50)\n",
    "            #print(\"ETT to Carina Distance error:\" , distance_error/50)\n",
    "\n",
    "            if ((distance_real/50) <= 3) or ((distance_real/50) >= 7):\n",
    "                incorrect_real += 1\n",
    "            else:\n",
    "                correct_real += 1\n",
    "\n",
    "            if ((distance_pred/50) <= 3) or ((distance_pred/50) >= 7):\n",
    "                incorrect_predictions += 1\n",
    "            else:\n",
    "                correct_predictions += 1\n",
    "\n",
    "            if (((distance_real/50) <= 3) or ((distance_real/50) >= 7)) and (((distance_pred/50) <= 3) or ((distance_pred/50) >= 7)):\n",
    "                TN +=1\n",
    "                #print(\"TN\")\n",
    "            elif (((distance_real/50) <= 3) or ((distance_real/50) >= 7)) and (((distance_pred/50) > 3) or ((distance_pred/50) < 7)):\n",
    "                FN +=1\n",
    "                #print(\"FN\")\n",
    "            elif (((distance_real/50) > 3) or ((distance_real/50) < 7)) and (((distance_pred/50) <= 3) or ((distance_pred/50) >= 7)):\n",
    "                FP +=1\n",
    "                #print(\"FP\")\n",
    "            elif (((distance_real/50) > 3) or ((distance_real/50) < 7)) and (((distance_pred/50) > 3) or ((distance_pred/50) < 7)):\n",
    "                TP +=1\n",
    "                #print(\"TP\")\n",
    "\n",
    "\n",
    "            bboxes = []\n",
    "            \n",
    "            #print(\"Boxes:\", output[prediction_number]['boxes'][[0]][[0]])\n",
    "            #print(\"Scores:\", output[prediction_number]['scores'])\n",
    "            for bbox in  output[prediction_number]['boxes'][[score_idx]][[0]].detach().cpu().numpy():\n",
    "                bboxes.append(list(map(int, bbox.tolist())))\n",
    "            \n",
    "            \n",
    "            if (len(bboxes) == 0):\n",
    "                print(\"No bounding boxes found at image: \", prediction_number)\n",
    "                no_preds_count += 1 #count preds with no bounding box at given threshold  \n",
    "            else:\n",
    "                total_number_of_predictions += 1    \n",
    "                example_error, point_error = calc_mean_distance_error(real_keypoints, pred_keypoints)\n",
    "                total_mean_distance_error += example_error\n",
    "                \n",
    "            \n",
    "            total_attempts += 1\n",
    "            \n",
    "            example_point_error = [pt for pt in point_error]\n",
    "            total_point_error_list.append(example_point_error)\n",
    "            example_error_list.append(example_error)\n",
    "            ETT_distance_error_list.append(distance_error)\n",
    "            #print(\"Example error: \", example_error)\n",
    "            \n",
    "            save_img_path = f\"F:\\ET_Tube\\saved_img_preds\\prediction_{str(prediction_number)}_{str(batch_count)}.jpg\"\n",
    "            pred_image_file_list.append(save_img_path)\n",
    "            visualize(image, bboxes, pred_keypoints, image_original=image, keypoints_original=real_keypoints, bboxes_original=real_bboxes, save=False, save_path = save_img_path, display=False)\n",
    "            plt.savefig(save_img_path)\n",
    "            #break\n",
    "        #break\n",
    "\n",
    "    print(\"Placement Correct predictions: \", correct_predictions)\n",
    "    print(\"Placement Incorrect predictions: \", incorrect_predictions)\n",
    "    print(\"Placement Correct real: \", correct_real)\n",
    "    print(\"Placement Incorrect real: \", incorrect_real)\n",
    "\n",
    "\n",
    "    total_mean_distance_error /= total_number_of_predictions\n",
    "    print(\"Total mean Euclidean distance error: \", total_mean_distance_error)\n",
    "    total_mean_ETT_distance_error = sum(ETT_distance_error_list)/len(ETT_distance_error_list)\n",
    "    print(\"Total ETT distance error: \", total_mean_ETT_distance_error)\n",
    "    print(\"No predictions count: \", no_preds_count)\n",
    "    print(\"Total attempts: \", total_attempts)\n",
    "    print(\"Completed at: \", datetime.datetime.now().strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TP: \", TP)\n",
    "print(\"FP: \", FP)\n",
    "print(\"TN: \", TN)\n",
    "print(\"FN: \", FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# Create a 2x2 confusion matrix\n",
    "confusion_matrix = [\n",
    "    [TP, FP],\n",
    "    [FN, TN]\n",
    "]\n",
    "\n",
    "# Create a heatmap using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.set(font_scale=1.4)  # Increase font size\n",
    "ax = sns.heatmap(confusion_matrix, annot=True, cmap=\"YlGnBu\", fmt=\"d\", annot_kws={\"size\": 16})\n",
    "ax.set_ylim(sorted(ax.get_xlim(), reverse=True))\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Predicted labels')\n",
    "ax.set_ylabel('True labels')\n",
    "ax.set_title('Confusion Matrix for ETT Placement')\n",
    "\n",
    "# Set tick labels\n",
    "ax.xaxis.set_ticklabels(['Correct Placement', 'Incorrect Placement'])\n",
    "ax.yaxis.set_ticklabels(['Correct Placement', 'Incorrect Placement'])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = TP / (TP + FP)\n",
    "print(\"Precision/PPV: \", precision)\n",
    "recall = TP / (TP + FN)\n",
    "print (\"Recall/Sensitivity: \", recall)\n",
    "NPV = TN / (TN + FN)\n",
    "print(\"NPV: \", NPV)\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "specificity = TN / (TN + FP)\n",
    "print(\"Specificity: \", specificity)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "print(\"F1 Score: \", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Proportion of Correct Keypoints (PCK)\n",
    "# Threshold is the maximum distance between the predicted and ground truth keypoints\n",
    "# Here it is written (default param) as 10 pixels\n",
    "# Perhaps this should be a function of the image size?\n",
    "def calculate_example_pck(total_point_error_list, threshold=10):\n",
    "    num_correct = 0\n",
    "    flat_list = list(itertools.chain(*total_point_error_list))\n",
    "\n",
    "    for point in flat_list:\n",
    "        if point <= threshold:\n",
    "            num_correct += 1\n",
    "            \n",
    "# Calculate the FCK\n",
    "    pck = num_correct / len(flat_list)\n",
    "    return pck\n",
    "\n",
    "pck_threshold_small = 10\n",
    "pck_small = calculate_example_pck(total_point_error_list, threshold=pck_threshold_small)\n",
    "print(f\"Fraction of Correct Keypoints: {calculate_example_pck(total_point_error_list, threshold=pck_threshold_small):0.3f}, at a threshold of {pck_threshold_small} pixels.\")\n",
    "\n",
    "pck_threshold_large = 50\n",
    "pck_large = calculate_example_pck(total_point_error_list, threshold=pck_threshold_large)\n",
    "print(f\"Fraction of Correct Keypoints: {calculate_example_pck(total_point_error_list, threshold=pck_threshold_large):0.3f}, at a threshold of {pck_threshold_large} pixels.\")\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_thumbnail(path):\n",
    "    i = Image.open(path)\n",
    "    i.thumbnail((500, 250), Image.Resampling.LANCZOS)\n",
    "    return i\n",
    "\n",
    "def image_base64(im):\n",
    "    if isinstance(im, str):\n",
    "        im = get_thumbnail(im)\n",
    "    with BytesIO() as buffer:\n",
    "        im.save(buffer, 'jpeg')\n",
    "        return base64.b64encode(buffer.getvalue()).decode()\n",
    "\n",
    "def image_formatter(im):\n",
    "    return f'<img src=\"data:image/jpeg;base64,{image_base64(im)}\">'\n",
    "\n",
    "df = pd.DataFrame(example_error_list, columns=['Example Error'])\n",
    "df['pck_small'] = pck_small\n",
    "df['pck_large'] = pck_large\n",
    "df['file'] = pred_image_file_list\n",
    "df['image'] = df.file.map(lambda f: get_thumbnail(f))\n",
    "df.head()\n",
    "\n",
    "HTML(df[['Example Error', 'image']].to_html(formatters={'image': image_formatter}, escape=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workbook = Workbook()\n",
    "worksheet = workbook.active\n",
    "\n",
    "# resize cells\n",
    "for row in range(2, len(df['file'])+2):\n",
    "    for col in range(1,2):\n",
    "        worksheet.row_dimensions[row].height = 400\n",
    "        col_letter = get_column_letter(col)\n",
    "        worksheet.column_dimensions[col_letter].width = 150\n",
    "        \n",
    "for col in range(2,50):\n",
    "    col_letter = get_column_letter(col)\n",
    "    worksheet.column_dimensions[col_letter].width = 30\n",
    "         \n",
    "# insert images\n",
    "for index, image in enumerate(df['file']):\n",
    "    if index == 0:\n",
    "        worksheet.cell(row=index+1, column=1, value=\"Image\")\n",
    "        worksheet.add_image(ExcelImage(image), anchor='A'+str(index+2))\n",
    "    else:\n",
    "        worksheet.add_image(ExcelImage(image), anchor='A'+str(index+2))\n",
    "\n",
    "excel_cell_values = []\n",
    "for excel_cell_value in df['Example Error']:\n",
    "    excel_cell_values.append(excel_cell_value)\n",
    "\n",
    "for index, excel_cell_value in enumerate(excel_cell_values):\n",
    "    if index == 0:\n",
    "        worksheet.cell(row=index+1, column=2, value=\"Example Error\")\n",
    "        worksheet.cell(row=index+2, column=2, value=excel_cell_value)\n",
    "    else:\n",
    "        worksheet.cell(row=index+2, column=2, value=excel_cell_value)\n",
    "\n",
    "# Find the last column\n",
    "last_column = worksheet.max_column\n",
    "\n",
    "# Add values to the last column\n",
    "worksheet.cell(row=1, column=last_column+1).value = f'Total Mean Euclidean Distance Error: {total_mean_distance_error}'\n",
    "worksheet.cell(row=2, column=last_column+1).value = f'No predictions count: {no_preds_count}'\n",
    "worksheet.cell(row=6, column=last_column+1).value = f'Fraction of Correct Keypoints at threshold of {pck_threshold_small} pixels: {pck_small}'\n",
    "worksheet.cell(row=7, column=last_column+1).value = f'Fraction of Correct Keypoints at threshold of {pck_threshold_large} pixels: {pck_large}'\n",
    "\n",
    "# save workbook\n",
    "workbook.save(workbook_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_predict(image_file):\n",
    "    raw_img = cv2.imread(image_file)\n",
    "    aspect_ratio = raw_img.shape[0]/raw_img.shape[1]\n",
    "\n",
    "    if aspect_ratio < 1.5 and aspect_ratio > 0.5:\n",
    "        raw_img = cv2.resize(raw_img, (456, 456))\n",
    "        test_img = np.moveaxis(raw_img, -1, 0)\n",
    "        test_img= np.expand_dims(test_img, axis=0)\n",
    "        test_img = torch.from_numpy(test_img).float().to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.to(device)            \n",
    "            model.eval()\n",
    "            output = model(test_img)\n",
    "        \n",
    "        scores = output[0]['keypoints_scores']\n",
    "        score_idx = get_best_keypoints(scores)\n",
    "\n",
    "        keypoints = output[0]['keypoints'][score_idx].detach().cpu().numpy()\n",
    "        for idx, kp in enumerate(keypoints):\n",
    "            current_keypoint = kp[:2].astype(int)\n",
    "            raw_img = cv2.circle(raw_img, current_keypoint, 1, (255,255,0), 10)\n",
    "            image_original = cv2.putText(raw_img, \" \" + keypoints_classes_ids2names[idx], current_keypoint, cv2.FONT_HERSHEY_PLAIN, 2, (255,0,0), 2, cv2.LINE_AA)\n",
    "\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.imshow(raw_img)\n",
    "    else:\n",
    "        print(f\"Image aspect ratio is {aspect_ratio:0.2F} it but be between 0.5 - 1.5\")\n",
    "\n",
    "        \n",
    "#test_img = \"/mnt/c/Users/nprim/Downloads/F1.jpg\"\n",
    "#process_and_predict(test_img) # random picture from the internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
