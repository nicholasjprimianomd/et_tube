{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 27\n",
    "#num = 3\n",
    "\n",
    "train_model = True\n",
    "load_weights = False\n",
    "make_labels = True\n",
    "\n",
    "\n",
    "import datetime\n",
    "num_keypoints = 2\n",
    "save_path = rf'F:\\ET_Tube\\CheXpert-v1.0\\weights\\test_weights_{datetime.datetime.now().strftime(\"%H_%M_%S\")}.pth'\n",
    "save_dir =  r\"D:\\ET_Tube\\CheXpert-v1.0\\checkpoints\" #checkpoint directory\n",
    "workbook_path = rf'F:\\ET_Tube\\CheXpert-v1.0\\predictions_{datetime.datetime.now().strftime(\"%H_%M_%S\")}.xlsx'\n",
    "\n",
    "batch_size = 1\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYPOINTS_FOLDER_TRAIN = fr\"C:\\Users\\nprim\\Downloads\\ET_TUBE.v{num}i.yolov7pytorch\\train\"\n",
    "KEYPOINTS_FOLDER_VALID = fr\"C:\\Users\\nprim\\Downloads\\ET_TUBE.v{num}i.yolov7pytorch\\valid\"\n",
    "#KEYPOINTS_FOLDER_TRAIN = fr\"C:\\Users\\nprim\\Downloads\\ET_TUBE.v23i.yolov7pytorch\\train\"\n",
    "#KEYPOINTS_FOLDER_TEST =  fr\"C:\\Users\\nprim\\Downloads\\ET_TUBE.v{num}i.yolov7pytorch\\test\"\n",
    "KEYPOINTS_FOLDER_TEST =  fr\"C:\\Users\\nprim\\Downloads\\ET_TUBE.v{num}i.yolov7pytorch\\test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import json, cv2, numpy as np, matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "import torchvision\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision.models.detection import KeypointRCNN\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import transforms, utils, engine, train\n",
    "from utils import collate_fn\n",
    "from engine import train_one_epoch, evaluate\n",
    "import random\n",
    "import base64\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from IPython.display import HTML\n",
    "from torchsummary import summary\n",
    "import math\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.drawing.image import Image as ExcelImage\n",
    "from openpyxl.utils import get_column_letter\n",
    "import glob\n",
    "from natsort import natsorted\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "print(\"Using torch\", torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if make_labels:\n",
    "    import json\n",
    "    import zipfile\n",
    "    path_to_zip_file = fr\"C:\\Users\\nprim\\Downloads\\ET_TUBE.v{num}i.yolov7pytorch.zip\"\n",
    "    \n",
    "    BASE = fr\"C:\\Users\\nprim\\Downloads\\ET_TUBE.v{num}i.yolov7pytorch\"\n",
    "    with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(BASE)\n",
    "\n",
    "    SET = [\"/train\", \"/valid\", \"/test\"]\n",
    "    #SET = [\"/train\",  \"/test\"]\n",
    "    #SET = [\"/train\", \"/valid\"]\n",
    "    #SET = [\"/train\"]\n",
    "    #SET = [\"/test\"]\n",
    "  \n",
    "    keypoint_names =  ['Carina', 'ET']\n",
    "\n",
    "    def dump2json(bboxes, keypoints_sorted, file_json):\n",
    "        annotations = {}\n",
    "        annotations['bboxes'], annotations['keypoints'] = bboxes, keypoints_sorted\n",
    "\n",
    "        with open(file_json, \"w\") as f:\n",
    "            json.dump(annotations, f)\n",
    "\n",
    "    def converter(file_labels, file_image, keypoint_names):\n",
    "\n",
    "        img = cv2.imread(file_image)\n",
    "        img_w, img_h = img.shape[1], img.shape[0]\n",
    "        \n",
    "        with open(file_labels) as f:\n",
    "            lines_txt = f.readlines()\n",
    "            lines = []\n",
    "            for line in lines_txt:\n",
    "                lines.append([int(line.split()[0])] + [round(float(el), 5) for el in line.split()[1:]])\n",
    "        \n",
    "        bboxes = []\n",
    "        keypoints = []\n",
    "\n",
    "        # Convert normalized coordinates to absolute coordinates\n",
    "        for line in lines:\n",
    "            # Number 0 is a class of rectangles related to bounding boxes.\n",
    "            if line[0] == 2:\n",
    "                x_c, y_c, w, h = round(line[1] * img_w), round(line[2] * img_h), round(line[3] * img_w), round(line[4] * img_h)\n",
    "                bboxes.append([round(x_c - w/2), round(y_c - h/2), round(x_c + w/2), round(y_c + h/2)])\n",
    "\n",
    "            elif line[0] == 0 or line[0] == 1: #append all other keypoints without class change\n",
    "                kp_id, x_c, y_c = line[0], round(line[1] * img_w), round(line[2] * img_h) \n",
    "                keypoints.append([kp_id, x_c, y_c])\n",
    "                \n",
    "\n",
    "        # iterating over each keypoint and looking to which bounding box it matches, dont need this for patellas\n",
    "        keypoints_sorted = [[[] for _ in keypoint_names] for _ in bboxes]\n",
    "\n",
    "        for kp in keypoints:\n",
    "            kp_id, kp_x, kp_y = kp[0], kp[1], kp[2]\n",
    "            for bbox_idx, bbox in enumerate(bboxes):\n",
    "                x1, y1, x2, y2 = bbox[0], bbox[1], bbox[2], bbox[3]\n",
    "                if x1 < kp_x < x2 and y1 < kp_y < y2:\n",
    "                    keypoints_sorted[bbox_idx][kp_id] = [kp_x, kp_y, 1] # All keypoints are visible\n",
    "                    \n",
    "        return bboxes, keypoints_sorted\n",
    "\n",
    "\n",
    "    for i in range(len(SET)):\n",
    "        IMAGES = BASE + SET[i] + \"/images\"\n",
    "        LABELS = BASE + SET[i] + \"/labels\"\n",
    "        ANNOTATIONS = BASE +  SET[i] + \"/annotations\"\n",
    "        \n",
    "        files_names = [file.split('.jpg')[0] for file in os.listdir(IMAGES)]\n",
    "\n",
    "        for file in tqdm((files_names), desc =f\"Set: {SET[i]}\"):\n",
    "        #for file in (files_names):\n",
    "            file_labels = os.path.join(LABELS, file + \".txt\")\n",
    "            file_image = os.path.join(IMAGES, file + \".jpg\")\n",
    "\n",
    "            #img = cv2.imread(file_image)\n",
    "            #if img.shape[0] != img.shape[1]:\n",
    "                #print(\"Non square image:\", file_image)\n",
    "                \n",
    "            bboxes, keypoints_sorted = converter(file_labels, file_image, keypoint_names)\n",
    "\n",
    "            for i in keypoints_sorted:\n",
    "                a,b = i\n",
    "                if (len(a) != 3 or len (b) != 3) :\n",
    "                    print(\"Error in file\", file)\n",
    "\n",
    "            if not os.path.exists(ANNOTATIONS):\n",
    "                os.makedirs(ANNOTATIONS)\n",
    "\n",
    "            dump2json(bboxes, keypoints_sorted, os.path.join(ANNOTATIONS, file + '.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMG_SIZE = 512\n",
    "\n",
    "def train_transform():\n",
    "    return A.Compose([\n",
    "        A.Sequential([\n",
    "            A.GaussNoise(var_limit=(50, 100.0), mean=0, per_channel=True, always_apply=False, p=0.5),\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n",
    "            A.Rotate(p=0.5, limit=20, border_mode=cv2.BORDER_CONSTANT, value=0),\n",
    "            A.CLAHE(clip_limit=(1,4), p=1)\n",
    "            #A.Resize(IMG_SIZE, IMG_SIZE, p=1)\n",
    "        ], p=1)\n",
    "    ],\n",
    "    keypoint_params=A.KeypointParams(format='xy'), \n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bboxes_labels'])\n",
    "    )\n",
    "\n",
    "def test_transform():\n",
    "    return A.Compose([\n",
    "        A.Sequential([\n",
    "            A.CLAHE(clip_limit=(1,4), p=1)\n",
    "            #A.Resize(IMG_SIZE, IMG_SIZE, p=1)\n",
    "        ], p=1)\n",
    "    ],\n",
    "    keypoint_params=A.KeypointParams(format='xy'), \n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bboxes_labels'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transform():\n",
    "    return A.Compose([\n",
    "        A.GaussNoise(var_limit=(10, 50), mean=0, per_channel=True, p=0.5), # Simulate sensor noise\n",
    "        A.ISONoise(intensity=(0.1, 0.5), p=0.5), # Additional noise type\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5), # Adjust brightness & contrast\n",
    "        A.Rotate(limit=15, border_mode=cv2.BORDER_CONSTANT, value=0, p=0.5), # Small rotations\n",
    "        A.HorizontalFlip(p=0.5), # Horizontal flip (use with caution, depending on orientation-specific structures)\n",
    "        #A.ElasticTransform(alpha=1, sigma=50, p=0.5), # Elastic deformation\n",
    "        A.CLAHE(clip_limit=(1,4), p=1) # Enhance local contrast\n",
    "        #A.Resize(IMG_SIZE, IMG_SIZE, p=1) if IMG_SIZE else None # Resize if needed\n",
    "    ],\n",
    "    keypoint_params=A.KeypointParams(format='xy'),\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bboxes_labels'])\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, demo=False):                \n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.demo = demo # Use demo=True if you need transformed and original images (for example, for visualization purposes)\n",
    "        self.imgs_files = sorted(os.listdir(os.path.join(root, \"images\")))\n",
    "        self.annotations_files = sorted(os.listdir(os.path.join(root, \"annotations\")))\n",
    "\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root, \"images\", self.imgs_files[idx])\n",
    "        annotations_path = os.path.join(self.root, \"annotations\", self.annotations_files[idx])\n",
    "\n",
    "        img_original = cv2.imread(img_path)\n",
    "        img_original = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)        \n",
    "        \n",
    "        with open(annotations_path) as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "            bboxes_original = data['bboxes']\n",
    "            keypoints_original = data['keypoints']\n",
    "                          \n",
    "            # All objects are patellas, so we can use the same label for all objects\n",
    "            bboxes_labels_original = ['ROI' for _ in bboxes_original]  \n",
    "\n",
    "\n",
    "\n",
    "            if self.transform:   \n",
    "                # Converting keypoints from [x,y,visibility]-format to [x, y]-format + Flattening nested list of keypoints            \n",
    "                # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "                # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]], where each keypoint is in [x, y]-format            \n",
    "                # Then we need to convert it to the following list:\n",
    "                # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2]\n",
    "                \n",
    "                keypoints_original_flattened = [el[0:2] for kp in keypoints_original for el in kp]\n",
    "\n",
    "                # Apply augmentations\n",
    "                transformed = self.transform(image=img_original, bboxes=bboxes_original, bboxes_labels=bboxes_labels_original, keypoints=keypoints_original_flattened)\n",
    "                img = transformed['image']\n",
    "                bboxes = transformed['bboxes']\n",
    "                \n",
    "                # Unflattening list transformed['keypoints']\n",
    "                # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "                # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2], where each keypoint is in [x, y]-format\n",
    "                # Then we need to convert it to the following list:\n",
    "                # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]]\n",
    "                \n",
    "                #keypoints_transformed_unflattened = np.reshape(np.array(transformed['keypoints']), (-1,3,2)).tolist()\n",
    "\n",
    "                keypoints_transformed_unflattened = [transformed['keypoints']]\n",
    "\n",
    "\n",
    "                #keypoints_transformed_unflattened = np.reshape(np.array(transformed['keypoints']), (-1,3,2)).tolist()\n",
    "                \n",
    "                # Converting transformed keypoints from [x, y]-format to [x,y,visibility]-format by appending original visibilities to transformed coordinates of keypoints\n",
    "                keypoints = []\n",
    "                for o_idx, obj in enumerate(keypoints_transformed_unflattened): # Iterating over objects\n",
    "                    obj_keypoints = []\n",
    "\n",
    "                    for k_idx, kp in enumerate(obj): # Iterating over keypoints in each object\n",
    "                        # kp - coordinates of keypoint\n",
    "                        # keypoints_original[o_idx][k_idx][2] - original visibility of keypoint\n",
    "                        obj_keypoints.append(np.array(kp).tolist() + [keypoints_original[o_idx][k_idx][2]])\n",
    "                    keypoints.append(obj_keypoints)\n",
    "        \n",
    "            else:\n",
    "                img, bboxes, keypoints = img_original, bboxes_original, keypoints_original\n",
    "                 \n",
    "            # Convert everything into a torch tensor        \n",
    "            bboxes = torch.as_tensor(bboxes, dtype=torch.float32)       \n",
    "            target = {}\n",
    "            target[\"boxes\"] = bboxes\n",
    "            target[\"labels\"] = torch.as_tensor([1 for _ in bboxes], dtype=torch.int64)\n",
    "            target[\"image_id\"] = torch.tensor([idx])\n",
    "            target[\"area\"] = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])\n",
    "            target[\"iscrowd\"] = torch.zeros(len(bboxes), dtype=torch.int64)\n",
    "\n",
    "            target[\"keypoints\"] = torch.as_tensor(keypoints, dtype=torch.float32) \n",
    "            \n",
    "            img = F.to_tensor(img)\n",
    "            \n",
    "            bboxes_original = torch.as_tensor(bboxes_original, dtype=torch.float32)\n",
    "            target_original = {}\n",
    "            target_original[\"boxes\"] = bboxes_original\n",
    "            target_original[\"labels\"] = torch.as_tensor([1 for _ in bboxes_original], dtype=torch.int64) # all objects are patellas\n",
    "            target_original[\"image_id\"] = torch.tensor([idx])\n",
    "            target_original[\"area\"] = (bboxes_original[:, 3] - bboxes_original[:, 1]) * (bboxes_original[:, 2] - bboxes_original[:, 0])\n",
    "            target_original[\"iscrowd\"] = torch.zeros(len(bboxes_original), dtype=torch.int64)\n",
    "            target_original[\"keypoints\"] = torch.as_tensor(keypoints_original, dtype=torch.float32)        \n",
    "            img_original = F.to_tensor(img_original)\n",
    "\n",
    "            if self.demo:\n",
    "                return img, target, img_original, target_original\n",
    "            else:\n",
    "                return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "dataset = ClassDataset(KEYPOINTS_FOLDER_TEST, transform=train_transform(), demo=True)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "iterator = iter(data_loader)\n",
    "batch = next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints_classes_ids2names = {0: 'Carina' , 1: 'ET'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(image, bboxes, keypoints, image_original=None, bboxes_original=None, keypoints_original=None, save = False, save_path = None, display=True):\n",
    "    fontsize = 12\n",
    "\n",
    "    for bbox in bboxes:\n",
    "        start_point = (bbox[0], bbox[1])\n",
    "        end_point = (bbox[2], bbox[3])\n",
    "        image = cv2.rectangle(image.copy(), start_point, end_point, (0,255,0), 2)\n",
    "    \n",
    "    for kps in keypoints:\n",
    "        for idx, kp in enumerate(kps):\n",
    "            image = cv2.circle(image.copy(), tuple(kp), 1, (255,0,0), 10)\n",
    "            image = cv2.putText(image.copy(), \" \" + keypoints_classes_ids2names[idx], tuple(kp), cv2.FONT_HERSHEY_PLAIN, 1, (255,0,0), 1, cv2.LINE_AA)\n",
    "\n",
    "    if image_original is None and keypoints_original is None:\n",
    "        plt.figure(figsize=(20,20))\n",
    "        plt.imshow(image)\n",
    "        \n",
    "\n",
    "    else:\n",
    "        for bbox in bboxes_original:\n",
    "            start_point = (bbox[0], bbox[1])\n",
    "            end_point = (bbox[2], bbox[3])\n",
    "            image_original = cv2.rectangle(image_original.copy(), start_point, end_point, (255,255,0), 2)\n",
    "        \n",
    "        for kps in keypoints_original:\n",
    "            for idx, kp in enumerate(kps):\n",
    "                image_original = cv2.circle(image_original, tuple(kp), 1, (255,255,0), 10)\n",
    "                image_original = cv2.putText(image_original, \" \" + keypoints_classes_ids2names[idx], tuple(kp), cv2.FONT_HERSHEY_PLAIN, 1, (255,0,0), 1, cv2.LINE_AA)\n",
    "        \n",
    "\n",
    "        f, ax = plt.subplots(1, 2, figsize=(40, 20))\n",
    "        ax[0].set_title('Original image', fontsize=fontsize)\n",
    "        ax[1].set_title('Predicted image', fontsize=fontsize) # also augmented image\n",
    "    \n",
    "        ax[0].imshow(image_original)\n",
    "        ax[1].imshow(image)\n",
    "        \n",
    "        if(save):\n",
    "            #print(\"Saving image to: \", save_path)\n",
    "            plt.gcf().set_size_inches(5, 10)\n",
    "            plt.savefig(save_path, dpi=100)\n",
    "            plt.close()\n",
    "            \n",
    "        if(display == False):\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = (batch[0][0].permute(1,2,0).numpy() * 255).astype(np.uint8)\n",
    "bboxes = batch[1][0]['boxes'].detach().cpu().numpy().astype(np.int32).tolist()\n",
    "\n",
    "keypoints = []\n",
    "for kps in batch[1][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist():\n",
    "    keypoints.append([kp[:2] for kp in kps])\n",
    "\n",
    "image_original = (batch[2][0].permute(1,2,0).numpy() * 255).astype(np.uint8)\n",
    "bboxes_original = batch[3][0]['boxes'].detach().cpu().numpy().astype(np.int32).tolist()\n",
    "\n",
    "keypoints_original = []\n",
    "for kps in batch[3][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist():\n",
    "    keypoints_original.append([kp[:2] for kp in kps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(image, bboxes, keypoints, image_original, bboxes_original, keypoints_original, display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_keypoints=2, anchor_sizes = (64, 128, 256) , anchor_ratios= (0.5, 0.83, 1.2, 2), weights_path=None):\n",
    "    \n",
    "    backbone = torchvision.models.convnext_large(weights='DEFAULT').features\n",
    "    backbone.out_channels = 1536 # 1536 for convnext_large, 1024 for resnet50\n",
    "    #backbone = torchvision.models.resnet101(weights='DEFAULT').features\n",
    "    #backbone.out_channels = 1024 \n",
    "\n",
    "    anchor_generator = AnchorGenerator(sizes=(anchor_sizes,), aspect_ratios=(anchor_ratios,))\n",
    "    \n",
    "    roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
    "                                                    output_size=7,\n",
    "                                                    sampling_ratio=2)\n",
    "    keypoint_roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
    "                                                             output_size=14,\n",
    "                                                             sampling_ratio=2)\n",
    "\n",
    "    model = KeypointRCNN(backbone,\n",
    "                          num_classes=2,\n",
    "                          rpn_anchor_generator=anchor_generator,\n",
    "                          box_roi_pool=roi_pooler,\n",
    "                          keypoint_roi_pool=keypoint_roi_pooler,\n",
    "                          num_keypoints=num_keypoints)\n",
    "    \n",
    "    #if weights_path:\n",
    "    #    print(\"loading weights from: \", weights_path)\n",
    "    #    state_dict = torch.load(weights_path)\n",
    "    #    model.load_state_dict(state_dict)        \n",
    "        \n",
    "    return model\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = ClassDataset(KEYPOINTS_FOLDER_TRAIN, transform=train_transform(), demo=False)\n",
    "dataset_test = ClassDataset(KEYPOINTS_FOLDER_TEST, transform=test_transform(), demo=False)\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_distance(kp1, kp2):\n",
    "    x1, y1 = kp1\n",
    "    x2, y2 = kp2\n",
    "    return math.sqrt((x2 - x1)**2 + (y2 - y1)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_epoch_error(model, data_loader_test, device):\n",
    "    with torch.inference_mode():\n",
    "        model.eval()\n",
    "        running_epoch_dist_error = 0\n",
    "        num_keypoint_predictions = 0\n",
    "        avg_dist_error = 0\n",
    "        for batch_idx, (images, targets) in enumerate(data_loader_test):\n",
    "            #print(f\"Processing batch {batch_idx}\")\n",
    "            #print(f\"Number of images: {len(images)}, Number of targets: {len(targets)}\")\n",
    "            \n",
    "            if not targets:\n",
    "                #print(\"No targets available for the current batch.\")\n",
    "                continue\n",
    "\n",
    "            images = list(image.to(device) for image in images)\n",
    "            predictions = model(images)\n",
    "            for i in range(len(predictions)):\n",
    "                for idx in range(num_keypoints):\n",
    "                    try:\n",
    "                        pred = predictions[i]['keypoints'][0][idx].detach().cpu().numpy().astype(np.int32)\n",
    "                        x1, y1, _ = predictions[i]['keypoints'][0][idx].detach().cpu().numpy().astype(np.int32)\n",
    "                        kp1 = (x1, y1)\n",
    "                        x2, y2, _ = targets[i]['keypoints'][0][idx].detach().cpu().numpy().astype(np.int32)\n",
    "                        kp2 = (x2, y2)\n",
    "                        num_keypoint_predictions += 1\n",
    "                        running_epoch_dist_error += calc_distance(kp1, kp2)\n",
    "                        avg_dist_error = running_epoch_dist_error / num_keypoint_predictions\n",
    "                    except:\n",
    "                        print(\"No prediction for val image.\")\n",
    "        print(\"Running Epoch Error: \", avg_dist_error)\n",
    "        score = -avg_dist_error\n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, fold, save_dir):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'epoch': fold,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, os.path.join(save_dir, f'checkpoint__{fold}_{epoch}.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def train_and_evaluate(model, train_data, val_data, device, hp_combination):\n",
    "    if train_model == True:\n",
    "        #print(\"Training model...\")\n",
    "\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        optimizer = torch.optim.SGD(params, lr=0.0005, momentum=0.8, weight_decay=0.0005)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.3)\n",
    "\n",
    "\n",
    "        #save_dir = 'checkpoints'\n",
    "        #os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch {epoch}\")\n",
    "            train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=100)\n",
    "            lr_scheduler.step()\n",
    "            #save_checkpoint(model, optimizer, epoch, save_dir)\n",
    "            #evaluate(model, data_loader_test, device)\n",
    "            \n",
    "            #model.eval()\n",
    "            \n",
    "        #torch.save(model.state_dict(), save_path)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(model):\n",
    "    for name, module in model.named_modules():\n",
    "        if not name.startswith(\"backbone\"):\n",
    "            if isinstance(model, nn.Conv2d) or isinstance(model, nn.Linear) or isinstance(model, nn.ConvTranspose2d):\n",
    "                model.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import itertools\n",
    "\n",
    "def grid_search(train_data, val_data, device, num_keypoints, hyperparams):\n",
    "    print(\"Started at: \", datetime.datetime.now().strftime(\"%H:%M:%S\"))\n",
    "    best_score = 0\n",
    "    best_hyperparams = None\n",
    "\n",
    "    for hp_combination in itertools.product(*hyperparams.values()):\n",
    "        print(f\"Running training with hyperparameters: {hp_combination}\")\n",
    "\n",
    "        anchor_size, anchor_ratios = hp_combination\n",
    "\n",
    "        model = get_model(2, anchor_size, anchor_ratios)\n",
    "        model.to(device)\n",
    "\n",
    "\n",
    "        #checkpoint_path = \"D:/ET_Tube/CheXpert-v1.0/checkpoints/checkpoint_9.pt\"\n",
    "        #checkpoint = torch.load(checkpoint_path)\n",
    "        #model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "        # Train and evaluate the model with the current hyperparameters\n",
    "        train_and_evaluate(model, train_data, val_data, device, hp_combination)\n",
    "        score = calc_epoch_error(model, data_loader_test, device)\n",
    "        \n",
    "\n",
    "        # Update the best score and hyperparameters if necessary\n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_hyperparams = hp_combination\n",
    "            print(f\"Saving best model at{save_path}\")\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "\n",
    "    print(f\"Best hyperparameters: {best_hyperparams}\")\n",
    "    print(\"Ended at: \", datetime.datetime.now().strftime(\"%H:%M:%S\"))\n",
    "    return best_hyperparams\n",
    "\n",
    "hyperparams = {\n",
    "    'anchor_sizes': [(64, 128, 256)],\n",
    "    'anchor_ratios': [(0.5, 0.83, 1.2, 2)]\n",
    "}\n",
    "\n",
    "#if train_model == True:\n",
    "#    best_hyperparams = grid_search(data_loader_train, data_loader_test, device, num_keypoints, hyperparams)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_distance(kp1, kp2):\n",
    "    x1, y1 = kp1\n",
    "    x2, y2 = kp2\n",
    "    return math.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "\n",
    "def calc_mean_distance_error(real_keypoints, pred_keypoints):\n",
    "    mean_distance = 0\n",
    "    point_error = [0,0,0]\n",
    "    assert len(real_keypoints) == len(pred_keypoints)\n",
    "    for i in range(len(real_keypoints)):\n",
    "        for j in range(len(real_keypoints[i])):\n",
    "            point_error[j] = calc_distance(real_keypoints[i][j], pred_keypoints[i][j])\n",
    "            mean_distance += point_error[j]\n",
    "    return mean_distance/len(keypoints), point_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_keypoints(scores):\n",
    "    avg_scores = []\n",
    "    for score in scores:\n",
    "        avg_scores.append(torch.mean(score))\n",
    "\n",
    "    try:\n",
    "        if len(avg_scores) > 0:\n",
    "            return avg_scores.index(max(avg_scores))\n",
    "        else:\n",
    "            raise ValueError(\"avg_scores is empty. Make sure you have calculated the average scores before finding the maximum.\")\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        return []  # or some other default value\n",
    "   \n",
    "    #return avg_scores.index(max(avg_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Proportion of Correct Keypoints (PCK)\n",
    "# Threshold is the maximum distance between the predicted and ground truth keypoints\n",
    "# Here it is written (default param) as 10 pixels\n",
    "# Perhaps this should be a function of the image size?\n",
    "def calculate_example_pck(total_point_error_list, threshold=10):\n",
    "    num_correct = 0\n",
    "    flat_list = list(itertools.chain(*total_point_error_list))\n",
    "\n",
    "    for point in flat_list:\n",
    "        if point <= threshold:\n",
    "            num_correct += 1\n",
    "            \n",
    "# Calculate the PCK\n",
    "    pck = num_correct / len(flat_list)\n",
    "    return pck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "k fold cross\n",
    "def train_and_evaluate(device, k_folds=5):\n",
    "    with open('output.txt', 'a') as output_file:\n",
    "        print(\"Starting at: \", datetime.datetime.now().strftime(\"%H:%M:%S\"))\n",
    "        \n",
    "\n",
    "        \n",
    "        if train_model == True:\n",
    "            # Combine the train and test datasets\n",
    "            combined_dataset = ConcatDataset([dataset_train, dataset_test])\n",
    "\n",
    "            # K-fold cross-validation\n",
    "            kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "            \n",
    "            fold = 0\n",
    "            for train_idx, val_idx in kf.split(combined_dataset):\n",
    "\n",
    "                example_error_list = []\n",
    "                pred_image_file_list = []\n",
    "                total_mean_distance_error = 0\n",
    "                total_mean_ETT_distance_error = 0\n",
    "                total_number_of_predictions = 0\n",
    "                no_preds_count = 0\n",
    "                total_attempts = 0\n",
    "                batch_count = 0\n",
    "                correct_predictions = 0\n",
    "                incorrect_predictions = 0\n",
    "                correct_real = 0\n",
    "                incorrect_real = 0\n",
    "                TP = 0\n",
    "                FP = 0\n",
    "                TN = 0\n",
    "                FN = 0\n",
    "                fold += 1\n",
    "                print(f\"Fold {fold}\")\n",
    "                \n",
    "                # Split the data into training and validation sets for this fold\n",
    "                train_data = [combined_dataset[i] for i in tqdm(train_idx)]\n",
    "                val_data = [combined_dataset[i] for i in tqdm(val_idx)]\n",
    "                \n",
    "                # Initialize the dataloaders\n",
    "                data_loader_train = DataLoader(train_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "                data_loader_val = DataLoader(val_data, batch_size=10, shuffle=False, collate_fn=collate_fn)\n",
    "                \n",
    "                model = get_model(2)\n",
    "                params = [p for p in model.parameters() if p.requires_grad]\n",
    "                optimizer = torch.optim.SGD(params, lr=0.0005, momentum=0.8, weight_decay=0.0005)\n",
    "                lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.3)\n",
    "\n",
    "                for epoch in range(num_epochs):\n",
    "                    model.to(device)\n",
    "                    train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=100)\n",
    "                    lr_scheduler.step()\n",
    "                    save_checkpoint(model, optimizer, epoch, fold, save_dir)\n",
    "                \n",
    "                #Evaluate the model on the validation set\n",
    "                with torch.inference_mode():\n",
    "\n",
    "                    #print(\"Loading model...\")\n",
    "                    #anchor_sizes, anchor_ratios = best_hyperparams        \n",
    "                    #model = get_model(2, anchor_sizes=anchor_sizes, anchor_ratios=anchor_ratios)\n",
    "                    #model.to(device)\n",
    "\n",
    "                    #checkpoint_path = r\"D:\\ET_Tube\\CheXpert-v1.0\\checkpoints\\checkpoint_3.pt\"\n",
    "                    #checkpoint = torch.load(checkpoint_path)\n",
    "                    #model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                    \n",
    "                    #test_batch_size = 1\n",
    "                    #data_loader_test = DataLoader(val_data, batch_size=test_batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "                    \n",
    "      \n",
    "                    #dataset_val = ClassDataset(KEYPOINTS_FOLDER_TEST, transform=test_transform(), demo=False)\n",
    "                    #data_loader_test = DataLoader(dataset_val, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "                    iterator = iter(data_loader_val)\n",
    "                    for item in iterator:\n",
    "                        try:\n",
    "                            images, targets = item\n",
    "                            images = list(image.to(device) for image in images)\n",
    "                        except StopIteration as e:\n",
    "                            print(\"StopIteration exception handled at batch: \", batch_count)\n",
    "                            \n",
    "                        model.to(device)            \n",
    "                        model.eval()\n",
    "                        output = model(images)   \n",
    "                        #print(\"output: \", output)\n",
    "\n",
    "                        batch_count += 1 \n",
    "\n",
    "                        for prediction_number in range(len(images)):\n",
    "                            \n",
    "                            real_keypoints = [] #list of keypoints for each image in batch\n",
    "\n",
    "                            #unpacking the targets, this is a pain but works to remove the 0/1 visibility dim (which we do not need because all keypoints are visible)\n",
    "                            for kps in targets[prediction_number]['keypoints']:\n",
    "                                real_keypoints.append([list(map(int, kp[:2])) for kp in kps])\n",
    "                            distance_real = calc_distance(real_keypoints[0][0], real_keypoints[0][1])\n",
    "                            #print(\"Real keypoints: \", real_keypoints)\n",
    "                        \n",
    "                            \n",
    "                            real_bboxes = targets[prediction_number]['boxes'].int().tolist()\n",
    "                            \n",
    "                            #permute(1,2,0) converts the tensor to numpy array. The tensor is in the format (C, H, W) and numpy array is in the format (H, W, C).\n",
    "                            #detach().cpu().numpy() detaches the tensor from the graph and converts it to numpy array and moves it to CPU.\n",
    "                            image = (images[prediction_number].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)  \n",
    "                            \n",
    "                            scores = output[prediction_number]['scores'].detach().cpu().numpy()\n",
    "                            if len(scores) == 0:\n",
    "                                print(\"No keypoints found at image: \", prediction_number)\n",
    "                                no_preds_count += 1\n",
    "                                break\n",
    "\n",
    "\n",
    "                            high_scores_idxs = np.where(scores > 0.1)[0].tolist() # Indexes of boxes with scores > 0.1\n",
    "                            #print(\"High Score idxs: \", high_scores_idxs)\n",
    "                            #print(\"Raw NMS Boxes len: \", len(output[0]['boxes'][high_scores_idxs]))\n",
    "                            #print(\"Raw NMS scores len: \", len(output[0]['scores'][high_scores_idxs]))\n",
    "                            try:\n",
    "                                #print(\"Box idx:\", output[0]['boxes'][high_scores_idxs])\n",
    "                                #print(\"Box scores:\", output[0]['scores'][high_scores_idxs])\n",
    "                                post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n",
    "                            except:\n",
    "                                print(\"NMS exception handled at batch: \", batch_count)\n",
    "                                post_nms_idxs = 0\n",
    "                                continue\n",
    "                            \n",
    "                            #print(\"-----------Post NMS idxs:-----------\", post_nms_idxs)\n",
    "\n",
    "                            #Making images based on keypoints_scores, instead of bbox scores now\n",
    "                            #print(\"Raw Keypoint scores: \", output[prediction_number]['keypoints_scores'])\n",
    "                            keypoint_scores = output[prediction_number]['keypoints_scores'][post_nms_idxs]\n",
    "                            #print(\"Keypoint scores: \", keypoint_scores)\n",
    "                            #print(\"Keypoint scores: \", keypoint_scores)\n",
    "                            score_idx = get_best_keypoints(keypoint_scores)\n",
    "                            #print(\"Best Keypoints IDX: \", score_idx)\n",
    "                            #score_idx = 0\n",
    "                            pred_keypoints = []\n",
    "                            keypoints = output[prediction_number]['keypoints'][score_idx].detach().cpu().numpy()\n",
    "                        \n",
    "                            for kp in keypoints:\n",
    "                                kp = list(map(int, kp)) #convert (x,y) coords in each keypoint to int\n",
    "                                pred_keypoints.append(kp[:2])\n",
    "\n",
    "\n",
    "                            if len(pred_keypoints) != 0:\n",
    "                                pred_keypoints = [pred_keypoints] #convert to list of lists to match real_keypoints format\n",
    "                                #print(\"Pred keypoints: \", pred_keypoints)\n",
    "                            \n",
    "                                distance_pred = calc_distance(pred_keypoints[0][0], pred_keypoints[0][1])\n",
    "                                distance_error = abs(distance_pred - distance_real)\n",
    "                            \n",
    "                            #print(\"Real ETT to Carina:\", distance_real/50)\n",
    "                            #print(\"Prediencted ETT to Carina:\", distance_pred/50)\n",
    "                            #print(\"ETT to Carina Distance error:\" , distance_error/50)\n",
    "\n",
    "                                if ((distance_real/50) <= 3) or ((distance_real/50) >= 7):\n",
    "                                    incorrect_real += 1\n",
    "                                else:\n",
    "                                    correct_real += 1\n",
    "\n",
    "                                if ((distance_pred/50) <= 3) or ((distance_pred/50) >= 7):\n",
    "                                    incorrect_predictions += 1\n",
    "                                else:\n",
    "                                    correct_predictions += 1\n",
    "\n",
    "                                if (((distance_real/50) <= 3) or ((distance_real/50) >= 7)) and (((distance_pred/50) <= 3) or ((distance_pred/50) >= 7)):\n",
    "                                    TN +=1\n",
    "                                    #print(\"TN\")\n",
    "                                elif (((distance_real/50) <= 3) or ((distance_real/50) >= 7)) and (((distance_pred/50) > 3) or ((distance_pred/50) < 7)):\n",
    "                                    FN +=1\n",
    "                                    #print(\"FN\")\n",
    "                                elif (((distance_real/50) > 3) or ((distance_real/50) < 7)) and (((distance_pred/50) <= 3) or ((distance_pred/50) >= 7)):\n",
    "                                    FP +=1\n",
    "                                    #print(\"FP\")\n",
    "                                elif (((distance_real/50) > 3) or ((distance_real/50) < 7)) and (((distance_pred/50) > 3) or ((distance_pred/50) < 7)):\n",
    "                                    TP +=1\n",
    "                                    #print(\"TP\")\n",
    "\n",
    "                                bboxes = []\n",
    "                                    \n",
    "                                #print(\"Boxes:\", output[prediction_number]['boxes'][[0]][[0]])\n",
    "                                #print(\"Scores:\", output[prediction_number]['scores'])\n",
    "                                for bbox in  output[prediction_number]['boxes'][[score_idx]][[0]].detach().cpu().numpy():\n",
    "                                    bboxes.append(list(map(int, bbox.tolist())))\n",
    "                                \n",
    "                                \n",
    "                                if (len(bboxes) == 0):\n",
    "                                    print(\"No bounding boxes found at image: \", prediction_number)\n",
    "                                    no_preds_count += 1 #count preds with no bounding box at given threshold  \n",
    "                                else:\n",
    "                                    total_number_of_predictions += 1    \n",
    "                                    example_error, point_error = calc_mean_distance_error(real_keypoints, pred_keypoints)\n",
    "                                    total_mean_distance_error += example_error\n",
    "                                \n",
    "                            \n",
    "                            \n",
    "                                example_point_error = [pt for pt in point_error]\n",
    "                                total_point_error_list.append(example_point_error)\n",
    "                                example_error_list.append(example_error)\n",
    "                                ETT_distance_error_list.append(distance_error)\n",
    "                                #print(\"Example error: \", example_error)\n",
    "                                \n",
    "                            \n",
    "                                #save_img_path = f\"F:\\ET_Tube\\saved_img_preds\\prediction_{str(prediction_number)}_{str(batch_count)}.jpg\"\n",
    "                                save_img_path = f\"F:\\ET_Tube\\saved_img_preds\\prediction_{str(prediction_number)}_{str(batch_count)}.jpg\"\n",
    "                                pred_image_file_list.append(save_img_path)\n",
    "                                visualize(image, bboxes, pred_keypoints, image_original=image, keypoints_original=real_keypoints, bboxes_original=real_bboxes, save=False, save_path = save_img_path, display=False)\n",
    "                                plt.savefig(save_img_path)\n",
    "\n",
    "                            total_attempts += 1\n",
    "\n",
    "                    print(\"Placement Correct predictions: \", correct_predictions)\n",
    "                    print(\"Placement Incorrect predictions: \", incorrect_predictions)\n",
    "                    print(\"Placement Correct real: \", correct_real)\n",
    "                    print(\"Placement Incorrect real: \", incorrect_real)\n",
    "\n",
    "                    if total_number_of_predictions != 0:\n",
    "                        total_mean_distance_error /= total_number_of_predictions\n",
    "                    print(\"Total mean Euclidean distance error: \", total_mean_distance_error)\n",
    "                    if len(ETT_distance_error_list) != 0:\n",
    "                        total_mean_ETT_distance_error = sum(ETT_distance_error_list)/len(ETT_distance_error_list)\n",
    "                    print(\"Total ETT distance error: \", total_mean_ETT_distance_error)\n",
    "                    print(\"No predictions count: \", no_preds_count)\n",
    "                    print(\"Total attempts: \", total_attempts)\n",
    "\n",
    "                    pck_threshold_large = 50\n",
    "                    pck = calculate_example_pck(total_point_error_list, threshold=pck_threshold_large)\n",
    "                    print(f\"Fraction of Correct Keypoints: {pck:0.3f}, at a threshold of {pck_threshold_large} pixels.\")\n",
    "                    pck_list.append(pck)\n",
    "\n",
    "                    print(\"TP: \", TP)\n",
    "                    print(\"FP: \", FP)\n",
    "                    print(\"TN: \", TN)\n",
    "                    print(\"FN: \", FN)\n",
    "\n",
    "                    epsilon = 1e-8 #to avoid division by 0 errors \n",
    "\n",
    "                    precision = TP / (TP + FP + epsilon)\n",
    "                    precision_list.append(precision)\n",
    "                    print(\"Precision/PPV: \", precision)\n",
    "                    recall = TP / (TP + FN + epsilon)\n",
    "                    recall_list.append(recall)\n",
    "                    print (\"Recall/Sensitivity: \", recall)\n",
    "                    NPV = TN / (TN + FN + epsilon)\n",
    "                    NPV_list.append(NPV)\n",
    "                    print(\"NPV: \", NPV)\n",
    "                    accuracy = (TP + TN) / (TP + TN + FP + FN + epsilon)\n",
    "                    accuracy_list.append(accuracy)\n",
    "                    print(\"Accuracy: \", accuracy)\n",
    "                    specificity = TN / (TN + FP + epsilon)\n",
    "                    specificity_list.append(specificity)\n",
    "                    print(\"Specificity: \", specificity)\n",
    "                    f1_score = 2 * (precision * recall) / (precision + recall + epsilon)\n",
    "                    f1_score_list.append(f1_score)\n",
    "                    print(\"F1 Score: \", f1_score)\n",
    "\n",
    "\n",
    "                    print(\"Completed at: \", datetime.datetime.now().strftime(\"%H:%M:%S\"))\n",
    "                # Reset the model weights for the next fold\n",
    "                print(\"Resetting model weights\")\n",
    "                model.apply(reset_weights)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "def train_and_evaluate(device, data_loader_train, data_loader_test):\n",
    "\n",
    "    print(\"Starting at: \", datetime.datetime.now())\n",
    "    \n",
    "    if train_model == True:\n",
    "        # Initialize the dataloaders\n",
    "        # You need to define these according to your application\n",
    "        data_loader_train = data_loader_train \n",
    "        data_loader_val = data_loader_test\n",
    "\n",
    "        example_error_list = []\n",
    "        pred_image_file_list = []\n",
    "        total_mean_distance_error = 0\n",
    "        total_mean_ETT_distance_error = 0\n",
    "        total_number_of_predictions = 0\n",
    "        no_preds_count = 0\n",
    "        total_attempts = 0\n",
    "        batch_count = 0\n",
    "        correct_predictions = 0\n",
    "        incorrect_predictions = 0\n",
    "        correct_real = 0\n",
    "        incorrect_real = 0\n",
    "        TP = 0\n",
    "        FP = 0\n",
    "        TN = 0\n",
    "        FN = 0\n",
    "        \n",
    "        model = get_model(2)\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        optimizer = torch.optim.SGD(params, lr=0.0005, momentum=0.8, weight_decay=0.0005)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.3)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.to(device)\n",
    "            train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=100)\n",
    "            lr_scheduler.step()\n",
    "            save_checkpoint(model, optimizer, epoch, 1, save_dir)\n",
    "        #Evaluate the model on the validation set\n",
    "        with torch.inference_mode():\n",
    "\n",
    "            batch_count = 0\n",
    "            iterator = iter(data_loader_val)\n",
    "            for item in iterator:\n",
    "                try:\n",
    "                    images, targets = item\n",
    "                    images = list(image.to(device) for image in images)\n",
    "                except StopIteration as e:\n",
    "                    print(\"StopIteration exception handled at batch: \", batch_count)\n",
    "                    \n",
    "                #model.to(device)            \n",
    "                #model.eval()\n",
    "                output = model(images)   \n",
    "                #print(\"output: \", output)\n",
    "\n",
    "                batch_count += 1 \n",
    "\n",
    "                for prediction_number in range(len(images)):\n",
    "                    \n",
    "                    real_keypoints = [] #list of keypoints for each image in batch\n",
    "\n",
    "                    #unpacking the targets, this is a pain but works to remove the 0/1 visibility dim (which we do not need because all keypoints are visible)\n",
    "                    for kps in targets[prediction_number]['keypoints']:\n",
    "                        real_keypoints.append([list(map(int, kp[:2])) for kp in kps])\n",
    "                    distance_real = calc_distance(real_keypoints[0][0], real_keypoints[0][1])\n",
    "                    #print(\"Real keypoints: \", real_keypoints)\n",
    "                \n",
    "                    \n",
    "                    real_bboxes = targets[prediction_number]['boxes'].int().tolist()\n",
    "                    \n",
    "                    #permute(1,2,0) converts the tensor to numpy array. The tensor is in the format (C, H, W) and numpy array is in the format (H, W, C).\n",
    "                    #detach().cpu().numpy() detaches the tensor from the graph and converts it to numpy array and moves it to CPU.\n",
    "                    image = (images[prediction_number].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)  \n",
    "                    \n",
    "                    scores = output[prediction_number]['scores'].detach().cpu().numpy()\n",
    "                    if len(scores) == 0:\n",
    "                        print(\"No keypoints found at image: \", prediction_number)\n",
    "                        no_preds_count += 1\n",
    "                        break\n",
    "\n",
    "\n",
    "                    high_scores_idxs = np.where(scores > 0)[0].tolist() # Indexes of boxes with scores > 0.1\n",
    "                    #print(\"High Score idxs: \", high_scores_idxs)\n",
    "                    #print(\"Raw NMS Boxes len: \", len(output[0]['boxes'][high_scores_idxs]))\n",
    "                    #print(\"Raw NMS scores len: \", len(output[0]['scores'][high_scores_idxs]))\n",
    "                    try:\n",
    "                        #print(\"Box idx:\", output[0]['boxes'][high_scores_idxs])\n",
    "                        #print(\"Box scores:\", output[0]['scores'][high_scores_idxs])\n",
    "                        post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n",
    "                    except:\n",
    "                        print(\"NMS exception handled at batch: \", batch_count)\n",
    "                        post_nms_idxs = 0\n",
    "                        continue\n",
    "                    \n",
    "                    #print(\"-----------Post NMS idxs:-----------\", post_nms_idxs)\n",
    "\n",
    "                    #Making images based on keypoints_scores, instead of bbox scores now\n",
    "                    #print(\"Raw Keypoint scores: \", output[prediction_number]['keypoints_scores'])\n",
    "                    keypoint_scores = output[prediction_number]['keypoints_scores'][post_nms_idxs]\n",
    "                    #print(\"Keypoint scores: \", keypoint_scores)\n",
    "                    #print(\"Keypoint scores: \", keypoint_scores)\n",
    "                    score_idx = get_best_keypoints(keypoint_scores)\n",
    "                    #print(\"Best Keypoints IDX: \", score_idx)\n",
    "                    #score_idx = 0\n",
    "                    pred_keypoints = []\n",
    "                    keypoints = output[prediction_number]['keypoints'][score_idx].detach().cpu().numpy()\n",
    "                \n",
    "                    for kp in keypoints:\n",
    "                        kp = list(map(int, kp)) #convert (x,y) coords in each keypoint to int\n",
    "                        pred_keypoints.append(kp[:2])\n",
    "\n",
    "\n",
    "                    if len(pred_keypoints) != 0:\n",
    "                        pred_keypoints = [pred_keypoints] #convert to list of lists to match real_keypoints format\n",
    "                        #print(\"Pred keypoints: \", pred_keypoints)\n",
    "                    \n",
    "                        distance_pred = calc_distance(pred_keypoints[0][0], pred_keypoints[0][1])\n",
    "                        distance_error = abs(distance_pred - distance_real)\n",
    "                    \n",
    "                    #print(\"Real ETT to Carina:\", distance_real/50)\n",
    "                    #print(\"Prediencted ETT to Carina:\", distance_pred/50)\n",
    "                    #print(\"ETT to Carina Distance error:\" , distance_error/50)\n",
    "\n",
    "                        if ((distance_real/50) <= 3) or ((distance_real/50) >= 7):\n",
    "                            incorrect_real += 1\n",
    "                        else:\n",
    "                            correct_real += 1\n",
    "\n",
    "                        if ((distance_pred/50) <= 3) or ((distance_pred/50) >= 7):\n",
    "                            incorrect_predictions += 1\n",
    "                        else:\n",
    "                            correct_predictions += 1\n",
    "\n",
    "                        if (((distance_real/50) <= 3) or ((distance_real/50) >= 7)) and (((distance_pred/50) <= 3) or ((distance_pred/50) >= 7)):\n",
    "                            TN +=1\n",
    "                            #print(\"TN\")\n",
    "                        elif (((distance_real/50) <= 3) or ((distance_real/50) >= 7)) and (((distance_pred/50) > 3) or ((distance_pred/50) < 7)):\n",
    "                            FN +=1\n",
    "                            #print(\"FN\")\n",
    "                        elif (((distance_real/50) > 3) or ((distance_real/50) < 7)) and (((distance_pred/50) <= 3) or ((distance_pred/50) >= 7)):\n",
    "                            FP +=1\n",
    "                            #print(\"FP\")\n",
    "                        elif (((distance_real/50) > 3) or ((distance_real/50) < 7)) and (((distance_pred/50) > 3) or ((distance_pred/50) < 7)):\n",
    "                            TP +=1\n",
    "                            #print(\"TP\")\n",
    "\n",
    "                        bboxes = []\n",
    "                            \n",
    "                        #print(\"Boxes:\", output[prediction_number]['boxes'][[0]][[0]])\n",
    "                        #print(\"Scores:\", output[prediction_number]['scores'])\n",
    "                        for bbox in  output[prediction_number]['boxes'][[score_idx]][[0]].detach().cpu().numpy():\n",
    "                            bboxes.append(list(map(int, bbox.tolist())))\n",
    "                        \n",
    "                        \n",
    "                        if (len(bboxes) == 0):\n",
    "                            print(\"No bounding boxes found at image: \", prediction_number)\n",
    "                            no_preds_count += 1 #count preds with no bounding box at given threshold  \n",
    "                        else:\n",
    "                            total_number_of_predictions += 1    \n",
    "                            example_error, point_error = calc_mean_distance_error(real_keypoints, pred_keypoints)\n",
    "                            total_mean_distance_error += example_error\n",
    "                        \n",
    "                    \n",
    "                    \n",
    "                        example_point_error = [pt for pt in point_error]\n",
    "                        total_point_error_list.append(example_point_error)\n",
    "                        example_error_list.append(example_error)\n",
    "                        ETT_distance_error_list.append(distance_error)\n",
    "                        #print(\"Example error: \", example_error)\n",
    "                        \n",
    "                    \n",
    "                        #save_img_path = f\"F:\\ET_Tube\\saved_img_preds\\prediction_{str(prediction_number)}_{str(batch_count)}.jpg\"\n",
    "                        save_img_path = f\"F:\\ET_Tube\\saved_img_preds\\prediction_{str(prediction_number)}_{str(batch_count)}.jpg\"\n",
    "                        pred_image_file_list.append(save_img_path)\n",
    "                        visualize(image, bboxes, pred_keypoints, image_original=image, keypoints_original=real_keypoints, bboxes_original=real_bboxes, save=False, save_path = save_img_path, display=False)\n",
    "                        plt.savefig(save_img_path)\n",
    "\n",
    "                    total_attempts += 1\n",
    "\n",
    "            print(\"Placement Correct predictions: \", correct_predictions)\n",
    "            print(\"Placement Incorrect predictions: \", incorrect_predictions)\n",
    "            print(\"Placement Correct real: \", correct_real)\n",
    "            print(\"Placement Incorrect real: \", incorrect_real)\n",
    "\n",
    "            if total_number_of_predictions != 0:\n",
    "                total_mean_distance_error /= total_number_of_predictions\n",
    "            print(\"Total mean Euclidean distance error: \", total_mean_distance_error)\n",
    "            if len(ETT_distance_error_list) != 0:\n",
    "                total_mean_ETT_distance_error = sum(ETT_distance_error_list)/len(ETT_distance_error_list)\n",
    "            print(\"Total ETT distance error: \", total_mean_ETT_distance_error)\n",
    "            print(\"No predictions count: \", no_preds_count)\n",
    "            print(\"Total attempts: \", total_attempts)\n",
    "\n",
    "            pck_threshold_large = 50\n",
    "            pck = calculate_example_pck(total_point_error_list, threshold=pck_threshold_large)\n",
    "            print(f\"Fraction of Correct Keypoints: {pck:0.3f}, at a threshold of {pck_threshold_large} pixels.\")\n",
    "            pck_list.append(pck)\n",
    "\n",
    "            print(\"TP: \", TP)\n",
    "            print(\"FP: \", FP)\n",
    "            print(\"TN: \", TN)\n",
    "            print(\"FN: \", FN)\n",
    "\n",
    "            epsilon = 1e-8 #to avoid division by 0 errors \n",
    "\n",
    "            precision = TP / (TP + FP + epsilon)\n",
    "            precision_list.append(precision)\n",
    "            print(\"Precision/PPV: \", precision)\n",
    "            recall = TP / (TP + FN + epsilon)\n",
    "            recall_list.append(recall)\n",
    "            print (\"Recall/Sensitivity: \", recall)\n",
    "            NPV = TN / (TN + FN + epsilon)\n",
    "            NPV_list.append(NPV)\n",
    "            print(\"NPV: \", NPV)\n",
    "            accuracy = (TP + TN) / (TP + TN + FP + FN + epsilon)\n",
    "            accuracy_list.append(accuracy)\n",
    "            print(\"Accuracy: \", accuracy)\n",
    "            specificity = TN / (TN + FP + epsilon)\n",
    "            specificity_list.append(specificity)\n",
    "            print(\"Specificity: \", specificity)\n",
    "            f1_score = 2 * (precision * recall) / (precision + recall + epsilon)\n",
    "            f1_score_list.append(f1_score)\n",
    "            print(\"F1 Score: \", f1_score)\n",
    "\n",
    "\n",
    "            print(\"Completed at: \", datetime.datetime.now().strftime(\"%H:%M:%S\"))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_one_epoch(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    criterion = torch.nn.MSELoss() # Loss function\n",
    "    example_distance_error_list = []\n",
    "    example_distance_error_list_MSE = []\n",
    "    pred_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_count, batch  in enumerate(data_loader):\n",
    "            images, kp = batch\n",
    "            assert len(images) == len(kp)\n",
    "            for i in range(len(images)):\n",
    "                image = images[i].to(device)\n",
    "\n",
    "                # Get ground truth keypoints, remove visibility column\n",
    "                kps = torch.squeeze(kp[i]['keypoints'][:, :, :2])\n",
    "\n",
    "                keypoints_gt = kps.to(device)\n",
    "                \n",
    "                # Get predictions\n",
    "                output = model(image.unsqueeze(0))\n",
    "                \n",
    "                # Check if predictions are available\n",
    "                if len(output) > 0 and 'keypoints' in output[0] and len(output[0]['keypoints']) > 0:\n",
    "                    #get highest score keypoint preds, again without visibility column\n",
    "                    keypoints_pred = output[0]['keypoints'][0][:, :-1]\n",
    "\n",
    "                    # Compute loss\n",
    "                    loss = criterion(keypoints_pred, keypoints_gt)\n",
    "                    total_loss += loss.item()\n",
    "                    pred_count += 1\n",
    "\n",
    "                    diff = keypoints_pred - keypoints_gt\n",
    "                    squared_diff = diff ** 2\n",
    "                    example_distance_error = torch.sqrt(torch.sum(squared_diff))\n",
    "                    example_distance_error_list.append(example_distance_error)\n",
    "\n",
    "                else:\n",
    "                    print(f\"No keypoints detected for image: {i}, batch: {batch_count}\")\n",
    "\n",
    "    # Compute average loss and accuracy\n",
    "    if pred_count > 0:\n",
    "        avg_loss = total_loss / pred_count\n",
    "        avg_distance_error = torch.mean(torch.stack(example_distance_error_list))\n",
    "\n",
    "        print(f\"Average Validation Loss: {avg_loss}, Average Validation Distance Error: {avg_distance_error}\")\n",
    "        return avg_loss, avg_distance_error\n",
    "    else:\n",
    "        print(\"No keypoints detected in the entire batch.\")\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No keypoints detected for image: 1, batch: 0\n",
      "No keypoints detected for image: 2, batch: 0\n",
      "No keypoints detected for image: 3, batch: 0\n",
      "No keypoints detected for image: 3, batch: 1\n",
      "No keypoints detected for image: 3, batch: 3\n",
      "No keypoints detected for image: 4, batch: 3\n",
      "No keypoints detected for image: 1, batch: 4\n",
      "No keypoints detected for image: 4, batch: 4\n",
      "No keypoints detected for image: 0, batch: 5\n",
      "No keypoints detected for image: 3, batch: 6\n",
      "No keypoints detected for image: 2, batch: 7\n",
      "No keypoints detected for image: 4, batch: 7\n",
      "No keypoints detected for image: 0, batch: 8\n",
      "No keypoints detected for image: 2, batch: 8\n",
      "No keypoints detected for image: 3, batch: 8\n",
      "No keypoints detected for image: 0, batch: 9\n",
      "No keypoints detected for image: 3, batch: 9\n",
      "Average Validation Loss: 1296064.0639204546, Average Validation Distance Error: 2203.552001953125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1296064.0639204546, tensor(2203.5520, device='cuda:0'))"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_one_epoch(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 622774.56609375, Average Validation Distance Error: 1471.21337890625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(622774.56609375, tensor(1471.2134, device='cuda:0'))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset = ClassDataset(KEYPOINTS_FOLDER_VALID, transform=test_transform(), demo=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=5, shuffle=False, collate_fn=collate_fn)\n",
    "model = get_model()\n",
    "model.to(device)\n",
    "\n",
    "validate_one_epoch(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at:  2023-08-12 11:25:17.992356\n",
      "Epoch: [0]  [  0/467]  eta: 0:07:27  lr: 0.000002  loss: 9.4916 (9.4916)  loss_classifier: 0.6387 (0.6387)  loss_box_reg: 0.0090 (0.0090)  loss_keypoint: 8.1316 (8.1316)  loss_objectness: 0.7110 (0.7110)  loss_rpn_box_reg: 0.0013 (0.0013)  time: 0.9582  data: 0.5376  max mem: 16304\n",
      "Epoch: [0]  [100/467]  eta: 0:09:34  lr: 0.000109  loss: 7.7446 (8.7713)  loss_classifier: 0.1082 (0.3799)  loss_box_reg: 0.0087 (0.0127)  loss_keypoint: 7.2284 (7.7369)  loss_objectness: 0.5207 (0.6350)  loss_rpn_box_reg: 0.0044 (0.0068)  time: 1.5744  data: 0.5579  max mem: 19017\n",
      "Epoch: [0]  [200/467]  eta: 0:07:01  lr: 0.000216  loss: 7.1533 (8.0797)  loss_classifier: 0.0383 (0.2170)  loss_box_reg: 0.0056 (0.0108)  loss_keypoint: 6.8565 (7.3702)  loss_objectness: 0.1868 (0.4746)  loss_rpn_box_reg: 0.0056 (0.0072)  time: 1.7784  data: 0.7388  max mem: 19017\n",
      "Epoch: [0]  [300/467]  eta: 0:04:23  lr: 0.000323  loss: 6.7194 (7.6894)  loss_classifier: 0.0189 (0.1555)  loss_box_reg: 0.0004 (0.0100)  loss_keypoint: 6.6359 (7.1561)  loss_objectness: 0.0837 (0.3612)  loss_rpn_box_reg: 0.0036 (0.0066)  time: 1.5016  data: 0.4375  max mem: 19017\n",
      "Epoch: [0]  [400/467]  eta: 0:01:46  lr: 0.000430  loss: 7.1878 (7.5261)  loss_classifier: 0.0465 (0.1274)  loss_box_reg: 0.0151 (0.0123)  loss_keypoint: 7.0167 (7.0904)  loss_objectness: 0.0610 (0.2900)  loss_rpn_box_reg: 0.0027 (0.0060)  time: 1.3788  data: 0.6144  max mem: 19017\n",
      "Epoch: [0]  [466/467]  eta: 0:00:01  lr: 0.000500  loss: 6.9734 (7.4655)  loss_classifier: 0.0452 (0.1166)  loss_box_reg: 0.0229 (0.0143)  loss_keypoint: 6.8055 (7.0704)  loss_objectness: 0.0555 (0.2583)  loss_rpn_box_reg: 0.0018 (0.0058)  time: 1.7381  data: 0.5507  max mem: 19017\n",
      "Epoch: [0] Total time: 0:12:31 (1.6100 s / it)\n",
      "No keypoints detected for image: 2, batch: 0\n",
      "No keypoints detected for image: 3, batch: 0\n",
      "No keypoints detected for image: 0, batch: 3\n",
      "No keypoints detected for image: 2, batch: 3\n",
      "No keypoints detected for image: 4, batch: 3\n",
      "No keypoints detected for image: 2, batch: 4\n",
      "No keypoints detected for image: 4, batch: 4\n",
      "No keypoints detected for image: 2, batch: 6\n",
      "No keypoints detected for image: 3, batch: 6\n",
      "No keypoints detected for image: 0, batch: 7\n",
      "No keypoints detected for image: 2, batch: 7\n",
      "No keypoints detected for image: 3, batch: 7\n",
      "No keypoints detected for image: 4, batch: 7\n",
      "No keypoints detected for image: 0, batch: 8\n",
      "No keypoints detected for image: 3, batch: 8\n",
      "Average Validation Loss: 1799737.275, Average Validation Distance Error: 2605.156982421875\n",
      "Epoch: [1]  [  0/467]  eta: 0:12:16  lr: 0.000500  loss: 7.6512 (7.6512)  loss_classifier: 0.0371 (0.0371)  loss_box_reg: 0.0145 (0.0145)  loss_keypoint: 7.5613 (7.5613)  loss_objectness: 0.0357 (0.0357)  loss_rpn_box_reg: 0.0028 (0.0028)  time: 1.5777  data: 0.9186  max mem: 19017\n",
      "Epoch: [1]  [100/467]  eta: 0:08:32  lr: 0.000500  loss: 6.8766 (6.9109)  loss_classifier: 0.0449 (0.0458)  loss_box_reg: 0.0199 (0.0233)  loss_keypoint: 6.7366 (6.7777)  loss_objectness: 0.0571 (0.0592)  loss_rpn_box_reg: 0.0034 (0.0048)  time: 1.5646  data: 0.6997  max mem: 19017\n",
      "Epoch: [1]  [200/467]  eta: 0:06:08  lr: 0.000500  loss: 6.8389 (6.9198)  loss_classifier: 0.0503 (0.0490)  loss_box_reg: 0.0361 (0.0272)  loss_keypoint: 6.6062 (6.7829)  loss_objectness: 0.0434 (0.0560)  loss_rpn_box_reg: 0.0033 (0.0048)  time: 1.5715  data: 0.5448  max mem: 19017\n",
      "Epoch: [1]  [300/467]  eta: 0:03:47  lr: 0.000500  loss: 6.5429 (6.8881)  loss_classifier: 0.0391 (0.0490)  loss_box_reg: 0.0221 (0.0293)  loss_keypoint: 6.4577 (6.7515)  loss_objectness: 0.0411 (0.0539)  loss_rpn_box_reg: 0.0031 (0.0045)  time: 1.3902  data: 0.5617  max mem: 19017\n",
      "Epoch: [1]  [400/467]  eta: 0:01:31  lr: 0.000500  loss: 6.2238 (6.7950)  loss_classifier: 0.0498 (0.0486)  loss_box_reg: 0.0410 (0.0309)  loss_keypoint: 6.1004 (6.6577)  loss_objectness: 0.0382 (0.0532)  loss_rpn_box_reg: 0.0029 (0.0046)  time: 1.3507  data: 0.6222  max mem: 19017\n",
      "Epoch: [1]  [466/467]  eta: 0:00:01  lr: 0.000500  loss: 6.3318 (6.7396)  loss_classifier: 0.0482 (0.0478)  loss_box_reg: 0.0408 (0.0318)  loss_keypoint: 6.1770 (6.6029)  loss_objectness: 0.0448 (0.0525)  loss_rpn_box_reg: 0.0046 (0.0046)  time: 1.6668  data: 0.6598  max mem: 19017\n",
      "Epoch: [1] Total time: 0:10:59 (1.4128 s / it)\n",
      "Average Validation Loss: 648085.8753985596, Average Validation Distance Error: 1031.525390625\n",
      "Epoch: [2]  [  0/467]  eta: 0:09:54  lr: 0.000500  loss: 5.7381 (5.7381)  loss_classifier: 0.0249 (0.0249)  loss_box_reg: 0.0111 (0.0111)  loss_keypoint: 5.6732 (5.6732)  loss_objectness: 0.0268 (0.0268)  loss_rpn_box_reg: 0.0023 (0.0023)  time: 1.2739  data: 0.6591  max mem: 19017\n",
      "Epoch: [2]  [100/467]  eta: 0:08:20  lr: 0.000500  loss: 6.0529 (6.2050)  loss_classifier: 0.0470 (0.0456)  loss_box_reg: 0.0424 (0.0443)  loss_keypoint: 5.7753 (6.0628)  loss_objectness: 0.0404 (0.0482)  loss_rpn_box_reg: 0.0029 (0.0042)  time: 1.1514  data: 0.6137  max mem: 19017\n",
      "Epoch: [2]  [200/467]  eta: 0:06:11  lr: 0.000500  loss: 5.9125 (6.1424)  loss_classifier: 0.0447 (0.0450)  loss_box_reg: 0.0544 (0.0473)  loss_keypoint: 5.6675 (6.0003)  loss_objectness: 0.0463 (0.0456)  loss_rpn_box_reg: 0.0042 (0.0041)  time: 1.3022  data: 0.5716  max mem: 19017\n",
      "Epoch: [2]  [300/467]  eta: 0:03:46  lr: 0.000500  loss: 6.1012 (6.1587)  loss_classifier: 0.0353 (0.0434)  loss_box_reg: 0.0518 (0.0468)  loss_keypoint: 6.0026 (6.0203)  loss_objectness: 0.0386 (0.0442)  loss_rpn_box_reg: 0.0045 (0.0040)  time: 1.4244  data: 0.5967  max mem: 19017\n",
      "Epoch: [2]  [400/467]  eta: 0:01:31  lr: 0.000500  loss: 5.8131 (6.0848)  loss_classifier: 0.0416 (0.0420)  loss_box_reg: 0.0576 (0.0471)  loss_keypoint: 5.6660 (5.9483)  loss_objectness: 0.0349 (0.0434)  loss_rpn_box_reg: 0.0023 (0.0039)  time: 1.4535  data: 0.6007  max mem: 19017\n",
      "Epoch: [2]  [466/467]  eta: 0:00:01  lr: 0.000500  loss: 5.3752 (6.0345)  loss_classifier: 0.0366 (0.0414)  loss_box_reg: 0.0480 (0.0481)  loss_keypoint: 5.1439 (5.8982)  loss_objectness: 0.0302 (0.0428)  loss_rpn_box_reg: 0.0036 (0.0039)  time: 1.2758  data: 0.5966  max mem: 19017\n",
      "Epoch: [2] Total time: 0:10:48 (1.3886 s / it)\n",
      "Average Validation Loss: 26462.03726196289, Average Validation Distance Error: 158.52896118164062\n",
      "Epoch: [3]  [  0/467]  eta: 0:21:35  lr: 0.000500  loss: 5.6082 (5.6082)  loss_classifier: 0.0424 (0.0424)  loss_box_reg: 0.0249 (0.0249)  loss_keypoint: 5.5011 (5.5011)  loss_objectness: 0.0369 (0.0369)  loss_rpn_box_reg: 0.0028 (0.0028)  time: 2.7740  data: 0.4206  max mem: 19017\n",
      "Epoch: [3]  [100/467]  eta: 0:09:03  lr: 0.000500  loss: 5.6448 (5.7115)  loss_classifier: 0.0380 (0.0388)  loss_box_reg: 0.0557 (0.0553)  loss_keypoint: 5.5804 (5.5771)  loss_objectness: 0.0339 (0.0368)  loss_rpn_box_reg: 0.0027 (0.0034)  time: 1.3532  data: 0.5365  max mem: 19017\n"
     ]
    }
   ],
   "source": [
    "if train_model == True:\n",
    "    print(\"Starting at: \", datetime.datetime.now())\n",
    "\n",
    "    model = get_model()\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.0005, momentum=0.8, weight_decay=0.0005)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.3)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.to(device)\n",
    "        train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=100)\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # Validation phase\n",
    "        validate_one_epoch(model, val_loader, device)\n",
    "\n",
    "        save_checkpoint(model, optimizer, epoch, 1, save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"num_epochs = 11\n",
    "if train_model == True:\n",
    "    print(\"Starting at: \", datetime.datetime.now())\n",
    "\n",
    "    model = get_model()\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.0005, momentum=0.8, weight_decay=0.0005)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.3)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.to(device)\n",
    "        train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=100)\n",
    "        lr_scheduler.step()\n",
    "        validate_one_epoch(model, data_loader_val, device)\n",
    "        save_checkpoint(model, optimizer, epoch, 1, save_dir)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "checkpoint_path = r\"D:\\ET_Tube\\CheXpert-v1.0\\checkpoints\\checkpoint__1_8.pt\"\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_size = 1\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=test_batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_list = []\n",
    "recall_list = []\n",
    "NPV_list = []\n",
    "accuracy_list = []\n",
    "specificity_list = []\n",
    "f1_score_list = []\n",
    "ETT_distance_error_list = []\n",
    "total_point_error_list = []\n",
    "pck_list = []\n",
    "\n",
    "\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "NPV_list = []\n",
    "accuracy_list = []\n",
    "specificity_list = []\n",
    "f1_score_list = []\n",
    "ETT_distance_error_list = []\n",
    "total_point_error_list = []\n",
    "pck_list = []\n",
    "\n",
    "example_error_list = []\n",
    "pred_image_file_list = []\n",
    "total_mean_distance_error = 0\n",
    "total_mean_ETT_distance_error = 0\n",
    "total_number_of_predictions = 0\n",
    "no_preds_count = 0\n",
    "total_attempts = 0\n",
    "batch_count = 0\n",
    "correct_predictions = 0\n",
    "incorrect_predictions = 0\n",
    "correct_real = 0\n",
    "incorrect_real = 0\n",
    "TP = 0\n",
    "FP = 0\n",
    "TN = 0\n",
    "FN = 0\n",
    "\n",
    "\n",
    "#Evaluate the model on the validation set\n",
    "\n",
    "model.to(device)            \n",
    "model.eval()\n",
    "\n",
    "batch_count = 0\n",
    "iterator = iter(data_loader_test)\n",
    "for item in iterator:\n",
    "    try:\n",
    "        images, targets = item\n",
    "        images = list(image.to(device) for image in images)\n",
    "    except StopIteration as e:\n",
    "        print(\"StopIteration exception handled at batch: \", batch_count)\n",
    "        \n",
    "\n",
    "    output = model(images)   \n",
    "    #print(\"output: \", output)\n",
    "\n",
    "    batch_count += 1 \n",
    "\n",
    "    for prediction_number in range(len(images)):\n",
    "        \n",
    "        real_keypoints = [] #list of keypoints for each image in batch\n",
    "\n",
    "        #unpacking the targets, this is a pain but works to remove the 0/1 visibility dim (which we do not need because all keypoints are visible)\n",
    "        for kps in targets[prediction_number]['keypoints']:\n",
    "            real_keypoints.append([list(map(int, kp[:2])) for kp in kps])\n",
    "        distance_real = calc_distance(real_keypoints[0][0], real_keypoints[0][1])\n",
    "        #print(\"Real keypoints: \", real_keypoints)\n",
    "    \n",
    "        \n",
    "        real_bboxes = targets[prediction_number]['boxes'].int().tolist()\n",
    "        \n",
    "        #permute(1,2,0) converts the tensor to numpy array. The tensor is in the format (C, H, W) and numpy array is in the format (H, W, C).\n",
    "        #detach().cpu().numpy() detaches the tensor from the graph and converts it to numpy array and moves it to CPU.\n",
    "        image = (images[prediction_number].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)  \n",
    "        \n",
    "        scores = output[prediction_number]['scores'].detach().cpu().numpy()\n",
    "        print(\"Scores: \", scores)\n",
    "        if len(scores) == 0:\n",
    "            print(\"No keypoints found at image: \", prediction_number)\n",
    "            no_preds_count += 1\n",
    "            break\n",
    "\n",
    "\n",
    "        high_scores_idxs = np.where(scores > 0)[0].tolist() # Indexes of boxes with scores > 0.1\n",
    "        #print(\"High Score idxs: \", high_scores_idxs)\n",
    "        #print(\"Raw NMS Boxes len: \", len(output[0]['boxes'][high_scores_idxs]))\n",
    "        #print(\"Raw NMS scores len: \", len(output[0]['scores'][high_scores_idxs]))\n",
    "        print(\"Raw NMS Boxes len: \", len(output[0]['boxes'][high_scores_idxs]))\n",
    "        print(\"High Score idxs: \", high_scores_idxs)\n",
    "        print(\"Output: \", output)\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(\"Raw NMS scores len: \", len(output[0]['scores'][high_scores_idxs]))\n",
    "        \n",
    "        print(\"******************\")\n",
    "        post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n",
    "\n",
    "        #print(\"-----------Post NMS idxs:-----------\", post_nms_idxs)\n",
    "\n",
    "        #Making images based on keypoints_scores, instead of bbox scores now\n",
    "        #print(\"Raw Keypoint scores: \", output[prediction_number]['keypoints_scores'])\n",
    "        keypoint_scores = output[prediction_number]['keypoints_scores'][post_nms_idxs]\n",
    "        #print(\"Keypoint scores: \", keypoint_scores)\n",
    "        #print(\"Keypoint scores: \", keypoint_scores)\n",
    "        score_idx = get_best_keypoints(keypoint_scores)\n",
    "        #print(\"Best Keypoints IDX: \", score_idx)\n",
    "        #score_idx = 0\n",
    "        pred_keypoints = []\n",
    "        keypoints = output[prediction_number]['keypoints'][score_idx].detach().cpu().numpy()\n",
    "    \n",
    "        for kp in keypoints:\n",
    "            kp = list(map(int, kp)) #convert (x,y) coords in each keypoint to int\n",
    "            pred_keypoints.append(kp[:2])\n",
    "\n",
    "\n",
    "        if len(pred_keypoints) != 0:\n",
    "            pred_keypoints = [pred_keypoints] #convert to list of lists to match real_keypoints format\n",
    "            #print(\"Pred keypoints: \", pred_keypoints)\n",
    "        \n",
    "            distance_pred = calc_distance(pred_keypoints[0][0], pred_keypoints[0][1])\n",
    "            distance_error = abs(distance_pred - distance_real)\n",
    "        \n",
    "        #print(\"Real ETT to Carina:\", distance_real/50)\n",
    "        #print(\"Prediencted ETT to Carina:\", distance_pred/50)\n",
    "        #print(\"ETT to Carina Distance error:\" , distance_error/50)\n",
    "\n",
    "            if ((distance_real/50) <= 3) or ((distance_real/50) >= 7):\n",
    "                incorrect_real += 1\n",
    "            else:\n",
    "                correct_real += 1\n",
    "\n",
    "            if ((distance_pred/50) <= 3) or ((distance_pred/50) >= 7):\n",
    "                incorrect_predictions += 1\n",
    "            else:\n",
    "                correct_predictions += 1\n",
    "\n",
    "            if (((distance_real/50) <= 3) or ((distance_real/50) >= 7)) and (((distance_pred/50) <= 3) or ((distance_pred/50) >= 7)):\n",
    "                TN +=1\n",
    "                #print(\"TN\")\n",
    "            elif (((distance_real/50) <= 3) or ((distance_real/50) >= 7)) and (((distance_pred/50) > 3) or ((distance_pred/50) < 7)):\n",
    "                FN +=1\n",
    "                #print(\"FN\")\n",
    "            elif (((distance_real/50) > 3) or ((distance_real/50) < 7)) and (((distance_pred/50) <= 3) or ((distance_pred/50) >= 7)):\n",
    "                FP +=1\n",
    "                #print(\"FP\")\n",
    "            elif (((distance_real/50) > 3) or ((distance_real/50) < 7)) and (((distance_pred/50) > 3) or ((distance_pred/50) < 7)):\n",
    "                TP +=1\n",
    "                #print(\"TP\")\n",
    "\n",
    "            bboxes = []\n",
    "                \n",
    "            #print(\"Boxes:\", output[prediction_number]['boxes'][[0]][[0]])\n",
    "            #print(\"Scores:\", output[prediction_number]['scores'])\n",
    "            for bbox in  output[prediction_number]['boxes'][[score_idx]][[0]].detach().cpu().numpy():\n",
    "                bboxes.append(list(map(int, bbox.tolist())))\n",
    "            \n",
    "            \n",
    "            if (len(bboxes) == 0):\n",
    "                print(\"No bounding boxes found at image: \", prediction_number)\n",
    "                no_preds_count += 1 #count preds with no bounding box at given threshold  \n",
    "            else:\n",
    "                total_number_of_predictions += 1    \n",
    "                example_error, point_error = calc_mean_distance_error(real_keypoints, pred_keypoints)\n",
    "                total_mean_distance_error += example_error\n",
    "            \n",
    "        \n",
    "        \n",
    "            example_point_error = [pt for pt in point_error]\n",
    "            total_point_error_list.append(example_point_error)\n",
    "            example_error_list.append(example_error)\n",
    "            ETT_distance_error_list.append(distance_error)\n",
    "            #print(\"Example error: \", example_error)\n",
    "            \n",
    "        \n",
    "            #save_img_path = f\"F:\\ET_Tube\\saved_img_preds\\prediction_{str(prediction_number)}_{str(batch_count)}.jpg\"\n",
    "            save_img_path = f\"F:\\ET_Tube\\saved_img_preds\\prediction_{str(prediction_number)}_{str(batch_count)}.jpg\"\n",
    "            pred_image_file_list.append(save_img_path)\n",
    "            visualize(image, bboxes, pred_keypoints, image_original=image, keypoints_original=real_keypoints, bboxes_original=real_bboxes, save=False, save_path = save_img_path, display=False)\n",
    "            plt.savefig(save_img_path)\n",
    "\n",
    "        total_attempts += 1\n",
    "\n",
    "print(\"Placement Correct predictions: \", correct_predictions)\n",
    "print(\"Placement Incorrect predictions: \", incorrect_predictions)\n",
    "print(\"Placement Correct real: \", correct_real)\n",
    "print(\"Placement Incorrect real: \", incorrect_real)\n",
    "\n",
    "if total_number_of_predictions != 0:\n",
    "    total_mean_distance_error /= total_number_of_predictions\n",
    "print(\"Total mean Euclidean distance error: \", total_mean_distance_error)\n",
    "if len(ETT_distance_error_list) != 0:\n",
    "    total_mean_ETT_distance_error = sum(ETT_distance_error_list)/len(ETT_distance_error_list)\n",
    "print(\"Total ETT distance error: \", total_mean_ETT_distance_error)\n",
    "print(\"No predictions count: \", no_preds_count)\n",
    "print(\"Total attempts: \", total_attempts)\n",
    "\n",
    "pck_threshold_large = 50\n",
    "pck = calculate_example_pck(total_point_error_list, threshold=pck_threshold_large)\n",
    "print(f\"Fraction of Correct Keypoints: {pck:0.3f}, at a threshold of {pck_threshold_large} pixels.\")\n",
    "pck_list.append(pck)\n",
    "\n",
    "print(\"TP: \", TP)\n",
    "print(\"FP: \", FP)\n",
    "print(\"TN: \", TN)\n",
    "print(\"FN: \", FN)\n",
    "\n",
    "epsilon = 1e-8 #to avoid division by 0 errors \n",
    "\n",
    "precision = TP / (TP + FP + epsilon)\n",
    "precision_list.append(precision)\n",
    "print(\"Precision/PPV: \", precision)\n",
    "recall = TP / (TP + FN + epsilon)\n",
    "recall_list.append(recall)\n",
    "print (\"Recall/Sensitivity: \", recall)\n",
    "NPV = TN / (TN + FN + epsilon)\n",
    "NPV_list.append(NPV)\n",
    "print(\"NPV: \", NPV)\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN + epsilon)\n",
    "accuracy_list.append(accuracy)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "specificity = TN / (TN + FP + epsilon)\n",
    "specificity_list.append(specificity)\n",
    "print(\"Specificity: \", specificity)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall + epsilon)\n",
    "f1_score_list.append(f1_score)\n",
    "print(\"F1 Score: \", f1_score)\n",
    "\n",
    "\n",
    "print(\"Completed at: \", datetime.datetime.now().strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "eucl_err = [29.192664806593392,30.417407798028098,54.13088229772116, 30.169575240273996]\n",
    "ETT_err = [16.936977627484584, 17.40249039641634, 17.95239513280611, 18.22531862299226]\n",
    "precision_list = [0.963414634028852,  0.9390243901293873, 0.926829268179655, 0.9878048779283165]\n",
    "recall_list = [0.963414634028852, 0.9624999998796876,  0.9743589742340566, 0.9204545453499484]\n",
    "NPV_list = [0.8888888885596707, 0.8888888885596707, 0.9259259255829904, 0.7407407404663923]\n",
    "accuracy_list = [0.944954128353674, 0.9266055045021463, 0.9266055045021463, 0.9266055045021463]\n",
    "specificity_list = [ 0.8888888885596707, 0.8275862066111771, 0.8064516126430801, 0.9523809519274377]\n",
    "f1_score_list = [ 0.963414629028852,0.9506172788340193, 0.949999994884375, 0.952941171364706 ]\n",
    "pck_list = [0.991,0.991,0.987, 0.989]\n",
    "\n",
    "stat_list = [precision_list, recall_list, NPV_list, accuracy_list, specificity_list, f1_score_list, pck_list, eucl_err , ETT_err]\n",
    "\n",
    "for list_ in stat_list:\n",
    "    std_dev = statistics.stdev(list_)\n",
    "    avg = sum(list_)/len(list_)\n",
    "    print(f\"The standard deviation of list is: {std_dev:.3f}\")\n",
    "    print(f\"The average of list is: {avg:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset = ConcatDataset([dataset_train, dataset_test])\n",
    "\n",
    "# K-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for train_idx, val_idx in kf.split(combined_dataset):\n",
    "\n",
    "    val_data = [combined_dataset[i] for i in tqdm(val_idx)]\n",
    "\n",
    "    global data_loader_val\n",
    "    data_loader_val = DataLoader(val_data, batch_size=1, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(2)\n",
    "checkpoint_path = r\"D:\\ET_Tube\\CheXpert-v1.0\\checkpoints\\checkpoint__5_8.pt\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "\n",
    "    print(\"Loading model...\")\n",
    " \n",
    "    test_batch_size = 1\n",
    "    #data_loader_test = DataLoader(dataset_test, batch_size=test_batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    iterator = iter(data_loader_test)\n",
    "\n",
    "    total_mean_distance_error = 0\n",
    "    total_mean_ETT_distance_error = 0\n",
    "    ETT_distance_error_list = []\n",
    "    total_point_error_list = []\n",
    "    example_error_list = []\n",
    "    pred_image_file_list = []\n",
    "    total_number_of_predictions = 0\n",
    "    no_preds_count = 0\n",
    "    total_attempts = 0\n",
    "    batch_count = 0\n",
    "    correct_predictions = 0\n",
    "    incorrect_predictions = 0\n",
    "    correct_real = 0\n",
    "    incorrect_real = 0\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for item in iterator:\n",
    "        \n",
    "        try:\n",
    "            images, targets = item\n",
    "            \n",
    "            images = [image.to(device) for image in images]\n",
    "        except StopIteration as e:\n",
    "            print(\"StopIteration exception handled at batch: \", batch_count)\n",
    "            \n",
    "        #model.to(device)            \n",
    "        #model.eval()\n",
    "        output = model(images)   \n",
    "        #print(\"output: \", output)\n",
    "\n",
    "        batch_count += 1 \n",
    "\n",
    "        for prediction_number in range(len(images)):\n",
    "            \n",
    "            real_keypoints = [] #list of keypoints for each image in batch\n",
    "\n",
    "            #unpacking the targets, this is a pain but works to remove the 0/1 visibility dim (which we do not need because all keypoints are visible)\n",
    "            for kps in targets[prediction_number]['keypoints']:\n",
    "                real_keypoints.append([list(map(int, kp[:2])) for kp in kps])\n",
    "            \n",
    "            distance_real = calc_distance(real_keypoints[0][0], real_keypoints[0][1])\n",
    "            \n",
    "        \n",
    "            \n",
    "            real_bboxes = targets[prediction_number]['boxes'].int().tolist()\n",
    "            \n",
    "            #permute(1,2,0) converts the tensor to numpy array. The tensor is in the format (C, H, W) and numpy array is in the format (H, W, C).\n",
    "            #detach().cpu().numpy() detaches the tensor from the graph and converts it to numpy array and moves it to CPU.\n",
    "            image = (images[prediction_number].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)  \n",
    "            \n",
    "            scores = output[prediction_number]['scores'].detach().cpu().numpy()\n",
    "            if len(scores) == 0:\n",
    "                print(\"No keypoints found at image: \", prediction_number)\n",
    "                no_preds_count += 1\n",
    "                break\n",
    "\n",
    "\n",
    "            high_scores_idxs = np.where(scores > 0.1)[0].tolist() # Indexes of boxes with scores > 0.1\n",
    "            #print(\"High Score idxs: \", high_scores_idxs)\n",
    "            #print(\"Raw NMS Boxes len: \", len(output[0]['boxes'][high_scores_idxs]))\n",
    "            #print(\"Raw NMS scores len: \", len(output[0]['scores'][high_scores_idxs]))\n",
    "            post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n",
    "            #post_nms_idxs = 0\n",
    "            #print(\"-----------Post NMS idxs:-----------\", post_nms_idxs)\n",
    "\n",
    "            #Making images based on keypoints_scores, instead of bbox scores now\n",
    "            #print(\"Raw Keypoint scores: \", output[prediction_number]['keypoints_scores'])\n",
    "            keypoint_scores = output[prediction_number]['keypoints_scores'][post_nms_idxs]\n",
    "            #print(\"Keypoint scores: \", keypoint_scores)\n",
    "            #print(\"Keypoint scores: \", keypoint_scores)\n",
    "            score_idx = get_best_keypoints(keypoint_scores)\n",
    "            #print(\"Best Keypoints IDX: \", score_idx)\n",
    "            #score_idx = 0\n",
    "            pred_keypoints = []\n",
    "            keypoints = output[prediction_number]['keypoints'][score_idx].detach().cpu().numpy()\n",
    "        \n",
    "            for kp in keypoints:\n",
    "                kp = list(map(int, kp)) #convert (x,y) coords in each keypoint to int\n",
    "                pred_keypoints.append(kp[:2])\n",
    "            pred_keypoints = [pred_keypoints] #convert to list of lists to match real_keypoints format\n",
    "            #print(\"Pred keypoints: \", pred_keypoints)\n",
    "\n",
    "            distance_pred = calc_distance(pred_keypoints[0][0], pred_keypoints[0][1])\n",
    "            distance_error = abs(distance_pred - distance_real)\n",
    "            \n",
    "            #print(\"Real ETT to Carina:\", distance_real/50)\n",
    "            #print(\"Prediencted ETT to Carina:\", distance_pred/50)\n",
    "            #print(\"ETT to Carina Distance error:\" , distance_error/50)\n",
    "\n",
    "            if ((distance_real/50) <= 3) or ((distance_real/50) >= 7):\n",
    "                incorrect_real += 1\n",
    "            else:\n",
    "                correct_real += 1\n",
    "\n",
    "            if ((distance_pred/50) <= 3) or ((distance_pred/50) >= 7):\n",
    "                incorrect_predictions += 1\n",
    "            else:\n",
    "                correct_predictions += 1\n",
    "\n",
    "            if (((distance_real/50) <= 3) or ((distance_real/50) >= 7)) and (((distance_pred/50) <= 3) or ((distance_pred/50) >= 7)):\n",
    "                TN +=1\n",
    "                #print(\"TN\")\n",
    "            elif (((distance_real/50) <= 3) or ((distance_real/50) >= 7)) and (((distance_pred/50) > 3) or ((distance_pred/50) < 7)):\n",
    "                FN +=1\n",
    "                #print(\"FN\")\n",
    "            elif (((distance_real/50) > 3) or ((distance_real/50) < 7)) and (((distance_pred/50) <= 3) or ((distance_pred/50) >= 7)):\n",
    "                FP +=1\n",
    "                #print(\"FP\")\n",
    "            elif (((distance_real/50) > 3) or ((distance_real/50) < 7)) and (((distance_pred/50) > 3) or ((distance_pred/50) < 7)):\n",
    "                TP +=1\n",
    "                #print(\"TP\")\n",
    "\n",
    "            bboxes = []\n",
    "            \n",
    "            #print(\"Boxes:\", output[prediction_number]['boxes'][[0]][[0]])\n",
    "            #print(\"Scores:\", output[prediction_number]['scores'])\n",
    "            for bbox in  output[prediction_number]['boxes'][[score_idx]][[0]].detach().cpu().numpy():\n",
    "                bboxes.append(list(map(int, bbox.tolist())))\n",
    "            \n",
    "            \n",
    "            if (len(bboxes) == 0):\n",
    "                print(\"No bounding boxes found at image: \", prediction_number)\n",
    "                no_preds_count += 1 #count preds with no bounding box at given threshold  \n",
    "            else:\n",
    "                total_number_of_predictions += 1    \n",
    "                example_error, point_error = calc_mean_distance_error(real_keypoints, pred_keypoints)\n",
    "                total_mean_distance_error += example_error\n",
    "                \n",
    "            \n",
    "            total_attempts += 1\n",
    "            \n",
    "            example_point_error = [pt for pt in point_error]\n",
    "            total_point_error_list.append(example_point_error)\n",
    "            example_error_list.append(example_error)\n",
    "            ETT_distance_error_list.append(distance_error)\n",
    "            #print(\"Example error: \", example_error)\n",
    "            \n",
    "            save_img_path = f\"F:\\ET_Tube\\saved_img_preds\\prediction_{str(prediction_number)}_{str(batch_count)}.jpg\"\n",
    "            pred_image_file_list.append(save_img_path)\n",
    "            visualize(image, bboxes, pred_keypoints, image_original=image, keypoints_original=real_keypoints, bboxes_original=real_bboxes, save=False, save_path = save_img_path, display=False)\n",
    "            plt.savefig(save_img_path)\n",
    "            #break\n",
    "        #break\n",
    "\n",
    "    print(\"Placement Correct predictions: \", correct_predictions)\n",
    "    print(\"Placement Incorrect predictions: \", incorrect_predictions)\n",
    "    print(\"Placement Correct real: \", correct_real)\n",
    "    print(\"Placement Incorrect real: \", incorrect_real)\n",
    "\n",
    "\n",
    "    total_mean_distance_error /= total_number_of_predictions\n",
    "    print(\"Total mean Euclidean distance error: \", total_mean_distance_error)\n",
    "    total_mean_ETT_distance_error = sum(ETT_distance_error_list)/len(ETT_distance_error_list)\n",
    "    print(\"Total ETT distance error: \", total_mean_ETT_distance_error)\n",
    "    print(\"No predictions count: \", no_preds_count)\n",
    "    print(\"Total attempts: \", total_attempts)\n",
    "    pck_threshold_large = 50\n",
    "    pck_large = calculate_example_pck(total_point_error_list, threshold=pck_threshold_large)\n",
    "    print(f\"Fraction of Correct Keypoints: {calculate_example_pck(total_point_error_list, threshold=pck_threshold_large):0.3f}, at a threshold of {pck_threshold_large} pixels.\")\n",
    "\n",
    "    print(\"TP: \", TP)\n",
    "    print(\"FP: \", FP)\n",
    "    print(\"TN: \", TN)\n",
    "    print(\"FN: \", FN)\n",
    "\n",
    "\n",
    "    precision = TP / (TP + FP)\n",
    "    print(\"Precision/PPV: \", precision)\n",
    "    recall = TP / (TP + FN)\n",
    "    print (\"Recall/Sensitivity: \", recall)\n",
    "    NPV = TN / (TN + FN)\n",
    "    NPV_list.append(NPV)\n",
    "    print(\"NPV: \", NPV)\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    accuracy_list.append(accuracy)\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    specificity = TN / (TN + FP)\n",
    "    specificity_list.append(specificity)\n",
    "    print(\"Specificity: \", specificity)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    f1_score_list.append(f1_score)\n",
    "    print(\"F1 Score: \", f1_score)\n",
    "\n",
    "\n",
    "    print(\"Completed at: \", datetime.datetime.now().strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "\n",
    "    print(\"Loading model...\")\n",
    " \n",
    "    test_batch_size = 1\n",
    "    #data_loader_test = DataLoader(dataset_test, batch_size=test_batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    iterator = iter(data_loader_test)\n",
    "\n",
    "    total_mean_distance_error = 0\n",
    "    total_mean_ETT_distance_error = 0\n",
    "    ETT_distance_error_list = []\n",
    "    total_point_error_list = []\n",
    "    example_error_list = []\n",
    "    pred_image_file_list = []\n",
    "    total_number_of_predictions = 0\n",
    "    no_preds_count = 0\n",
    "    total_attempts = 0\n",
    "    batch_count = 0\n",
    "    correct_predictions = 0\n",
    "    incorrect_predictions = 0\n",
    "    correct_real = 0\n",
    "    incorrect_real = 0\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for item in iterator:\n",
    "        try:\n",
    "            images, targets = item\n",
    "            \n",
    "            images = [image.to(device) for image in images]\n",
    "        except StopIteration as e:\n",
    "            print(\"StopIteration exception handled at batch: \", batch_count)\n",
    "            \n",
    "        #model.to(device)            \n",
    "        #model.eval()\n",
    "        output = model(images)   \n",
    "        batch_count += 1 \n",
    "\n",
    "        for prediction_number in range(len(images)):\n",
    "            \n",
    "            real_keypoints = [] #list of keypoints for each image in batch\n",
    "\n",
    "            #unpacking the targets, this is a pain but works to remove the 0/1 visibility dim (which we do not need because all keypoints are visible)\n",
    "            for kps in targets[prediction_number]['keypoints']:\n",
    "                real_keypoints.append([list(map(int, kp[:2])) for kp in kps])\n",
    "            distance_real = calc_distance(real_keypoints[0][0], real_keypoints[0][1])\n",
    "            #print(\"Real keypoints: \", real_keypoints)\n",
    "        \n",
    "            \n",
    "            real_bboxes = targets[prediction_number]['boxes'].int().tolist()\n",
    "            \n",
    "            #permute(1,2,0) converts the tensor to numpy array. The tensor is in the format (C, H, W) and numpy array is in the format (H, W, C).\n",
    "            #detach().cpu().numpy() detaches the tensor from the graph and converts it to numpy array and moves it to CPU.\n",
    "            image = (images[prediction_number].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)  \n",
    "            \n",
    "            scores = output[prediction_number]['scores'].detach().cpu().numpy()\n",
    "            if len(scores) == 0:\n",
    "                print(\"No keypoints found at image: \", prediction_number)\n",
    "                no_preds_count += 1\n",
    "                break\n",
    "\n",
    "\n",
    "            high_scores_idxs = np.where(scores > 0.1)[0].tolist() # Indexes of boxes with scores > 0.1\n",
    "            #print(\"High Score idxs: \", high_scores_idxs)\n",
    "            #print(\"Raw NMS Boxes len: \", len(output[0]['boxes'][high_scores_idxs]))\n",
    "            #print(\"Raw NMS scores len: \", len(output[0]['scores'][high_scores_idxs]))\n",
    "            try:\n",
    "                #print(\"Box idx:\", output[0]['boxes'][high_scores_idxs])\n",
    "                #print(\"Box scores:\", output[0]['scores'][high_scores_idxs])\n",
    "                post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n",
    "            except:\n",
    "                print(\"NMS exception handled at batch: \", batch_count)\n",
    "                post_nms_idxs = 0\n",
    "                continue\n",
    "            \n",
    "            #print(\"-----------Post NMS idxs:-----------\", post_nms_idxs)\n",
    "\n",
    "            #Making images based on keypoints_scores, instead of bbox scores now\n",
    "            #print(\"Raw Keypoint scores: \", output[prediction_number]['keypoints_scores'])\n",
    "            keypoint_scores = output[prediction_number]['keypoints_scores'][post_nms_idxs]\n",
    "            #print(\"Keypoint scores: \", keypoint_scores)\n",
    "            #print(\"Keypoint scores: \", keypoint_scores)\n",
    "            score_idx = get_best_keypoints(keypoint_scores)\n",
    "            #print(\"Best Keypoints IDX: \", score_idx)\n",
    "            #score_idx = 0\n",
    "            pred_keypoints = []\n",
    "            keypoints = output[prediction_number]['keypoints'][score_idx].detach().cpu().numpy()\n",
    "        \n",
    "            for kp in keypoints:\n",
    "                kp = list(map(int, kp)) #convert (x,y) coords in each keypoint to int\n",
    "                pred_keypoints.append(kp[:2])\n",
    "\n",
    "\n",
    "            if len(pred_keypoints) != 0:\n",
    "                pred_keypoints = [pred_keypoints] #convert to list of lists to match real_keypoints format\n",
    "                #print(\"Pred keypoints: \", pred_keypoints)\n",
    "            \n",
    "                distance_pred = calc_distance(pred_keypoints[0][0], pred_keypoints[0][1])\n",
    "                distance_error = abs(distance_pred - distance_real)\n",
    "            \n",
    "            #print(\"Real ETT to Carina:\", distance_real/50)\n",
    "            #print(\"Prediencted ETT to Carina:\", distance_pred/50)\n",
    "            #print(\"ETT to Carina Distance error:\" , distance_error/50)\n",
    "\n",
    "                if ((distance_real/50) <= 3) or ((distance_real/50) >= 7):\n",
    "                    incorrect_real += 1\n",
    "                else:\n",
    "                    correct_real += 1\n",
    "\n",
    "                if ((distance_pred/50) <= 3) or ((distance_pred/50) >= 7):\n",
    "                    incorrect_predictions += 1\n",
    "                else:\n",
    "                    correct_predictions += 1\n",
    "\n",
    "                if (((distance_real/50) <= 3) or ((distance_real/50) >= 7)) and (((distance_pred/50) <= 3) or ((distance_pred/50) >= 7)):\n",
    "                    TN +=1\n",
    "                    #print(\"TN\")\n",
    "                elif (((distance_real/50) <= 3) or ((distance_real/50) >= 7)) and (((distance_pred/50) > 3) or ((distance_pred/50) < 7)):\n",
    "                    FN +=1\n",
    "                    #print(\"FN\")\n",
    "                elif (((distance_real/50) > 3) or ((distance_real/50) < 7)) and (((distance_pred/50) <= 3) or ((distance_pred/50) >= 7)):\n",
    "                    FP +=1\n",
    "                    #print(\"FP\")\n",
    "                elif (((distance_real/50) > 3) or ((distance_real/50) < 7)) and (((distance_pred/50) > 3) or ((distance_pred/50) < 7)):\n",
    "                    TP +=1\n",
    "                    #print(\"TP\")\n",
    "\n",
    "                bboxes = []\n",
    "                    \n",
    "                #print(\"Boxes:\", output[prediction_number]['boxes'][[0]][[0]])\n",
    "                #print(\"Scores:\", output[prediction_number]['scores'])\n",
    "                for bbox in  output[prediction_number]['boxes'][[score_idx]][[0]].detach().cpu().numpy():\n",
    "                    bboxes.append(list(map(int, bbox.tolist())))\n",
    "                \n",
    "                \n",
    "                if (len(bboxes) == 0):\n",
    "                    print(\"No bounding boxes found at image: \", prediction_number)\n",
    "                    no_preds_count += 1 #count preds with no bounding box at given threshold  \n",
    "                else:\n",
    "                    total_number_of_predictions += 1    \n",
    "                    example_error, point_error = calc_mean_distance_error(real_keypoints, pred_keypoints)\n",
    "                    total_mean_distance_error += example_error\n",
    "                \n",
    "            \n",
    "            \n",
    "                example_point_error = [pt for pt in point_error]\n",
    "                total_point_error_list.append(example_point_error)\n",
    "                example_error_list.append(example_error)\n",
    "                ETT_distance_error_list.append(distance_error)\n",
    "                #print(\"Example error: \", example_error)\n",
    "                \n",
    "            \n",
    "                #save_img_path = f\"F:\\ET_Tube\\saved_img_preds\\prediction_{str(prediction_number)}_{str(batch_count)}.jpg\"\n",
    "                save_img_path = f\"F:\\ET_Tube\\saved_img_preds\\prediction_{str(prediction_number)}_{str(batch_count)}.jpg\"\n",
    "                pred_image_file_list.append(save_img_path)\n",
    "                visualize(image, bboxes, pred_keypoints, image_original=image, keypoints_original=real_keypoints, bboxes_original=real_bboxes, save=False, save_path = save_img_path, display=False)\n",
    "                plt.savefig(save_img_path)\n",
    "\n",
    "            total_attempts += 1\n",
    "\n",
    "    print(\"Placement Correct predictions: \", correct_predictions)\n",
    "    print(\"Placement Incorrect predictions: \", incorrect_predictions)\n",
    "    print(\"Placement Correct real: \", correct_real)\n",
    "    print(\"Placement Incorrect real: \", incorrect_real)\n",
    "\n",
    "    if total_number_of_predictions != 0:\n",
    "        total_mean_distance_error /= total_number_of_predictions\n",
    "    print(\"Total mean Euclidean distance error: \", total_mean_distance_error)\n",
    "    if len(ETT_distance_error_list) != 0:\n",
    "        total_mean_ETT_distance_error = sum(ETT_distance_error_list)/len(ETT_distance_error_list)\n",
    "    print(\"Total ETT distance error: \", total_mean_ETT_distance_error)\n",
    "    print(\"No predictions count: \", no_preds_count)\n",
    "    print(\"Total attempts: \", total_attempts)\n",
    "\n",
    "    pck_threshold_large = 50\n",
    "    pck = calculate_example_pck(total_point_error_list, threshold=pck_threshold_large)\n",
    "    print(f\"Fraction of Correct Keypoints: {pck:0.3f}, at a threshold of {pck_threshold_large} pixels.\")\n",
    "    pck_list.append(pck)\n",
    "\n",
    "    print(\"TP: \", TP)\n",
    "    print(\"FP: \", FP)\n",
    "    print(\"TN: \", TN)\n",
    "    print(\"FN: \", FN)\n",
    "\n",
    "    epsilon = 1e-8 #to avoid division by 0 errors \n",
    "\n",
    "    precision = TP / (TP + FP + epsilon)\n",
    "    precision_list.append(precision)\n",
    "    print(\"Precision/PPV: \", precision)\n",
    "    recall = TP / (TP + FN + epsilon)\n",
    "    recall_list.append(recall)\n",
    "    print (\"Recall/Sensitivity: \", recall)\n",
    "    NPV = TN / (TN + FN + epsilon)\n",
    "    NPV_list.append(NPV)\n",
    "    print(\"NPV: \", NPV)\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN + epsilon)\n",
    "    accuracy_list.append(accuracy)\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    specificity = TN / (TN + FP + epsilon)\n",
    "    specificity_list.append(specificity)\n",
    "    print(\"Specificity: \", specificity)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall + epsilon)\n",
    "    f1_score_list.append(f1_score)\n",
    "    print(\"F1 Score: \", f1_score)\n",
    "\n",
    "\n",
    "    print(\"Completed at: \", datetime.datetime.now().strftime(\"%H:%M:%S\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "TP, FP, FN, TN = 0, 0, 0, 0\n",
    "\n",
    "confusion_matrix = [\n",
    "    [TP, FP],\n",
    "    [FN, TN]\n",
    "]\n",
    "\n",
    "# Create a heatmap using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.set(font_scale=1.4)  # Increase font size\n",
    "ax = sns.heatmap(confusion_matrix, annot=True, cmap=\"YlGnBu\", fmt=\"d\", annot_kws={\"size\": 16})\n",
    "ax.set_ylim(sorted(ax.get_xlim(), reverse=True))\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Predicted labels')\n",
    "ax.set_ylabel('True labels')\n",
    "ax.set_title('Confusion Matrix for ETT Placement')\n",
    "\n",
    "# Set tick labels\n",
    "ax.xaxis.set_ticklabels(['Correct ETT Placement', 'Incorrect ETT Placement'])\n",
    "ax.yaxis.set_ticklabels(['Correct ETT Placement', 'Incorrect ETT Placement'])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"precision = TP / (TP + FP)\n",
    "print(\"Precision/PPV: \", precision)\n",
    "recall = TP / (TP + FN)\n",
    "print (\"Recall/Sensitivity: \", recall)\n",
    "NPV = TN / (TN + FN)\n",
    "print(\"NPV: \", NPV)\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "specificity = TN / (TN + FP)\n",
    "print(\"Specificity: \", specificity)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "print(\"F1 Score: \", f1_score)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_thumbnail(path):\n",
    "    i = Image.open(path)\n",
    "    i.thumbnail((500, 250), Image.Resampling.LANCZOS)\n",
    "    return i\n",
    "\n",
    "def image_base64(im):\n",
    "    if isinstance(im, str):\n",
    "        im = get_thumbnail(im)\n",
    "    with BytesIO() as buffer:\n",
    "        im.save(buffer, 'jpeg')\n",
    "        return base64.b64encode(buffer.getvalue()).decode()\n",
    "\n",
    "def image_formatter(im):\n",
    "    return f'<img src=\"data:image/jpeg;base64,{image_base64(im)}\">'\n",
    "\n",
    "df = pd.DataFrame(example_error_list, columns=['Example Error'])\n",
    "df['pck_small'] = pck_small\n",
    "df['pck_large'] = pck_large\n",
    "df['file'] = pred_image_file_list\n",
    "df['image'] = df.file.map(lambda f: get_thumbnail(f))\n",
    "df.head()\n",
    "\n",
    "HTML(df[['Example Error', 'image']].to_html(formatters={'image': image_formatter}, escape=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workbook = Workbook()\n",
    "worksheet = workbook.active\n",
    "\n",
    "# resize cells\n",
    "for row in range(2, len(df['file'])+2):\n",
    "    for col in range(1,2):\n",
    "        worksheet.row_dimensions[row].height = 400\n",
    "        col_letter = get_column_letter(col)\n",
    "        worksheet.column_dimensions[col_letter].width = 150\n",
    "        \n",
    "for col in range(2,50):\n",
    "    col_letter = get_column_letter(col)\n",
    "    worksheet.column_dimensions[col_letter].width = 30\n",
    "         \n",
    "# insert images\n",
    "for index, image in enumerate(df['file']):\n",
    "    if index == 0:\n",
    "        worksheet.cell(row=index+1, column=1, value=\"Image\")\n",
    "        worksheet.add_image(ExcelImage(image), anchor='A'+str(index+2))\n",
    "    else:\n",
    "        worksheet.add_image(ExcelImage(image), anchor='A'+str(index+2))\n",
    "\n",
    "excel_cell_values = []\n",
    "for excel_cell_value in df['Example Error']:\n",
    "    excel_cell_values.append(excel_cell_value)\n",
    "\n",
    "for index, excel_cell_value in enumerate(excel_cell_values):\n",
    "    if index == 0:\n",
    "        worksheet.cell(row=index+1, column=2, value=\"Example Error\")\n",
    "        worksheet.cell(row=index+2, column=2, value=excel_cell_value)\n",
    "    else:\n",
    "        worksheet.cell(row=index+2, column=2, value=excel_cell_value)\n",
    "\n",
    "# Find the last column\n",
    "last_column = worksheet.max_column\n",
    "\n",
    "# Add values to the last column\n",
    "worksheet.cell(row=1, column=last_column+1).value = f'Total Mean Euclidean Distance Error: {total_mean_distance_error}'\n",
    "worksheet.cell(row=2, column=last_column+1).value = f'No predictions count: {no_preds_count}'\n",
    "worksheet.cell(row=6, column=last_column+1).value = f'Fraction of Correct Keypoints at threshold of {pck_threshold_small} pixels: {pck_small}'\n",
    "worksheet.cell(row=7, column=last_column+1).value = f'Fraction of Correct Keypoints at threshold of {pck_threshold_large} pixels: {pck_large}'\n",
    "\n",
    "# save workbook\n",
    "workbook.save(workbook_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_predict(image_file):\n",
    "    raw_img = cv2.imread(image_file)\n",
    "    aspect_ratio = raw_img.shape[0]/raw_img.shape[1]\n",
    "\n",
    "    if aspect_ratio < 1.5 and aspect_ratio > 0.5:\n",
    "        raw_img = cv2.resize(raw_img, (456, 456))\n",
    "        test_img = np.moveaxis(raw_img, -1, 0)\n",
    "        test_img= np.expand_dims(test_img, axis=0)\n",
    "        test_img = torch.from_numpy(test_img).float().to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.to(device)            \n",
    "            model.eval()\n",
    "            output = model(test_img)\n",
    "        \n",
    "        scores = output[0]['keypoints_scores']\n",
    "        score_idx = get_best_keypoints(scores)\n",
    "\n",
    "        keypoints = output[0]['keypoints'][score_idx].detach().cpu().numpy()\n",
    "        for idx, kp in enumerate(keypoints):\n",
    "            current_keypoint = kp[:2].astype(int)\n",
    "            raw_img = cv2.circle(raw_img, current_keypoint, 1, (255,255,0), 10)\n",
    "            image_original = cv2.putText(raw_img, \" \" + keypoints_classes_ids2names[idx], current_keypoint, cv2.FONT_HERSHEY_PLAIN, 2, (255,0,0), 2, cv2.LINE_AA)\n",
    "\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.imshow(raw_img)\n",
    "    else:\n",
    "        print(f\"Image aspect ratio is {aspect_ratio:0.2F} it but be between 0.5 - 1.5\")\n",
    "\n",
    "        \n",
    "#test_img = \"/mnt/c/Users/nprim/Downloads/F1.jpg\"\n",
    "#process_and_predict(test_img) # random picture from the internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
