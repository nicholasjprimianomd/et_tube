{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json, cv2, numpy as np, matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "import torchvision\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision.models.detection import KeypointRCNN\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import transforms, utils, engine, train\n",
    "from utils import collate_fn\n",
    "from engine import train_one_epoch, evaluate\n",
    "import random\n",
    "import base64\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from IPython.display import HTML\n",
    "from torchsummary import summary\n",
    "import math\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.drawing.image import Image as ExcelImage\n",
    "from openpyxl.utils import get_column_letter\n",
    "import glob\n",
    "from natsort import natsorted\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "\n",
    "print(\"Using torch\", torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 27\n",
    "\n",
    "train_model = True\n",
    "load_weights = False\n",
    "make_labels = False\n",
    "\n",
    "num_keypoints = 2\n",
    "save_path = rf'F:\\ET_Tube\\CheXpert-v1.0\\weights\\test_weights_{datetime.datetime.now().strftime(\"%H_%M_%S\")}.pth'\n",
    "save_dir =  r\"D:\\ET_Tube\\CheXpert-v1.0\\checkpoints\" #checkpoint directory\n",
    "workbook_path = rf'F:\\ET_Tube\\CheXpert-v1.0\\predictions_{datetime.datetime.now().strftime(\"%H_%M_%S\")}.xlsx'\n",
    "\n",
    "batch_size = 2\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYPOINTS_FOLDER_TRAIN = fr\"C:\\Users\\nprim\\Downloads\\ET_TUBE.v{num}i.yolov7pytorch\\train\"\n",
    "KEYPOINTS_FOLDER_VALID = fr\"C:\\Users\\nprim\\Downloads\\ET_TUBE.v{num}i.yolov7pytorch\\valid\"\n",
    "KEYPOINTS_FOLDER_TEST =  fr\"C:\\Users\\nprim\\Downloads\\ET_TUBE.v{num}i.yolov7pytorch\\test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if make_labels:\n",
    "    import json\n",
    "    import zipfile\n",
    "    path_to_zip_file = fr\"C:\\Users\\nprim\\Downloads\\ET_TUBE.v{num}i.yolov7pytorch.zip\"\n",
    "    \n",
    "    BASE = fr\"C:\\Users\\nprim\\Downloads\\ET_TUBE.v{num}i.yolov7pytorch\"\n",
    "    with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(BASE)\n",
    "\n",
    "    SET = [\"/train\", \"/valid\", \"/test\"]\n",
    "    #SET = [\"/train\",  \"/test\"]\n",
    "    #SET = [\"/train\", \"/valid\"]\n",
    "    #SET = [\"/train\"]\n",
    "    #SET = [\"/test\"]\n",
    "  \n",
    "    keypoint_names =  ['Carina', 'ET']\n",
    "\n",
    "    def dump2json(bboxes, keypoints_sorted, file_json):\n",
    "        annotations = {}\n",
    "        annotations['bboxes'], annotations['keypoints'] = bboxes, keypoints_sorted\n",
    "\n",
    "        with open(file_json, \"w\") as f:\n",
    "            json.dump(annotations, f)\n",
    "\n",
    "    def converter(file_labels, file_image, keypoint_names):\n",
    "\n",
    "        img = cv2.imread(file_image)\n",
    "        img_w, img_h = img.shape[1], img.shape[0]\n",
    "        \n",
    "        with open(file_labels) as f:\n",
    "            lines_txt = f.readlines()\n",
    "            lines = []\n",
    "            for line in lines_txt:\n",
    "                lines.append([int(line.split()[0])] + [round(float(el), 5) for el in line.split()[1:]])\n",
    "        \n",
    "        bboxes = []\n",
    "        keypoints = []\n",
    "\n",
    "        # Convert normalized coordinates to absolute coordinates\n",
    "        for line in lines:\n",
    "            # Number 0 is a class of rectangles related to bounding boxes.\n",
    "            if line[0] == 2:\n",
    "                x_c, y_c, w, h = round(line[1] * img_w), round(line[2] * img_h), round(line[3] * img_w), round(line[4] * img_h)\n",
    "                bboxes.append([round(x_c - w/2), round(y_c - h/2), round(x_c + w/2), round(y_c + h/2)])\n",
    "\n",
    "            elif line[0] == 0 or line[0] == 1: #append all other keypoints without class change\n",
    "                kp_id, x_c, y_c = line[0], round(line[1] * img_w), round(line[2] * img_h) \n",
    "                keypoints.append([kp_id, x_c, y_c])\n",
    "                \n",
    "\n",
    "        # iterating over each keypoint and looking to which bounding box it matches, dont need this for patellas\n",
    "        keypoints_sorted = [[[] for _ in keypoint_names] for _ in bboxes]\n",
    "\n",
    "        for kp in keypoints:\n",
    "            kp_id, kp_x, kp_y = kp[0], kp[1], kp[2]\n",
    "            for bbox_idx, bbox in enumerate(bboxes):\n",
    "                x1, y1, x2, y2 = bbox[0], bbox[1], bbox[2], bbox[3]\n",
    "                if x1 < kp_x < x2 and y1 < kp_y < y2:\n",
    "                    keypoints_sorted[bbox_idx][kp_id] = [kp_x, kp_y, 1] # All keypoints are visible\n",
    "                    \n",
    "        return bboxes, keypoints_sorted\n",
    "\n",
    "\n",
    "    for i in range(len(SET)):\n",
    "        IMAGES = BASE + SET[i] + \"/images\"\n",
    "        LABELS = BASE + SET[i] + \"/labels\"\n",
    "        ANNOTATIONS = BASE +  SET[i] + \"/annotations\"\n",
    "        \n",
    "        files_names = [file.split('.jpg')[0] for file in os.listdir(IMAGES)]\n",
    "\n",
    "        for file in tqdm((files_names), desc =f\"Set: {SET[i]}\"):\n",
    "        #for file in (files_names):\n",
    "            file_labels = os.path.join(LABELS, file + \".txt\")\n",
    "            file_image = os.path.join(IMAGES, file + \".jpg\")\n",
    "\n",
    "            #img = cv2.imread(file_image)\n",
    "            #if img.shape[0] != img.shape[1]:\n",
    "                #print(\"Non square image:\", file_image)\n",
    "                \n",
    "            bboxes, keypoints_sorted = converter(file_labels, file_image, keypoint_names)\n",
    "\n",
    "            for i in keypoints_sorted:\n",
    "                a,b = i\n",
    "                if (len(a) != 3 or len (b) != 3) :\n",
    "                    print(\"Error in file\", file)\n",
    "\n",
    "            if not os.path.exists(ANNOTATIONS):\n",
    "                os.makedirs(ANNOTATIONS)\n",
    "\n",
    "            dump2json(bboxes, keypoints_sorted, os.path.join(ANNOTATIONS, file + '.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMG_SIZE = 512\n",
    "\n",
    "def train_transform():\n",
    "    return A.Compose([\n",
    "        A.GaussNoise(var_limit=(10, 50), mean=0, per_channel=True, p=0.5), # Simulate sensor noise\n",
    "        A.ISONoise(intensity=(0.1, 0.5), p=0.5), # Additional noise type\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5), # Adjust brightness & contrast\n",
    "        A.Rotate(limit=15, border_mode=cv2.BORDER_CONSTANT, value=0, p=0.5), # Small rotations\n",
    "        A.HorizontalFlip(p=0.5), # Horizontal flip (use with caution, depending on orientation-specific structures)\n",
    "        #A.ElasticTransform(alpha=1, sigma=50, p=0.5), # Elastic deformation\n",
    "        A.CLAHE(clip_limit=(1,4), p=1) # Enhance local contrast\n",
    "        #A.Resize(IMG_SIZE, IMG_SIZE, p=1) if IMG_SIZE else None # Resize if needed\n",
    "    ],\n",
    "    keypoint_params=A.KeypointParams(format='xy'),\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bboxes_labels'])\n",
    "    )\n",
    "\n",
    "def test_valid_transform():\n",
    "    return A.Compose([\n",
    "        A.Sequential([\n",
    "            A.CLAHE(clip_limit=(1,4), p=1)\n",
    "            #A.Resize(IMG_SIZE, IMG_SIZE, p=1)\n",
    "        ], p=1)\n",
    "    ],\n",
    "    keypoint_params=A.KeypointParams(format='xy'), \n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bboxes_labels'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, demo=False):                \n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.demo = demo # Use demo=True if you need transformed and original images (for example, for visualization purposes)\n",
    "        self.imgs_files = sorted(os.listdir(os.path.join(root, \"images\")))\n",
    "        self.annotations_files = sorted(os.listdir(os.path.join(root, \"annotations\")))\n",
    "\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root, \"images\", self.imgs_files[idx])\n",
    "        annotations_path = os.path.join(self.root, \"annotations\", self.annotations_files[idx])\n",
    "\n",
    "        img_original = cv2.imread(img_path)\n",
    "        img_original = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)        \n",
    "        \n",
    "        with open(annotations_path) as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "            bboxes_original = data['bboxes']\n",
    "            keypoints_original = data['keypoints']\n",
    "                          \n",
    "            # All objects are patellas, so we can use the same label for all objects\n",
    "            bboxes_labels_original = ['ROI' for _ in bboxes_original]  \n",
    "\n",
    "\n",
    "            if self.transform:   \n",
    "                # Converting keypoints from [x,y,visibility]-format to [x, y]-format + Flattening nested list of keypoints            \n",
    "                # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "                # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]], where each keypoint is in [x, y]-format            \n",
    "                # Then we need to convert it to the following list:\n",
    "                # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2]\n",
    "                \n",
    "                keypoints_original_flattened = [el[0:2] for kp in keypoints_original for el in kp]\n",
    "\n",
    "                # Apply augmentations\n",
    "                transformed = self.transform(image=img_original, bboxes=bboxes_original, bboxes_labels=bboxes_labels_original, keypoints=keypoints_original_flattened)\n",
    "                img = transformed['image']\n",
    "                bboxes = transformed['bboxes']\n",
    "                \n",
    "                # Unflattening list transformed['keypoints']\n",
    "                # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "                # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2], where each keypoint is in [x, y]-format\n",
    "                # Then we need to convert it to the following list:\n",
    "                # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]]\n",
    "                \n",
    "                #keypoints_transformed_unflattened = np.reshape(np.array(transformed['keypoints']), (-1,3,2)).tolist()\n",
    "\n",
    "                keypoints_transformed_unflattened = [transformed['keypoints']]\n",
    "\n",
    "\n",
    "                #keypoints_transformed_unflattened = np.reshape(np.array(transformed['keypoints']), (-1,3,2)).tolist()\n",
    "                \n",
    "                # Converting transformed keypoints from [x, y]-format to [x,y,visibility]-format by appending original visibilities to transformed coordinates of keypoints\n",
    "                keypoints = []\n",
    "                for o_idx, obj in enumerate(keypoints_transformed_unflattened): # Iterating over objects\n",
    "                    obj_keypoints = []\n",
    "\n",
    "                    for k_idx, kp in enumerate(obj): # Iterating over keypoints in each object\n",
    "                        # kp - coordinates of keypoint\n",
    "                        # keypoints_original[o_idx][k_idx][2] - original visibility of keypoint\n",
    "                        obj_keypoints.append(np.array(kp).tolist() + [keypoints_original[o_idx][k_idx][2]])\n",
    "                    keypoints.append(obj_keypoints)\n",
    "        \n",
    "            else:\n",
    "                img, bboxes, keypoints = img_original, bboxes_original, keypoints_original\n",
    "                 \n",
    "            # Convert everything into a torch tensor        \n",
    "            bboxes = torch.as_tensor(bboxes, dtype=torch.float32)       \n",
    "            target = {}\n",
    "            target[\"boxes\"] = bboxes\n",
    "            target[\"labels\"] = torch.as_tensor([1 for _ in bboxes], dtype=torch.int64)\n",
    "            target[\"image_id\"] = torch.tensor([idx])\n",
    "            target[\"area\"] = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])\n",
    "            target[\"iscrowd\"] = torch.zeros(len(bboxes), dtype=torch.int64)\n",
    "\n",
    "            target[\"keypoints\"] = torch.as_tensor(keypoints, dtype=torch.float32) \n",
    "            \n",
    "            img = F.to_tensor(img)\n",
    "            \n",
    "            bboxes_original = torch.as_tensor(bboxes_original, dtype=torch.float32)\n",
    "            target_original = {}\n",
    "            target_original[\"boxes\"] = bboxes_original\n",
    "            target_original[\"labels\"] = torch.as_tensor([1 for _ in bboxes_original], dtype=torch.int64) # all objects are carinas\n",
    "            target_original[\"image_id\"] = torch.tensor([idx])\n",
    "            target_original[\"area\"] = (bboxes_original[:, 3] - bboxes_original[:, 1]) * (bboxes_original[:, 2] - bboxes_original[:, 0])\n",
    "            target_original[\"iscrowd\"] = torch.zeros(len(bboxes_original), dtype=torch.int64)\n",
    "            target_original[\"keypoints\"] = torch.as_tensor(keypoints_original, dtype=torch.float32)        \n",
    "            img_original = F.to_tensor(img_original)\n",
    "\n",
    "            if self.demo:\n",
    "                return img, target, img_original, target_original\n",
    "            else:\n",
    "                return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ClassDataset(KEYPOINTS_FOLDER_TEST, transform=train_transform(), demo=True)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "iterator = iter(data_loader)\n",
    "batch = next(iterator)\n",
    "\n",
    "keypoints_classes_ids2names = {0: 'Carina' , 1: 'ET'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(image, bboxes, keypoints, image_original=None, bboxes_original=None, keypoints_original=None, augmented=True, save=False, save_path=None, display=True):\n",
    "    fontsize = 12\n",
    "\n",
    "    for bbox in bboxes:\n",
    "        start_point = (bbox[0], bbox[1])\n",
    "        end_point = (bbox[2], bbox[3])\n",
    "        image = cv2.rectangle(image.copy(), start_point, end_point, (0,255,0), 2)\n",
    "    \n",
    "    for kps in keypoints:\n",
    "        for idx, kp in enumerate(kps):\n",
    "            image = cv2.circle(image.copy(), tuple(kp), 1, (255,0,0), 10)\n",
    "            image = cv2.putText(image.copy(), \" \" + keypoints_classes_ids2names[idx], tuple(kp), cv2.FONT_HERSHEY_PLAIN, 1, (255,0,0), 1, cv2.LINE_AA)\n",
    "\n",
    "    if image_original is None and keypoints_original is None:\n",
    "        plt.figure(figsize=(20,20))\n",
    "        plt.imshow(image)\n",
    "        \n",
    "\n",
    "    else:\n",
    "        for bbox in bboxes_original:\n",
    "            start_point = (bbox[0], bbox[1])\n",
    "            end_point = (bbox[2], bbox[3])\n",
    "            image_original = cv2.rectangle(image_original.copy(), start_point, end_point, (255,255,0), 2)\n",
    "        \n",
    "        for kps in keypoints_original:\n",
    "            for idx, kp in enumerate(kps):\n",
    "                image_original = cv2.circle(image_original, tuple(kp), 1, (255,255,0), 10)\n",
    "                image_original = cv2.putText(image_original, \" \" + keypoints_classes_ids2names[idx], tuple(kp), cv2.FONT_HERSHEY_PLAIN, 1, (255,0,0), 1, cv2.LINE_AA)\n",
    "        \n",
    "\n",
    "        f, ax = plt.subplots(1, 2, figsize=(40, 20))\n",
    "        ax[0].set_title('Original image', fontsize=fontsize)\n",
    "        if augmented:\n",
    "            ax[1].set_title('Augmented image', fontsize=fontsize)\n",
    "        else:\n",
    "            ax[1].set_title('Predicted image', fontsize=fontsize) # also augmented image\n",
    "    \n",
    "        ax[0].imshow(image_original)\n",
    "        ax[1].imshow(image)\n",
    "        \n",
    "        if(save):\n",
    "            print(\"Saving image to: \", save_path)\n",
    "            plt.gcf().set_size_inches(5, 10)\n",
    "            plt.savefig(save_path, dpi=100)\n",
    "            plt.close()\n",
    "            \n",
    "        if(display == False):\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = (batch[0][0].permute(1,2,0).numpy() * 255).astype(np.uint8)\n",
    "bboxes = batch[1][0]['boxes'].detach().cpu().numpy().astype(np.int32).tolist()\n",
    "\n",
    "keypoints = []\n",
    "for kps in batch[1][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist():\n",
    "    keypoints.append([kp[:2] for kp in kps])\n",
    "\n",
    "image_original = (batch[2][0].permute(1,2,0).numpy() * 255).astype(np.uint8)\n",
    "bboxes_original = batch[3][0]['boxes'].detach().cpu().numpy().astype(np.int32).tolist()\n",
    "\n",
    "keypoints_original = []\n",
    "for kps in batch[3][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist():\n",
    "    keypoints_original.append([kp[:2] for kp in kps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(image, bboxes, keypoints, image_original, bboxes_original, keypoints_original, augmented=True, save=False, save_path=None, display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_keypoints=2, anchor_sizes = (64, 128, 256) , anchor_ratios= (0.5, 0.83, 1.2, 2), weights_path=None):\n",
    "    \n",
    "    backbone = torchvision.models.convnext_large(weights='DEFAULT').features\n",
    "    backbone.out_channels = 1536\n",
    "\n",
    "    anchor_generator = AnchorGenerator(sizes=(anchor_sizes,), aspect_ratios=(anchor_ratios,))\n",
    "    \n",
    "    roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
    "                                                    output_size=7,\n",
    "                                                    sampling_ratio=2)\n",
    "    keypoint_roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
    "                                                             output_size=14,\n",
    "                                                             sampling_ratio=2)\n",
    "\n",
    "    model = KeypointRCNN(backbone,\n",
    "                          num_classes=2,\n",
    "                          rpn_anchor_generator=anchor_generator,\n",
    "                          box_roi_pool=roi_pooler,\n",
    "                          keypoint_roi_pool=keypoint_roi_pooler,\n",
    "                          num_keypoints=num_keypoints)        \n",
    "        \n",
    "    return model\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = ClassDataset(KEYPOINTS_FOLDER_TRAIN, transform=train_transform(), demo=False)\n",
    "dataset_valid = ClassDataset(KEYPOINTS_FOLDER_VALID, transform=test_valid_transform():, demo=False)\n",
    "dataset_test = ClassDataset(KEYPOINTS_FOLDER_TEST, transform=test_valid_transform():, demo=False)\n",
    "\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "data_loader_valid = DataLoader(dataset_valid, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_distance(kp1, kp2):\n",
    "    x1, y1 = kp1\n",
    "    x2, y2 = kp2\n",
    "    return math.sqrt((x2 - x1)**2 + (y2 - y1)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_epoch_error(model, data_loader_test, device):\n",
    "    with torch.inference_mode():\n",
    "        model.eval()\n",
    "        running_epoch_dist_error = 0\n",
    "        num_keypoint_predictions = 0\n",
    "        avg_dist_error = 0\n",
    "        for batch_idx, (images, targets) in enumerate(data_loader_test):\n",
    "            \n",
    "            if not targets:\n",
    "                #print(\"No targets available for the current batch.\")\n",
    "                continue\n",
    "\n",
    "            images = list(image.to(device) for image in images)\n",
    "            predictions = model(images)\n",
    "            for i in range(len(predictions)):\n",
    "                for idx in range(num_keypoints):\n",
    "                    try:\n",
    "                        pred = predictions[i]['keypoints'][0][idx].detach().cpu().numpy().astype(np.int32)\n",
    "                        x1, y1, _ = predictions[i]['keypoints'][0][idx].detach().cpu().numpy().astype(np.int32)\n",
    "                        kp1 = (x1, y1)\n",
    "                        x2, y2, _ = targets[i]['keypoints'][0][idx].detach().cpu().numpy().astype(np.int32)\n",
    "                        kp2 = (x2, y2)\n",
    "                        num_keypoint_predictions += 1\n",
    "                        running_epoch_dist_error += calc_distance(kp1, kp2)\n",
    "                        avg_dist_error = running_epoch_dist_error / num_keypoint_predictions\n",
    "                    except:\n",
    "                        print(\"No prediction for val image.\")\n",
    "        print(\"Running Epoch Error: \", avg_dist_error)\n",
    "        score = -avg_dist_error\n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, fold, save_dir):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'epoch': fold,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, os.path.join(save_dir, f'checkpoint__{fold}_{epoch}.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(model):\n",
    "    for name, module in model.named_modules():\n",
    "        if not name.startswith(\"backbone\"):\n",
    "            if isinstance(model, nn.Conv2d) or isinstance(model, nn.Linear) or isinstance(model, nn.ConvTranspose2d):\n",
    "                model.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_distance(kp1, kp2):\n",
    "    x1, y1 = kp1\n",
    "    x2, y2 = kp2\n",
    "    return math.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "\n",
    "def calc_mean_distance_error(real_keypoints, pred_keypoints):\n",
    "    mean_distance = 0\n",
    "    point_error = [0,0,0]\n",
    "    assert len(real_keypoints) == len(pred_keypoints)\n",
    "    for i in range(len(real_keypoints)):\n",
    "        for j in range(len(real_keypoints[i])):\n",
    "            point_error[j] = calc_distance(real_keypoints[i][j], pred_keypoints[i][j])\n",
    "            mean_distance += point_error[j]\n",
    "    return mean_distance/len(keypoints), point_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_keypoints(scores):\n",
    "    avg_scores = []\n",
    "    for score in scores:\n",
    "        avg_scores.append(torch.mean(score))\n",
    "\n",
    "    try:\n",
    "        if len(avg_scores) > 0:\n",
    "            return avg_scores.index(max(avg_scores))\n",
    "        else:\n",
    "            raise ValueError(\"avg_scores is empty. Make sure you have calculated the average scores before finding the maximum.\")\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        return []  # or some other default value\n",
    "   \n",
    "    #return avg_scores.index(max(avg_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Proportion of Correct Keypoints (PCK)\n",
    "# Threshold is the maximum distance between the predicted and ground truth keypoints\n",
    "# Here it is written (default param) as 10 pixels\n",
    "# Perhaps this should be a function of the image size?\n",
    "def calculate_example_pck(total_point_error_list, threshold=10):\n",
    "    num_correct = 0\n",
    "    flat_list = list(itertools.chain(*total_point_error_list))\n",
    "\n",
    "    for point in flat_list:\n",
    "        if point <= threshold:\n",
    "            num_correct += 1\n",
    "            \n",
    "# Calculate the PCK\n",
    "    pck = num_correct / len(flat_list)\n",
    "    return pck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_one_epoch(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    criterion = torch.nn.MSELoss() # Loss function\n",
    "    example_distance_error_list = []\n",
    "    example_distance_error_list_MSE = []\n",
    "    pred_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_count, batch  in enumerate(data_loader):\n",
    "            images, kp = batch\n",
    "            assert len(images) == len(kp)\n",
    "            for i in range(len(images)):\n",
    "                image = images[i].to(device)\n",
    "\n",
    "                # Get ground truth keypoints, remove visibility column\n",
    "                kps = torch.squeeze(kp[i]['keypoints'][:, :, :2])\n",
    "\n",
    "                keypoints_gt = kps.to(device)\n",
    "                \n",
    "                # Get predictions\n",
    "                output = model(image.unsqueeze(0))\n",
    "                \n",
    "                # Check if predictions are available\n",
    "                if len(output) > 0 and 'keypoints' in output[0] and len(output[0]['keypoints']) > 0:\n",
    "                    #get highest score keypoint preds, again without visibility column\n",
    "                    keypoints_pred = output[0]['keypoints'][0][:, :-1]\n",
    "\n",
    "                    # Compute loss\n",
    "                    loss = criterion(keypoints_pred, keypoints_gt)\n",
    "                    total_loss += loss.item()\n",
    "                    pred_count += 1\n",
    "\n",
    "                    diff = keypoints_pred - keypoints_gt\n",
    "                    squared_diff = diff ** 2\n",
    "                    example_distance_error = torch.sqrt(torch.sum(squared_diff))\n",
    "                    example_distance_error_list.append(example_distance_error)\n",
    "\n",
    "                else:\n",
    "                    print(f\"No keypoints detected for image: {i}, batch: {batch_count}\")\n",
    "\n",
    "    # Compute average loss and accuracy\n",
    "    if pred_count > 0:\n",
    "        avg_loss = total_loss / pred_count\n",
    "        avg_distance_error = torch.mean(torch.stack(example_distance_error_list))\n",
    "\n",
    "        print(f\"Average Validation Loss: {avg_loss}, Average Validation Distance Error: {avg_distance_error}\")\n",
    "        return avg_loss, avg_distance_error\n",
    "    else:\n",
    "        print(\"No keypoints detected in the entire batch.\")\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "def get_full_dataset():\n",
    "    # Define individual datasets\n",
    "    dataset_train = ClassDataset(KEYPOINTS_FOLDER_TRAIN, transform=train_transform(), demo=False)\n",
    "    dataset_valid = ClassDataset(KEYPOINTS_FOLDER_VALID, transform=test_valid_transform(), demo=False)\n",
    "\n",
    "    # Combine the datasets\n",
    "    full_dataset = ConcatDataset([dataset_train, dataset_valid])\n",
    "\n",
    "    return full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Number of folds\n",
    "K = 5\n",
    "\n",
    "# Get the full dataset\n",
    "full_dataset = get_full_dataset()\n",
    "\n",
    "# Split the dataset into K folds\n",
    "kfold = KFold(n_splits=K, shuffle=True, random_state=42)\n",
    "\n",
    "# Iterate through each fold\n",
    "for fold, (train_ids, val_ids) in enumerate(kfold.split(full_dataset)):\n",
    "    print(f\"Fold {fold + 1}/{K}\")\n",
    "\n",
    "    # Create data loaders for this fold\n",
    "    train_subset = torch.utils.data.Subset(full_dataset, train_ids)\n",
    "    val_subset = torch.utils.data.Subset(full_dataset, val_ids)\n",
    "\n",
    "\n",
    "    data_loader_train = torch.utils.data.DataLoader(train_subset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = torch.utils.data.DataLoader(val_subset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # Initialize the model for this fold\n",
    "    model = get_model()\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.0005, momentum=0.8, weight_decay=0.0005)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.3)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.to(device)\n",
    "        train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=100)\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        validate_one_epoch(model, val_loader, device)\n",
    "\n",
    "        # Save checkpoint for this fold and epoch\n",
    "        save_checkpoint(model, optimizer, epoch, fold + 1, save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model == True:\n",
    "    print(\"Starting at: \", datetime.datetime.now())\n",
    "\n",
    "    model = get_model()\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.0005, momentum=0.8, weight_decay=0.0005)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.3)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.to(device)\n",
    "        train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=100)\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        validate_one_epoch(model, val_loader, device)\n",
    "\n",
    "        save_checkpoint(model, optimizer, epoch, 1, save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_list = []\n",
    "recall_list = []\n",
    "NPV_list = []\n",
    "accuracy_list = []\n",
    "specificity_list = []\n",
    "f1_score_list = []\n",
    "ETT_distance_error_list = []\n",
    "total_point_error_list = []\n",
    "pck_list = []\n",
    "\n",
    "\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "NPV_list = []\n",
    "accuracy_list = []\n",
    "specificity_list = []\n",
    "f1_score_list = []\n",
    "ETT_distance_error_list = []\n",
    "total_point_error_list = []\n",
    "pck_list = []\n",
    "\n",
    "example_error_list = []\n",
    "pred_image_file_list = []\n",
    "total_mean_distance_error = 0\n",
    "total_mean_ETT_distance_error = 0\n",
    "total_number_of_predictions = 0\n",
    "no_preds_count = 0\n",
    "total_attempts = 0\n",
    "batch_count = 0\n",
    "correct_predictions = 0\n",
    "incorrect_predictions = 0\n",
    "correct_real = 0\n",
    "incorrect_real = 0\n",
    "TP = 0\n",
    "FP = 0\n",
    "TN = 0\n",
    "FN = 0\n",
    "\n",
    "\n",
    "#Evaluate the model on the test set\n",
    "model.to(device)            \n",
    "model.eval()\n",
    "\n",
    "batch_count = 0\n",
    "iterator = iter(data_loader_test)\n",
    "for item in iterator:\n",
    "    try:\n",
    "        images, targets = item\n",
    "        images = list(image.to(device) for image in images)\n",
    "    except StopIteration as e:\n",
    "        print(\"StopIteration exception handled at batch: \", batch_count)\n",
    "        \n",
    "\n",
    "    output = model(images)   \n",
    "    #print(\"output: \", output)\n",
    "\n",
    "    batch_count += 1 \n",
    "\n",
    "    for prediction_number in range(len(images)):\n",
    "        \n",
    "        real_keypoints = [] #list of keypoints for each image in batch\n",
    "\n",
    "        #unpacking the targets, this is a pain but works to remove the 0/1 visibility dim (which we do not need because all keypoints are visible)\n",
    "        for kps in targets[prediction_number]['keypoints']:\n",
    "            real_keypoints.append([list(map(int, kp[:2])) for kp in kps])\n",
    "        distance_real = calc_distance(real_keypoints[0][0], real_keypoints[0][1])\n",
    "        #print(\"Real keypoints: \", real_keypoints)\n",
    "    \n",
    "        \n",
    "        real_bboxes = targets[prediction_number]['boxes'].int().tolist()\n",
    "        \n",
    "        #permute(1,2,0) converts the tensor to numpy array. The tensor is in the format (C, H, W) and numpy array is in the format (H, W, C).\n",
    "        #detach().cpu().numpy() detaches the tensor from the graph and converts it to numpy array and moves it to CPU.\n",
    "        image = (images[prediction_number].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)  \n",
    "        \n",
    "        scores = output[prediction_number]['scores'].detach().cpu().numpy()\n",
    "        print(\"Scores: \", scores)\n",
    "        if len(scores) == 0:\n",
    "            print(\"No keypoints found at image: \", prediction_number)\n",
    "            no_preds_count += 1\n",
    "            break\n",
    "\n",
    "\n",
    "        high_scores_idxs = np.where(scores > 0)[0].tolist() # Indexes of boxes with scores > 0.1\n",
    "        #print(\"High Score idxs: \", high_scores_idxs)\n",
    "        #print(\"Raw NMS Boxes len: \", len(output[0]['boxes'][high_scores_idxs]))\n",
    "        #print(\"Raw NMS scores len: \", len(output[0]['scores'][high_scores_idxs]))\n",
    "        #print(\"Raw NMS Boxes len: \", len(output[0]['boxes'][high_scores_idxs]))\n",
    "        #print(\"High Score idxs: \", high_scores_idxs)\n",
    "        #print(\"Output: \", output)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #print(\"Raw NMS scores len: \", len(output[0]['scores'][high_scores_idxs]))\n",
    "        \n",
    "        #print(\"******************\")\n",
    "        post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n",
    "\n",
    "        #print(\"-----------Post NMS idxs:-----------\", post_nms_idxs)\n",
    "\n",
    "        #Making images based on keypoints_scores, instead of bbox scores now\n",
    "        #print(\"Raw Keypoint scores: \", output[prediction_number]['keypoints_scores'])\n",
    "        keypoint_scores = output[prediction_number]['keypoints_scores'][post_nms_idxs]\n",
    "        #print(\"Keypoint scores: \", keypoint_scores)\n",
    "        #print(\"Keypoint scores: \", keypoint_scores)\n",
    "        score_idx = get_best_keypoints(keypoint_scores)\n",
    "        #print(\"Best Keypoints IDX: \", score_idx)\n",
    "        #score_idx = 0\n",
    "        pred_keypoints = []\n",
    "        keypoints = output[prediction_number]['keypoints'][score_idx].detach().cpu().numpy()\n",
    "    \n",
    "        for kp in keypoints:\n",
    "            kp = list(map(int, kp)) #convert (x,y) coords in each keypoint to int\n",
    "            pred_keypoints.append(kp[:2])\n",
    "\n",
    "\n",
    "        if len(pred_keypoints) != 0:\n",
    "            pred_keypoints = [pred_keypoints] #convert to list of lists to match real_keypoints format\n",
    "            #print(\"Pred keypoints: \", pred_keypoints)\n",
    "        \n",
    "            distance_pred = calc_distance(pred_keypoints[0][0], pred_keypoints[0][1])\n",
    "            distance_error = abs(distance_pred - distance_real)\n",
    "        \n",
    "        #print(\"Real ETT to Carina:\", distance_real/50)\n",
    "        #print(\"Prediencted ETT to Carina:\", distance_pred/50)\n",
    "        #print(\"ETT to Carina Distance error:\" , distance_error/50)\n",
    "\n",
    "            if ((distance_real/50) <= 3) or ((distance_real/50) >= 7):\n",
    "                incorrect_real += 1\n",
    "            else:\n",
    "                correct_real += 1\n",
    "\n",
    "            if ((distance_pred/50) <= 3) or ((distance_pred/50) >= 7):\n",
    "                incorrect_predictions += 1\n",
    "            else:\n",
    "                correct_predictions += 1\n",
    "\n",
    "            if (((distance_real/50) <= 3) or ((distance_real/50) >= 7)) and (((distance_pred/50) <= 3) or ((distance_pred/50) >= 7)):\n",
    "                TN +=1\n",
    "                #print(\"TN\")\n",
    "            elif (((distance_real/50) <= 3) or ((distance_real/50) >= 7)) and (((distance_pred/50) > 3) or ((distance_pred/50) < 7)):\n",
    "                FN +=1\n",
    "                #print(\"FN\")\n",
    "            elif (((distance_real/50) > 3) or ((distance_real/50) < 7)) and (((distance_pred/50) <= 3) or ((distance_pred/50) >= 7)):\n",
    "                FP +=1\n",
    "                #print(\"FP\")\n",
    "            elif (((distance_real/50) > 3) or ((distance_real/50) < 7)) and (((distance_pred/50) > 3) or ((distance_pred/50) < 7)):\n",
    "                TP +=1\n",
    "                #print(\"TP\")\n",
    "\n",
    "            bboxes = []\n",
    "                \n",
    "            for bbox in  output[prediction_number]['boxes'][[score_idx]][[0]].detach().cpu().numpy():\n",
    "                bboxes.append(list(map(int, bbox.tolist())))\n",
    "            \n",
    "            \n",
    "            if (len(bboxes) == 0):\n",
    "                print(\"No bounding boxes found at image: \", prediction_number)\n",
    "                no_preds_count += 1 #count preds with no bounding box at given threshold  \n",
    "            else:\n",
    "                total_number_of_predictions += 1    \n",
    "                example_error, point_error = calc_mean_distance_error(real_keypoints, pred_keypoints)\n",
    "                total_mean_distance_error += example_error\n",
    "            \n",
    "        \n",
    "        \n",
    "            example_point_error = [pt for pt in point_error]\n",
    "            total_point_error_list.append(example_point_error)\n",
    "            example_error_list.append(example_error)\n",
    "            ETT_distance_error_list.append(distance_error)\n",
    "            \n",
    "\n",
    "            save_img_path = f\"F:\\ET_Tube\\saved_img_preds\\prediction_{str(prediction_number)}_{str(batch_count)}.jpg\"\n",
    "            pred_image_file_list.append(save_img_path)\n",
    "            visualize(image, bboxes, pred_keypoints, image_original=image, keypoints_original=real_keypoints, bboxes_original=real_bboxes, save=False, save_path = save_img_path, display=False)\n",
    "            plt.savefig(save_img_path)\n",
    "\n",
    "        total_attempts += 1\n",
    "\n",
    "print(\"Placement Correct predictions: \", correct_predictions)\n",
    "print(\"Placement Incorrect predictions: \", incorrect_predictions)\n",
    "print(\"Placement Correct real: \", correct_real)\n",
    "print(\"Placement Incorrect real: \", incorrect_real)\n",
    "\n",
    "if total_number_of_predictions != 0:\n",
    "    total_mean_distance_error /= total_number_of_predictions\n",
    "print(\"Total mean Euclidean distance error: \", total_mean_distance_error)\n",
    "if len(ETT_distance_error_list) != 0:\n",
    "    total_mean_ETT_distance_error = sum(ETT_distance_error_list)/len(ETT_distance_error_list)\n",
    "print(\"Total ETT distance error: \", total_mean_ETT_distance_error)\n",
    "print(\"No predictions count: \", no_preds_count)\n",
    "print(\"Total attempts: \", total_attempts)\n",
    "\n",
    "pck_threshold_large = 50\n",
    "pck = calculate_example_pck(total_point_error_list, threshold=pck_threshold_large)\n",
    "print(f\"Fraction of Correct Keypoints: {pck:0.3f}, at a threshold of {pck_threshold_large} pixels.\")\n",
    "pck_list.append(pck)\n",
    "\n",
    "print(\"TP: \", TP)\n",
    "print(\"FP: \", FP)\n",
    "print(\"TN: \", TN)\n",
    "print(\"FN: \", FN)\n",
    "\n",
    "epsilon = 1e-8 #to avoid division by 0 errors \n",
    "\n",
    "precision = TP / (TP + FP + epsilon)\n",
    "precision_list.append(precision)\n",
    "print(\"Precision/PPV: \", precision)\n",
    "recall = TP / (TP + FN + epsilon)\n",
    "recall_list.append(recall)\n",
    "print (\"Recall/Sensitivity: \", recall)\n",
    "NPV = TN / (TN + FN + epsilon)\n",
    "NPV_list.append(NPV)\n",
    "print(\"NPV: \", NPV)\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN + epsilon)\n",
    "accuracy_list.append(accuracy)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "specificity = TN / (TN + FP + epsilon)\n",
    "specificity_list.append(specificity)\n",
    "print(\"Specificity: \", specificity)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall + epsilon)\n",
    "f1_score_list.append(f1_score)\n",
    "print(\"F1 Score: \", f1_score)\n",
    "\n",
    "\n",
    "print(\"Completed at: \", datetime.datetime.now().strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "eucl_err = [29.192664806593392,30.417407798028098,54.13088229772116, 30.169575240273996]\n",
    "ETT_err = [16.936977627484584, 17.40249039641634, 17.95239513280611, 18.22531862299226]\n",
    "precision_list = [0.963414634028852,  0.9390243901293873, 0.926829268179655, 0.9878048779283165]\n",
    "recall_list = [0.963414634028852, 0.9624999998796876,  0.9743589742340566, 0.9204545453499484]\n",
    "NPV_list = [0.8888888885596707, 0.8888888885596707, 0.9259259255829904, 0.7407407404663923]\n",
    "accuracy_list = [0.944954128353674, 0.9266055045021463, 0.9266055045021463, 0.9266055045021463]\n",
    "specificity_list = [ 0.8888888885596707, 0.8275862066111771, 0.8064516126430801, 0.9523809519274377]\n",
    "f1_score_list = [ 0.963414629028852,0.9506172788340193, 0.949999994884375, 0.952941171364706 ]\n",
    "pck_list = [0.991,0.991,0.987, 0.989]\n",
    "\n",
    "stat_list = [precision_list, recall_list, NPV_list, accuracy_list, specificity_list, f1_score_list, pck_list, eucl_err , ETT_err]\n",
    "\n",
    "for list_ in stat_list:\n",
    "    std_dev = statistics.stdev(list_)\n",
    "    avg = sum(list_)/len(list_)\n",
    "    print(f\"The standard deviation of list is: {std_dev:.3f}\")\n",
    "    print(f\"The average of list is: {avg:.3f}\")\n",
    "\n",
    "\n",
    "\n",
    "combined_dataset = ConcatDataset([dataset_train, dataset_test])\n",
    "\n",
    "# K-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for train_idx, val_idx in kf.split(combined_dataset):\n",
    "\n",
    "    val_data = [combined_dataset[i] for i in tqdm(val_idx)]\n",
    "\n",
    "    global data_loader_val\n",
    "    data_loader_val = DataLoader(val_data, batch_size=1, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "TP, FP, FN, TN = 0, 0, 0, 0\n",
    "\n",
    "confusion_matrix = [\n",
    "    [TP, FP],\n",
    "    [FN, TN]\n",
    "]\n",
    "\n",
    "# Create a heatmap using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.set(font_scale=1.4)  # Increase font size\n",
    "ax = sns.heatmap(confusion_matrix, annot=True, cmap=\"YlGnBu\", fmt=\"d\", annot_kws={\"size\": 16})\n",
    "ax.set_ylim(sorted(ax.get_xlim(), reverse=True))\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Predicted labels')\n",
    "ax.set_ylabel('True labels')\n",
    "ax.set_title('Confusion Matrix for ETT Placement')\n",
    "\n",
    "# Set tick labels\n",
    "ax.xaxis.set_ticklabels(['Correct ETT Placement', 'Incorrect ETT Placement'])\n",
    "ax.yaxis.set_ticklabels(['Correct ETT Placement', 'Incorrect ETT Placement'])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_thumbnail(path):\n",
    "    i = Image.open(path)\n",
    "    i.thumbnail((500, 250), Image.Resampling.LANCZOS)\n",
    "    return i\n",
    "\n",
    "def image_base64(im):\n",
    "    if isinstance(im, str):\n",
    "        im = get_thumbnail(im)\n",
    "    with BytesIO() as buffer:\n",
    "        im.save(buffer, 'jpeg')\n",
    "        return base64.b64encode(buffer.getvalue()).decode()\n",
    "\n",
    "def image_formatter(im):\n",
    "    return f'<img src=\"data:image/jpeg;base64,{image_base64(im)}\">'\n",
    "\n",
    "df = pd.DataFrame(example_error_list, columns=['Example Error'])\n",
    "df['pck_small'] = pck_small\n",
    "df['pck_large'] = pck_large\n",
    "df['file'] = pred_image_file_list\n",
    "df['image'] = df.file.map(lambda f: get_thumbnail(f))\n",
    "df.head()\n",
    "\n",
    "HTML(df[['Example Error', 'image']].to_html(formatters={'image': image_formatter}, escape=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workbook = Workbook()\n",
    "worksheet = workbook.active\n",
    "\n",
    "# resize cells\n",
    "for row in range(2, len(df['file'])+2):\n",
    "    for col in range(1,2):\n",
    "        worksheet.row_dimensions[row].height = 400\n",
    "        col_letter = get_column_letter(col)\n",
    "        worksheet.column_dimensions[col_letter].width = 150\n",
    "        \n",
    "for col in range(2,50):\n",
    "    col_letter = get_column_letter(col)\n",
    "    worksheet.column_dimensions[col_letter].width = 30\n",
    "         \n",
    "# insert images\n",
    "for index, image in enumerate(df['file']):\n",
    "    if index == 0:\n",
    "        worksheet.cell(row=index+1, column=1, value=\"Image\")\n",
    "        worksheet.add_image(ExcelImage(image), anchor='A'+str(index+2))\n",
    "    else:\n",
    "        worksheet.add_image(ExcelImage(image), anchor='A'+str(index+2))\n",
    "\n",
    "excel_cell_values = []\n",
    "for excel_cell_value in df['Example Error']:\n",
    "    excel_cell_values.append(excel_cell_value)\n",
    "\n",
    "for index, excel_cell_value in enumerate(excel_cell_values):\n",
    "    if index == 0:\n",
    "        worksheet.cell(row=index+1, column=2, value=\"Example Error\")\n",
    "        worksheet.cell(row=index+2, column=2, value=excel_cell_value)\n",
    "    else:\n",
    "        worksheet.cell(row=index+2, column=2, value=excel_cell_value)\n",
    "\n",
    "# Find the last column\n",
    "last_column = worksheet.max_column\n",
    "\n",
    "# Add values to the last column\n",
    "worksheet.cell(row=1, column=last_column+1).value = f'Total Mean Euclidean Distance Error: {total_mean_distance_error}'\n",
    "worksheet.cell(row=2, column=last_column+1).value = f'No predictions count: {no_preds_count}'\n",
    "worksheet.cell(row=6, column=last_column+1).value = f'Fraction of Correct Keypoints at threshold of {pck_threshold_small} pixels: {pck_small}'\n",
    "worksheet.cell(row=7, column=last_column+1).value = f'Fraction of Correct Keypoints at threshold of {pck_threshold_large} pixels: {pck_large}'\n",
    "\n",
    "# save workbook\n",
    "workbook.save(workbook_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_predict(image_file):\n",
    "    raw_img = cv2.imread(image_file)\n",
    "    aspect_ratio = raw_img.shape[0]/raw_img.shape[1]\n",
    "\n",
    "    if aspect_ratio < 1.5 and aspect_ratio > 0.5:\n",
    "        raw_img = cv2.resize(raw_img, (456, 456))\n",
    "        test_img = np.moveaxis(raw_img, -1, 0)\n",
    "        test_img= np.expand_dims(test_img, axis=0)\n",
    "        test_img = torch.from_numpy(test_img).float().to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.to(device)            \n",
    "            model.eval()\n",
    "            output = model(test_img)\n",
    "        \n",
    "        scores = output[0]['keypoints_scores']\n",
    "        score_idx = get_best_keypoints(scores)\n",
    "\n",
    "        keypoints = output[0]['keypoints'][score_idx].detach().cpu().numpy()\n",
    "        for idx, kp in enumerate(keypoints):\n",
    "            current_keypoint = kp[:2].astype(int)\n",
    "            raw_img = cv2.circle(raw_img, current_keypoint, 1, (255,255,0), 10)\n",
    "            image_original = cv2.putText(raw_img, \" \" + keypoints_classes_ids2names[idx], current_keypoint, cv2.FONT_HERSHEY_PLAIN, 2, (255,0,0), 2, cv2.LINE_AA)\n",
    "\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.imshow(raw_img)\n",
    "    else:\n",
    "        print(f\"Image aspect ratio is {aspect_ratio:0.2F} it but be between 0.5 - 1.5\")\n",
    "\n",
    "        \n",
    "#test_img = \"/mnt/c/Users/nprim/Downloads/F1.jpg\"\n",
    "#process_and_predict(test_img) # random picture from the internet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
