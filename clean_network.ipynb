{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json, cv2, numpy as np, matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "import torchvision\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision.models.detection import KeypointRCNN\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import transforms, utils, engine, train\n",
    "from utils import collate_fn\n",
    "from engine import train_one_epoch, evaluate\n",
    "import random\n",
    "import base64\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from IPython.display import HTML\n",
    "from torchsummary import summary\n",
    "import math\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.drawing.image import Image as ExcelImage\n",
    "from openpyxl.utils import get_column_letter\n",
    "import glob\n",
    "from natsort import natsorted\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import statistics\n",
    "from hyperopt import fmin, tpe\n",
    "from hyperopt import STATUS_OK\n",
    "\n",
    "\n",
    "print(\"Using torch\", torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 27\n",
    "\n",
    "train_model = True\n",
    "load_weights = False\n",
    "make_labels = False\n",
    "\n",
    "num_keypoints = 2\n",
    "save_path = rf'F:\\ET_Tube\\CheXpert-v1.0\\weights\\test_weights_{datetime.datetime.now().strftime(\"%H_%M_%S\")}.pth'\n",
    "save_dir =  r\"D:\\ET_Tube\\CheXpert-v1.0\\checkpoints\" #checkpoint directory\n",
    "workbook_path = rf'F:\\ET_Tube\\CheXpert-v1.0\\predictions_{datetime.datetime.now().strftime(\"%H_%M_%S\")}.xlsx'\n",
    "\n",
    "batch_size = 1\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYPOINTS_FOLDER_TRAIN = fr\"C:\\Users\\nprim\\Downloads\\ET_TUBE.v{num}i.yolov7pytorch\\train\"\n",
    "KEYPOINTS_FOLDER_VALID = fr\"C:\\Users\\nprim\\Downloads\\ET_TUBE.v{num}i.yolov7pytorch\\valid\"\n",
    "KEYPOINTS_FOLDER_TEST =  fr\"C:\\Users\\nprim\\Downloads\\ET_TUBE.v{num}i.yolov7pytorch\\test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if make_labels:\n",
    "    import json\n",
    "    import zipfile\n",
    "    path_to_zip_file = fr\"C:\\Users\\nprim\\Downloads\\ET_TUBE.v{num}i.yolov7pytorch.zip\"\n",
    "    \n",
    "    BASE = fr\"C:\\Users\\nprim\\Downloads\\ET_TUBE.v{num}i.yolov7pytorch\"\n",
    "    with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(BASE)\n",
    "\n",
    "    SET = [\"/train\", \"/valid\", \"/test\"]\n",
    "    #SET = [\"/train\",  \"/test\"]\n",
    "    #SET = [\"/train\", \"/valid\"]\n",
    "    #SET = [\"/train\"]\n",
    "    #SET = [\"/test\"]\n",
    "  \n",
    "    keypoint_names =  ['Carina', 'ET']\n",
    "\n",
    "    def dump2json(bboxes, keypoints_sorted, file_json):\n",
    "        annotations = {}\n",
    "        annotations['bboxes'], annotations['keypoints'] = bboxes, keypoints_sorted\n",
    "\n",
    "        with open(file_json, \"w\") as f:\n",
    "            json.dump(annotations, f)\n",
    "\n",
    "    def converter(file_labels, file_image, keypoint_names):\n",
    "\n",
    "        img = cv2.imread(file_image)\n",
    "        img_w, img_h = img.shape[1], img.shape[0]\n",
    "        \n",
    "        with open(file_labels) as f:\n",
    "            lines_txt = f.readlines()\n",
    "            lines = []\n",
    "            for line in lines_txt:\n",
    "                lines.append([int(line.split()[0])] + [round(float(el), 5) for el in line.split()[1:]])\n",
    "        \n",
    "        bboxes = []\n",
    "        keypoints = []\n",
    "\n",
    "        # Convert normalized coordinates to absolute coordinates\n",
    "        for line in lines:\n",
    "            # Number 0 is a class of rectangles related to bounding boxes.\n",
    "            if line[0] == 2:\n",
    "                x_c, y_c, w, h = round(line[1] * img_w), round(line[2] * img_h), round(line[3] * img_w), round(line[4] * img_h)\n",
    "                bboxes.append([round(x_c - w/2), round(y_c - h/2), round(x_c + w/2), round(y_c + h/2)])\n",
    "\n",
    "            elif line[0] == 0 or line[0] == 1: #append all other keypoints without class change\n",
    "                kp_id, x_c, y_c = line[0], round(line[1] * img_w), round(line[2] * img_h) \n",
    "                keypoints.append([kp_id, x_c, y_c])\n",
    "                \n",
    "\n",
    "        # iterating over each keypoint and looking to which bounding box it matches, dont need this for single boxes\n",
    "        keypoints_sorted = [[[] for _ in keypoint_names] for _ in bboxes]\n",
    "\n",
    "        for kp in keypoints:\n",
    "            kp_id, kp_x, kp_y = kp[0], kp[1], kp[2]\n",
    "            for bbox_idx, bbox in enumerate(bboxes):\n",
    "                x1, y1, x2, y2 = bbox[0], bbox[1], bbox[2], bbox[3]\n",
    "                if x1 < kp_x < x2 and y1 < kp_y < y2:\n",
    "                    keypoints_sorted[bbox_idx][kp_id] = [kp_x, kp_y, 1] # All keypoints are visible\n",
    "                    \n",
    "        return bboxes, keypoints_sorted\n",
    "\n",
    "\n",
    "    for i in range(len(SET)):\n",
    "        IMAGES = BASE + SET[i] + \"/images\"\n",
    "        LABELS = BASE + SET[i] + \"/labels\"\n",
    "        ANNOTATIONS = BASE +  SET[i] + \"/annotations\"\n",
    "        \n",
    "        files_names = [file.split('.jpg')[0] for file in os.listdir(IMAGES)]\n",
    "\n",
    "        for file in tqdm((files_names), desc =f\"Set: {SET[i]}\"):\n",
    "        #for file in (files_names):\n",
    "            file_labels = os.path.join(LABELS, file + \".txt\")\n",
    "            file_image = os.path.join(IMAGES, file + \".jpg\")\n",
    "\n",
    "            #img = cv2.imread(file_image)\n",
    "            #if img.shape[0] != img.shape[1]:\n",
    "                #print(\"Non square image:\", file_image)\n",
    "                \n",
    "            bboxes, keypoints_sorted = converter(file_labels, file_image, keypoint_names)\n",
    "\n",
    "            for i in keypoints_sorted:\n",
    "                a,b = i\n",
    "                if (len(a) != 3 or len (b) != 3) :\n",
    "                    print(\"Error in file\", file)\n",
    "\n",
    "            if not os.path.exists(ANNOTATIONS):\n",
    "                os.makedirs(ANNOTATIONS)\n",
    "\n",
    "            dump2json(bboxes, keypoints_sorted, os.path.join(ANNOTATIONS, file + '.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 256\n",
    "\n",
    "def train_transform():\n",
    "    return A.Compose([\n",
    "        A.GaussNoise(var_limit=(10, 50), mean=0, per_channel=True, p=0.5), \n",
    "        A.ISONoise(intensity=(0.1, 0.5), p=0.5), \n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5), \n",
    "        A.Rotate(limit=15, border_mode=cv2.BORDER_CONSTANT, value=0, p=0.5), \n",
    "        A.CLAHE(clip_limit=(1,4), p=1), # Enhance local contrast\n",
    "        A.Resize(IMG_SIZE, IMG_SIZE, p=1)\n",
    "    ],\n",
    "    keypoint_params=A.KeypointParams(format='xy'),\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bboxes_labels'])\n",
    "    )\n",
    "\n",
    "def test_valid_transform():\n",
    "    return A.Compose([\n",
    "        A.Sequential([\n",
    "            A.CLAHE(clip_limit=(1,4), p=1),\n",
    "            A.Resize(IMG_SIZE, IMG_SIZE, p=1)\n",
    "        ], p=1)\n",
    "    ],\n",
    "    keypoint_params=A.KeypointParams(format='xy'), \n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bboxes_labels'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, demo=False):                \n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.demo = demo # Use demo=True if you need transformed and original images (for example, for visualization purposes)\n",
    "        self.imgs_files = sorted(os.listdir(os.path.join(root, \"images\")))\n",
    "        self.annotations_files = sorted(os.listdir(os.path.join(root, \"annotations\")))\n",
    "\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root, \"images\", self.imgs_files[idx])\n",
    "        annotations_path = os.path.join(self.root, \"annotations\", self.annotations_files[idx])\n",
    "\n",
    "        img_original = cv2.imread(img_path)\n",
    "        img_original = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)        \n",
    "        \n",
    "        with open(annotations_path) as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "            bboxes_original = data['bboxes']\n",
    "            keypoints_original = data['keypoints']\n",
    "                          \n",
    "            # All objects are patellas, so we can use the same label for all objects\n",
    "            bboxes_labels_original = ['ROI' for _ in bboxes_original]  \n",
    "\n",
    "\n",
    "            if self.transform:   \n",
    "                # Converting keypoints from [x,y,visibility]-format to [x, y]-format + Flattening nested list of keypoints            \n",
    "                # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "                # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]], where each keypoint is in [x, y]-format            \n",
    "                # Then we need to convert it to the following list:\n",
    "                # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2]\n",
    "                \n",
    "                keypoints_original_flattened = [el[0:2] for kp in keypoints_original for el in kp]\n",
    "\n",
    "                # Apply augmentations\n",
    "                transformed = self.transform(image=img_original, bboxes=bboxes_original, bboxes_labels=bboxes_labels_original, keypoints=keypoints_original_flattened)\n",
    "                img = transformed['image']\n",
    "                bboxes = transformed['bboxes']\n",
    "                \n",
    "                # Unflattening list transformed['keypoints']\n",
    "                # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "                # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2], where each keypoint is in [x, y]-format\n",
    "                # Then we need to convert it to the following list:\n",
    "                # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]]\n",
    "                \n",
    "                #keypoints_transformed_unflattened = np.reshape(np.array(transformed['keypoints']), (-1,3,2)).tolist()\n",
    "\n",
    "                keypoints_transformed_unflattened = [transformed['keypoints']]\n",
    "\n",
    "\n",
    "                #keypoints_transformed_unflattened = np.reshape(np.array(transformed['keypoints']), (-1,3,2)).tolist()\n",
    "                \n",
    "                # Converting transformed keypoints from [x, y]-format to [x,y,visibility]-format by appending original visibilities to transformed coordinates of keypoints\n",
    "                keypoints = []\n",
    "                for o_idx, obj in enumerate(keypoints_transformed_unflattened): # Iterating over objects\n",
    "                    obj_keypoints = []\n",
    "\n",
    "                    for k_idx, kp in enumerate(obj): # Iterating over keypoints in each object\n",
    "                        # kp - coordinates of keypoint\n",
    "                        # keypoints_original[o_idx][k_idx][2] - original visibility of keypoint\n",
    "                        obj_keypoints.append(np.array(kp).tolist() + [keypoints_original[o_idx][k_idx][2]])\n",
    "                    keypoints.append(obj_keypoints)\n",
    "        \n",
    "            else:\n",
    "                img, bboxes, keypoints = img_original, bboxes_original, keypoints_original\n",
    "                 \n",
    "            # Convert everything into a torch tensor        \n",
    "            bboxes = torch.as_tensor(bboxes, dtype=torch.float32)       \n",
    "            target = {}\n",
    "            target[\"boxes\"] = bboxes\n",
    "            target[\"labels\"] = torch.as_tensor([1 for _ in bboxes], dtype=torch.int64)\n",
    "            target[\"image_id\"] = torch.tensor([idx])\n",
    "            target[\"area\"] = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])\n",
    "            target[\"iscrowd\"] = torch.zeros(len(bboxes), dtype=torch.int64)\n",
    "\n",
    "            target[\"keypoints\"] = torch.as_tensor(keypoints, dtype=torch.float32) \n",
    "            \n",
    "            img = F.to_tensor(img)\n",
    "            \n",
    "            bboxes_original = torch.as_tensor(bboxes_original, dtype=torch.float32)\n",
    "            target_original = {}\n",
    "            target_original[\"boxes\"] = bboxes_original\n",
    "            target_original[\"labels\"] = torch.as_tensor([1 for _ in bboxes_original], dtype=torch.int64) # all objects are carinas\n",
    "            target_original[\"image_id\"] = torch.tensor([idx])\n",
    "            target_original[\"area\"] = (bboxes_original[:, 3] - bboxes_original[:, 1]) * (bboxes_original[:, 2] - bboxes_original[:, 0])\n",
    "            target_original[\"iscrowd\"] = torch.zeros(len(bboxes_original), dtype=torch.int64)\n",
    "            target_original[\"keypoints\"] = torch.as_tensor(keypoints_original, dtype=torch.float32)        \n",
    "            img_original = F.to_tensor(img_original)\n",
    "\n",
    "            if self.demo:\n",
    "                return img, target, img_original, target_original\n",
    "            else:\n",
    "                return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ClassDataset(KEYPOINTS_FOLDER_TEST, transform=train_transform(), demo=True)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "iterator = iter(data_loader)\n",
    "batch = next(iterator)\n",
    "\n",
    "keypoints_classes_ids2names = {0: 'Carina' , 1: 'ET'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(image, bboxes, keypoints, image_original=None, bboxes_original=None, keypoints_original=None, augmented=True, save=False, save_path=None, display=True):\n",
    "    fontsize = 12\n",
    "\n",
    "    for bbox in bboxes:\n",
    "        start_point = (bbox[0], bbox[1])\n",
    "        end_point = (bbox[2], bbox[3])\n",
    "        image = cv2.rectangle(image.copy(), start_point, end_point, (0,255,0), 2)\n",
    "    \n",
    "    for kps in keypoints:\n",
    "        for idx, kp in enumerate(kps):\n",
    "            image = cv2.circle(image.copy(), tuple(kp), 1, (255,0,0), 10)\n",
    "            image = cv2.putText(image.copy(), \" \" + keypoints_classes_ids2names[idx], tuple(kp), cv2.FONT_HERSHEY_PLAIN, 1, (255,0,0), 1, cv2.LINE_AA)\n",
    "\n",
    "    if image_original is None and keypoints_original is None:\n",
    "        plt.figure(figsize=(20,20))\n",
    "        plt.imshow(image)\n",
    "        \n",
    "\n",
    "    else:\n",
    "        for bbox in bboxes_original:\n",
    "            start_point = (bbox[0], bbox[1])\n",
    "            end_point = (bbox[2], bbox[3])\n",
    "            image_original = cv2.rectangle(image_original.copy(), start_point, end_point, (255,255,0), 2)\n",
    "        \n",
    "        for kps in keypoints_original:\n",
    "            for idx, kp in enumerate(kps):\n",
    "                image_original = cv2.circle(image_original, tuple(kp), 1, (255,255,0), 10)\n",
    "                image_original = cv2.putText(image_original, \" \" + keypoints_classes_ids2names[idx], tuple(kp), cv2.FONT_HERSHEY_PLAIN, 1, (255,0,0), 1, cv2.LINE_AA)\n",
    "        \n",
    "\n",
    "        f, ax = plt.subplots(1, 2, figsize=(40, 20))\n",
    "        ax[0].set_title('Original image', fontsize=fontsize)\n",
    "        if augmented:\n",
    "            ax[1].set_title('Augmented image', fontsize=fontsize)\n",
    "        else:\n",
    "            ax[1].set_title('Predicted image', fontsize=fontsize) # also augmented image\n",
    "    \n",
    "        ax[0].imshow(image_original)\n",
    "        ax[1].imshow(image)\n",
    "        \n",
    "        if(save):\n",
    "            print(\"Saving image to: \", save_path)\n",
    "            plt.gcf().set_size_inches(5, 10)\n",
    "            plt.savefig(save_path, dpi=100)\n",
    "            plt.close()\n",
    "            \n",
    "        if(display == False):\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = (batch[0][0].permute(1,2,0).numpy() * 255).astype(np.uint8)\n",
    "bboxes = batch[1][0]['boxes'].detach().cpu().numpy().astype(np.int32).tolist()\n",
    "\n",
    "keypoints = []\n",
    "for kps in batch[1][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist():\n",
    "    keypoints.append([kp[:2] for kp in kps])\n",
    "\n",
    "image_original = (batch[2][0].permute(1,2,0).numpy() * 255).astype(np.uint8)\n",
    "bboxes_original = batch[3][0]['boxes'].detach().cpu().numpy().astype(np.int32).tolist()\n",
    "\n",
    "keypoints_original = []\n",
    "for kps in batch[3][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist():\n",
    "    keypoints_original.append([kp[:2] for kp in kps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(image, bboxes, keypoints, image_original, bboxes_original, keypoints_original, augmented=True, save=False, save_path=None, display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_keypoints=2, anchor_sizes = (64, 128, 256) , anchor_ratios= (0.5, 0.83, 1.2, 2), weights_path=None):\n",
    "    \n",
    "    backbone = torchvision.models.convnext_large(weights='DEFAULT').features\n",
    "    backbone.out_channels = 1536\n",
    "\n",
    "    anchor_generator = AnchorGenerator(sizes=(anchor_sizes,), aspect_ratios=(anchor_ratios,))\n",
    "    \n",
    "    roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
    "                                                    output_size=7,\n",
    "                                                    sampling_ratio=2)\n",
    "    keypoint_roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
    "                                                             output_size=14,\n",
    "                                                             sampling_ratio=2)\n",
    "\n",
    "    model = KeypointRCNN(backbone,\n",
    "                          num_classes=2,\n",
    "                          rpn_anchor_generator=anchor_generator,\n",
    "                          box_roi_pool=roi_pooler,\n",
    "                          keypoint_roi_pool=keypoint_roi_pooler,\n",
    "                          num_keypoints=num_keypoints)        \n",
    "        \n",
    "    return model\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = ClassDataset(KEYPOINTS_FOLDER_TRAIN, transform=train_transform(), demo=False)\n",
    "dataset_valid = ClassDataset(KEYPOINTS_FOLDER_VALID, transform=test_valid_transform(), demo=False)\n",
    "dataset_test = ClassDataset(KEYPOINTS_FOLDER_TEST, transform=test_valid_transform(), demo=False)\n",
    "\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "data_loader_valid = DataLoader(dataset_valid, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "import random\n",
    "num_train_samples = 100 # for example, you want 100 samples from the training dataset\n",
    "num_valid_samples = 5\n",
    "\n",
    "# Randomly sample indices for the training and validation subsets\n",
    "train_indices = random.sample(range(len(dataset_train)), num_train_samples)\n",
    "valid_indices = random.sample(range(len(dataset_valid)), num_valid_samples)\n",
    "\n",
    "# Create random subsets using the sampled indices\n",
    "subset_dataset_train = Subset(dataset_train, indices=train_indices)\n",
    "subset_dataset_valid = Subset(dataset_valid, indices=valid_indices)\n",
    "\n",
    "data_loader_train_subset = DataLoader(subset_dataset_train, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "data_loader_valid_subset = DataLoader(subset_dataset_valid, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_distance(kp1, kp2):\n",
    "    x1, y1 = kp1\n",
    "    x2, y2 = kp2\n",
    "    return math.sqrt((x2 - x1)**2 + (y2 - y1)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_epoch_error(model, data_loader_test, device):\n",
    "    with torch.inference_mode():\n",
    "        model.eval()\n",
    "        running_epoch_dist_error = 0\n",
    "        num_keypoint_predictions = 0\n",
    "        avg_dist_error = 0\n",
    "        for batch_idx, (images, targets) in enumerate(data_loader_test):\n",
    "            \n",
    "            if not targets:\n",
    "                #print(\"No targets available for the current batch.\")\n",
    "                continue\n",
    "\n",
    "            images = list(image.to(device) for image in images)\n",
    "            predictions = model(images)\n",
    "            for i in range(len(predictions)):\n",
    "                for idx in range(num_keypoints):\n",
    "                    try:\n",
    "                        pred = predictions[i]['keypoints'][0][idx].detach().cpu().numpy().astype(np.int32)\n",
    "                        x1, y1, _ = predictions[i]['keypoints'][0][idx].detach().cpu().numpy().astype(np.int32)\n",
    "                        kp1 = (x1, y1)\n",
    "                        x2, y2, _ = targets[i]['keypoints'][0][idx].detach().cpu().numpy().astype(np.int32)\n",
    "                        kp2 = (x2, y2)\n",
    "                        num_keypoint_predictions += 1\n",
    "                        running_epoch_dist_error += calc_distance(kp1, kp2)\n",
    "                        avg_dist_error = running_epoch_dist_error / num_keypoint_predictions\n",
    "                    except:\n",
    "                        print(\"No prediction for val image.\")\n",
    "        print(\"Running Epoch Error: \", avg_dist_error)\n",
    "        score = -avg_dist_error\n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, fold, save_dir):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'epoch': fold,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, os.path.join(save_dir, f'checkpoint__{fold}_{epoch}.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_distance(kp1, kp2):\n",
    "    x1, y1 = kp1\n",
    "    x2, y2 = kp2\n",
    "    return math.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "\n",
    "def calc_mean_distance_error(real_keypoints, pred_keypoints):\n",
    "    mean_distance = 0\n",
    "    point_error = [0,0,0]\n",
    "    assert len(real_keypoints) == len(pred_keypoints)\n",
    "    for i in range(len(real_keypoints)):\n",
    "        for j in range(len(real_keypoints[i])):\n",
    "            point_error[j] = calc_distance(real_keypoints[i][j], pred_keypoints[i][j])\n",
    "            mean_distance += point_error[j]\n",
    "    return mean_distance/len(keypoints), point_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_keypoints(scores):\n",
    "    avg_scores = []\n",
    "    for score in scores:\n",
    "        avg_scores.append(torch.mean(score))\n",
    "\n",
    "    try:\n",
    "        if len(avg_scores) > 0:\n",
    "            return avg_scores.index(max(avg_scores))\n",
    "        else:\n",
    "            raise ValueError(\"avg_scores is empty. Make sure you have calculated the average scores before finding the maximum.\")\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        return []  # or some other default value\n",
    "   \n",
    "    #return avg_scores.index(max(avg_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Proportion of Correct Keypoints (PCK)\n",
    "# Threshold is the maximum distance between the predicted and ground truth keypoints\n",
    "# Here it is written (default param) as 10 pixels\n",
    "# Perhaps this should be a function of the image size?\n",
    "def calculate_example_pck(total_point_error_list, threshold=10):\n",
    "    num_correct = 0\n",
    "    flat_list = list(itertools.chain(*total_point_error_list))\n",
    "\n",
    "    for point in flat_list:\n",
    "        if point <= threshold:\n",
    "            num_correct += 1\n",
    "            \n",
    "# Calculate the PCK\n",
    "    pck = num_correct / len(flat_list)\n",
    "    return pck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_one_epoch(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    criterion = torch.nn.MSELoss() # Loss function\n",
    "    example_distance_error_list = []\n",
    "    pred_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_count, batch  in enumerate(data_loader):\n",
    "            images, kp = batch\n",
    "            assert len(images) == len(kp)\n",
    "            for i in range(len(images)):\n",
    "                image = images[i].to(device)\n",
    "\n",
    "                # Get ground truth keypoints, remove visibility column\n",
    "                kps = torch.squeeze(kp[i]['keypoints'][:, :, :2])\n",
    "\n",
    "                keypoints_gt = kps.to(device)\n",
    "                \n",
    "                # Get predictions\n",
    "                output = model(image.unsqueeze(0))\n",
    "                \n",
    "                # Check if predictions are available\n",
    "                if len(output) > 0 and 'keypoints' in output[0] and len(output[0]['keypoints']) > 0:\n",
    "                    #get highest score keypoint preds, again without visibility column\n",
    "                    keypoints_pred = output[0]['keypoints'][0][:, :-1]\n",
    "\n",
    "                    # Compute loss\n",
    "                    loss = criterion(keypoints_pred, keypoints_gt)\n",
    "                    total_loss += loss.item()\n",
    "                    pred_count += 1\n",
    "\n",
    "                    diff = keypoints_pred - keypoints_gt\n",
    "                    squared_diff = diff ** 2\n",
    "                    example_distance_error = torch.sqrt(torch.sum(squared_diff))\n",
    "                    example_distance_error_list.append(example_distance_error)\n",
    "                else:\n",
    "                    pass\n",
    "                    #print(f\"No keypoints detected for image: {i}, batch: {batch_count}\")\n",
    "\n",
    "    # Compute average loss and accuracy\n",
    "    if pred_count > 0:\n",
    "        avg_loss = total_loss / pred_count\n",
    "        avg_distance_error = torch.mean(torch.stack(example_distance_error_list))\n",
    "\n",
    "        print(f\"Average Validation Loss: {avg_loss}, Average Validation Distance Error: {avg_distance_error}\")\n",
    "        return avg_loss\n",
    "    else:\n",
    "        print(\"No keypoints detected in the entire batch.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "def get_full_dataset():\n",
    "    # Define individual datasets\n",
    "    dataset_train = ClassDataset(KEYPOINTS_FOLDER_TRAIN, transform=train_transform(), demo=False)\n",
    "    dataset_valid = ClassDataset(KEYPOINTS_FOLDER_VALID, transform=test_valid_transform(), demo=False)\n",
    "\n",
    "    # Combine the datasets\n",
    "    full_dataset = ConcatDataset([dataset_train, dataset_valid])\n",
    "\n",
    "    return full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian optimization for hyperparam tuning. We will prob need to extend this for best results. Just making sure it works for now.\n",
    "from hyperopt import hp\n",
    "\n",
    "space = {\n",
    "    'anchor_sizes_idx': hp.choice('anchor_sizes', [0, 1]),\n",
    "    'anchor_ratios_idx': hp.choice('anchor_ratios', [0, 1]),\n",
    "}\n",
    "\n",
    "anchor_sizes_options = [(32, 64, 128), (64, 128, 256)]\n",
    "anchor_ratios_options = [(0.5, 1, 2), (0.5, 0.83, 1.2, 2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [  0/100]  eta: 0:02:03  lr: 0.000006  loss: 9.4595 (9.4595)  loss_classifier: 0.7201 (0.7201)  loss_box_reg: 0.0195 (0.0195)  loss_keypoint: 7.9540 (7.9540)  loss_objectness: 0.7644 (0.7644)  loss_rpn_box_reg: 0.0015 (0.0015)  time: 1.2318  data: 0.8652  max mem: 19587\n",
      "Epoch: [0]  [ 99/100]  eta: 0:00:00  lr: 0.000500  loss: 7.1433 (8.2136)  loss_classifier: 0.0325 (0.2236)  loss_box_reg: 0.0085 (0.0150)  loss_keypoint: 6.8273 (7.4700)  loss_objectness: 0.2471 (0.5000)  loss_rpn_box_reg: 0.0025 (0.0048)  time: 0.8123  data: 0.5865  max mem: 19587\n",
      "Epoch: [0] Total time: 0:01:19 (0.7901 s / it)       \n",
      "Average Validation Loss: 7868.93701171875, Average Validation Distance Error: 176.1496124267578\n",
      "Loss improved last epoch.                            \n",
      "Epoch: [1]  [  0/100]  eta: 0:01:31  lr: 0.000500  loss: 7.6834 (7.6834)  loss_classifier: 0.0703 (0.0703)  loss_box_reg: 0.0341 (0.0341)  loss_keypoint: 7.3120 (7.3120)  loss_objectness: 0.2656 (0.2656)  loss_rpn_box_reg: 0.0014 (0.0014)  time: 0.9111  data: 0.5411  max mem: 19587\n",
      "Epoch: [1]  [ 99/100]  eta: 0:00:00  lr: 0.000500  loss: 6.6372 (7.0015)  loss_classifier: 0.0404 (0.0480)  loss_box_reg: 0.0159 (0.0178)  loss_keypoint: 6.5301 (6.8210)  loss_objectness: 0.0650 (0.1110)  loss_rpn_box_reg: 0.0020 (0.0035)  time: 0.7918  data: 0.5656  max mem: 19587\n",
      "Epoch: [1] Total time: 0:01:22 (0.8286 s / it)       \n",
      "Average Validation Loss: 13779.71123046875, Average Validation Distance Error: 229.6251983642578\n",
      "Epochs without improvement:                          \n",
      "1                                                    \n",
      "Epoch: [2]  [  0/100]  eta: 0:02:24  lr: 0.000500  loss: 6.9082 (6.9082)  loss_classifier: 0.0685 (0.0685)  loss_box_reg: 0.0275 (0.0275)  loss_keypoint: 6.7067 (6.7067)  loss_objectness: 0.1040 (0.1040)  loss_rpn_box_reg: 0.0015 (0.0015)  time: 1.4480  data: 1.0387  max mem: 19587\n",
      "  0%|          | 0/2 [02:54<?, ?trial/s, best loss=?]"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def objective(params):\n",
    "    \n",
    "    anchor_sizes = anchor_sizes_options[params['anchor_sizes_idx']]\n",
    "    anchor_ratios = anchor_ratios_options[params['anchor_ratios_idx']]\n",
    "    validation_losses = []\n",
    "\n",
    "    model = get_model(anchor_sizes=anchor_sizes, anchor_ratios=anchor_ratios)\n",
    "    model.to(device)\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.0005, momentum=0.8, weight_decay=0.0005)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.3)\n",
    "\n",
    "    # Parameters for early stopping\n",
    "    patience = 5  # Number of epochs with no improvement after which training will be stopped\n",
    "    best_validation_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(10):\n",
    "        train_one_epoch(model, optimizer, data_loader_train_subset, device, epoch, print_freq=100)\n",
    "        lr_scheduler.step()\n",
    "        validation_loss = validate_one_epoch(model, data_loader_valid_subset, device)\n",
    "        validation_losses.append(validation_loss)\n",
    "\n",
    "        # Check for improvement\n",
    "        if validation_loss < best_validation_loss:\n",
    "            best_validation_loss = validation_loss\n",
    "            epochs_without_improvement = 0\n",
    "            best_model = deepcopy(model)\n",
    "            print(\"Loss improved last epoch.\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(\"Epochs without improvement: \", epochs_without_improvement)\n",
    "\n",
    "        save_checkpoint(model, optimizer, epoch, 1, save_dir)\n",
    "\n",
    "        # Check for early stopping\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f'Early stopping after {patience} epochs without improvement')\n",
    "            model = best_model  # Restore the best model\n",
    "            break\n",
    "\n",
    "    return {'loss': best_validation_loss, 'status': STATUS_OK}\n",
    "\n",
    "best_params = fmin(objective, space, algo=tpe.suggest, max_evals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [  0/467]  eta: 0:09:27  lr: 0.000002  loss: 9.6281 (9.6281)  loss_classifier: 0.6894 (0.6894)  loss_box_reg: 0.0087 (0.0087)  loss_keypoint: 8.2967 (8.2967)  loss_objectness: 0.6314 (0.6314)  loss_rpn_box_reg: 0.0019 (0.0019)  time: 1.2157  data: 0.8715  max mem: 19587\n",
      "Epoch: [0]  [100/467]  eta: 0:05:00  lr: 0.000109  loss: 7.8270 (8.7884)  loss_classifier: 0.0907 (0.3859)  loss_box_reg: 0.0199 (0.0244)  loss_keypoint: 7.3349 (7.8443)  loss_objectness: 0.3791 (0.5292)  loss_rpn_box_reg: 0.0043 (0.0045)  time: 0.8251  data: 0.6004  max mem: 19587\n",
      "Epoch: [0]  [200/467]  eta: 0:03:42  lr: 0.000216  loss: 7.1682 (8.0403)  loss_classifier: 0.0359 (0.2224)  loss_box_reg: 0.0102 (0.0213)  loss_keypoint: 6.9214 (7.4165)  loss_objectness: 0.1245 (0.3748)  loss_rpn_box_reg: 0.0042 (0.0053)  time: 0.9168  data: 0.6928  max mem: 19587\n",
      "Epoch: [0]  [300/467]  eta: 0:02:16  lr: 0.000323  loss: 6.5834 (7.6949)  loss_classifier: 0.0476 (0.1646)  loss_box_reg: 0.0246 (0.0214)  loss_keypoint: 6.4533 (7.2277)  loss_objectness: 0.0690 (0.2764)  loss_rpn_box_reg: 0.0032 (0.0048)  time: 0.7884  data: 0.5640  max mem: 19587\n",
      "Epoch: [0]  [400/467]  eta: 0:00:55  lr: 0.000430  loss: 6.7587 (7.4867)  loss_classifier: 0.0635 (0.1373)  loss_box_reg: 0.0369 (0.0235)  loss_keypoint: 6.5757 (7.1038)  loss_objectness: 0.0339 (0.2176)  loss_rpn_box_reg: 0.0027 (0.0045)  time: 0.8481  data: 0.6290  max mem: 19587\n",
      "Epoch: [0]  [466/467]  eta: 0:00:00  lr: 0.000500  loss: 6.7301 (7.4035)  loss_classifier: 0.0519 (0.1255)  loss_box_reg: 0.0348 (0.0251)  loss_keypoint: 6.5822 (7.0560)  loss_objectness: 0.0327 (0.1925)  loss_rpn_box_reg: 0.0024 (0.0044)  time: 0.7890  data: 0.5692  max mem: 19587\n",
      "Epoch: [0] Total time: 0:06:31 (0.8375 s / it)       \n",
      "Average Validation Loss: 11507.101644058228, Average Validation Distance Error: 199.79977416992188\n",
      "Epoch: [0]  [  0/467]  eta: 0:09:37  lr: 0.000002  loss: 9.7982 (9.7982)  loss_classifier: 0.7583 (0.7583)  loss_box_reg: 0.0072 (0.0072)  loss_keypoint: 8.2642 (8.2642)  loss_objectness: 0.7564 (0.7564)  loss_rpn_box_reg: 0.0122 (0.0122)  time: 1.2358  data: 0.8597  max mem: 19587\n",
      "Epoch: [0]  [100/467]  eta: 0:04:58  lr: 0.000109  loss: 7.8878 (8.8678)  loss_classifier: 0.1073 (0.4243)  loss_box_reg: 0.0107 (0.0126)  loss_keypoint: 7.2461 (7.8082)  loss_objectness: 0.4716 (0.6169)  loss_rpn_box_reg: 0.0033 (0.0057)  time: 0.7964  data: 0.5618  max mem: 19587\n",
      "Epoch: [0]  [200/467]  eta: 0:03:40  lr: 0.000216  loss: 6.8355 (8.0711)  loss_classifier: 0.0420 (0.2427)  loss_box_reg: 0.0187 (0.0178)  loss_keypoint: 6.5977 (7.3690)  loss_objectness: 0.1428 (0.4355)  loss_rpn_box_reg: 0.0034 (0.0061)  time: 0.8245  data: 0.5961  max mem: 19587\n",
      "Epoch: [0]  [300/467]  eta: 0:02:17  lr: 0.000323  loss: 6.6999 (7.7124)  loss_classifier: 0.0456 (0.1784)  loss_box_reg: 0.0251 (0.0191)  loss_keypoint: 6.5947 (7.1863)  loss_objectness: 0.0633 (0.3227)  loss_rpn_box_reg: 0.0050 (0.0059)  time: 0.7545  data: 0.5283  max mem: 19587\n",
      "Epoch: [0]  [400/467]  eta: 0:00:54  lr: 0.000430  loss: 6.8239 (7.5170)  loss_classifier: 0.0644 (0.1478)  loss_box_reg: 0.0398 (0.0215)  loss_keypoint: 6.6963 (7.0859)  loss_objectness: 0.0446 (0.2565)  loss_rpn_box_reg: 0.0021 (0.0053)  time: 0.8798  data: 0.6515  max mem: 19587\n",
      "Epoch: [0]  [466/467]  eta: 0:00:00  lr: 0.000500  loss: 6.4835 (7.4086)  loss_classifier: 0.0626 (0.1349)  loss_box_reg: 0.0455 (0.0237)  loss_keypoint: 6.2795 (7.0190)  loss_objectness: 0.0370 (0.2260)  loss_rpn_box_reg: 0.0019 (0.0051)  time: 0.8328  data: 0.6071  max mem: 19587\n",
      "Epoch: [0] Total time: 0:06:26 (0.8279 s / it)                                  \n",
      "Average Validation Loss: 12678.481219951924, Average Validation Distance Error: 221.0535888671875\n",
      "100%|██████████| 2/2 [13:23<00:00, 401.54s/trial, best loss: 11507.101644058228]\n"
     ]
    }
   ],
   "source": [
    "def objective(params):\n",
    "    \n",
    "    anchor_sizes = anchor_sizes_options[params['anchor_sizes_idx']]\n",
    "    anchor_ratios = anchor_ratios_options[params['anchor_ratios_idx']]\n",
    "    validation_losses = []\n",
    "\n",
    "    # Initialize the model with hyperparameters\n",
    "    model = get_model(anchor_sizes=anchor_sizes,\n",
    "                    anchor_ratios=anchor_ratios)\n",
    "    model.to(device)\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.0005, momentum=0.8, weight_decay=0.0005)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.3)\n",
    "\n",
    "    for epoch in range(1):\n",
    "        train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=100)\n",
    "        lr_scheduler.step()\n",
    "        validation_loss = validate_one_epoch(model, data_loader_valid, device)\n",
    "        validation_losses.append(validation_loss)\n",
    "        save_checkpoint(model, optimizer, epoch, 1, save_dir)\n",
    "\n",
    "    return {'loss': np.mean(validation_losses), 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "best_params = fmin(objective, space, algo=tpe.suggest, max_evals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'anchor_ratios': 0, 'anchor_sizes': 1}\n"
     ]
    }
   ],
   "source": [
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# K fold Cross Validation training. Will use this once we have chosen best hyperparams. Should also add early stopping.\n",
    " \n",
    "# Number of folds\n",
    "K = 5\n",
    "\n",
    "# Get the full dataset\n",
    "full_dataset = get_full_dataset()\n",
    "\n",
    "# Split the dataset into K folds\n",
    "kfold = KFold(n_splits=K, shuffle=True, random_state=42)\n",
    "\n",
    "# Iterate through each fold\n",
    "for fold, (train_ids, val_ids) in enumerate(kfold.split(full_dataset)):\n",
    "    print(f\"Fold {fold + 1}/{K}\")\n",
    "\n",
    "    # Create data loaders for this fold\n",
    "    train_subset = torch.utils.data.Subset(full_dataset, train_ids)\n",
    "    val_subset = torch.utils.data.Subset(full_dataset, val_ids)\n",
    "\n",
    "    data_loader_train = torch.utils.data.DataLoader(train_subset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = torch.utils.data.DataLoader(val_subset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # Initialize the model for this fold\n",
    "    model = get_model()\n",
    "    model.to(device)\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.0005, momentum=0.8, weight_decay=0.0005)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.3)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=100)\n",
    "        lr_scheduler.step()\n",
    "        validate_one_epoch(model, val_loader, device)\n",
    "        # Save checkpoint for this fold and epoch\n",
    "        save_checkpoint(model, optimizer, epoch, fold + 1, save_dir)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 42\u001b[0m\n\u001b[0;32m     38\u001b[0m FN \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     41\u001b[0m \u001b[39m#Evaluate the model on the test set\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m model\u001b[39m.\u001b[39mto(device)            \n\u001b[0;32m     43\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m     45\u001b[0m batch_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "precision_list = []\n",
    "recall_list = []\n",
    "NPV_list = []\n",
    "accuracy_list = []\n",
    "specificity_list = []\n",
    "f1_score_list = []\n",
    "ETT_distance_error_list = []\n",
    "total_point_error_list = []\n",
    "pck_list = []\n",
    "\n",
    "\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "NPV_list = []\n",
    "accuracy_list = []\n",
    "specificity_list = []\n",
    "f1_score_list = []\n",
    "ETT_distance_error_list = []\n",
    "total_point_error_list = []\n",
    "pck_small_list = []\n",
    "pck_large_list = []\n",
    "\n",
    "example_error_list = []\n",
    "pred_image_file_list = []\n",
    "total_mean_distance_error = 0\n",
    "total_mean_ETT_distance_error = 0\n",
    "total_number_of_predictions = 0\n",
    "no_preds_count = 0\n",
    "total_attempts = 0\n",
    "batch_count = 0\n",
    "correct_predictions = 0\n",
    "incorrect_predictions = 0\n",
    "correct_real = 0\n",
    "incorrect_real = 0\n",
    "TP = 0\n",
    "FP = 0\n",
    "TN = 0\n",
    "FN = 0\n",
    "\n",
    "\n",
    "#Evaluate the model on the test set\n",
    "model.to(device)            \n",
    "model.eval()\n",
    "\n",
    "batch_count = 0\n",
    "iterator = iter(data_loader_test)\n",
    "for item in iterator:\n",
    "    try:\n",
    "        images, targets = item\n",
    "        images = list(image.to(device) for image in images)\n",
    "    except StopIteration as e:\n",
    "        print(\"StopIteration exception handled at batch: \", batch_count)   \n",
    "\n",
    "    output = model(images)   \n",
    "\n",
    "    batch_count += 1 \n",
    "\n",
    "    for prediction_number in range(len(images)):\n",
    "        \n",
    "        real_keypoints = [] #list of keypoints for each image in batch\n",
    "\n",
    "        #unpacking the targets, this is a pain but works to remove the 0/1 visibility dim (which we do not need because all keypoints are visible)\n",
    "        for kps in targets[prediction_number]['keypoints']:\n",
    "            real_keypoints.append([list(map(int, kp[:2])) for kp in kps])\n",
    "        distance_real = calc_distance(real_keypoints[0][0], real_keypoints[0][1])\n",
    "        #print(\"Real keypoints: \", real_keypoints)\n",
    "    \n",
    "        \n",
    "        real_bboxes = targets[prediction_number]['boxes'].int().tolist()\n",
    "        \n",
    "        #permute(1,2,0) converts the tensor to numpy array. The tensor is in the format (C, H, W) and numpy array is in the format (H, W, C).\n",
    "        #detach().cpu().numpy() detaches the tensor from the graph and converts it to numpy array and moves it to CPU.\n",
    "        image = (images[prediction_number].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)  \n",
    "        \n",
    "        scores = output[prediction_number]['scores'].detach().cpu().numpy()\n",
    "        #print(\"Scores: \", scores)\n",
    "        if len(scores) == 0:\n",
    "            print(\"No keypoints found at image: \", prediction_number)\n",
    "            no_preds_count += 1\n",
    "            break\n",
    "\n",
    "\n",
    "        high_scores_idxs = np.where(scores > 0)[0].tolist() # Indexes of boxes with scores > 0.1\n",
    "        #print(\"High Score idxs: \", high_scores_idxs)\n",
    "        #print(\"Raw NMS Boxes len: \", len(output[0]['boxes'][high_scores_idxs]))\n",
    "        #print(\"Raw NMS scores len: \", len(output[0]['scores'][high_scores_idxs]))\n",
    "        #print(\"Raw NMS Boxes len: \", len(output[0]['boxes'][high_scores_idxs]))\n",
    "        #print(\"High Score idxs: \", high_scores_idxs)\n",
    "        #print(\"Output: \", output)\n",
    "        \n",
    "        #print(\"Raw NMS scores len: \", len(output[0]['scores'][high_scores_idxs]))\n",
    "        \n",
    "        #print(\"******************\")\n",
    "        post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n",
    "\n",
    "        #print(\"-----------Post NMS idxs:-----------\", post_nms_idxs)\n",
    "\n",
    "        #Making images based on keypoints_scores, instead of bbox scores now\n",
    "        #print(\"Raw Keypoint scores: \", output[prediction_number]['keypoints_scores'])\n",
    "        keypoint_scores = output[prediction_number]['keypoints_scores'][post_nms_idxs]\n",
    "        #print(\"Keypoint scores: \", keypoint_scores)\n",
    "        #print(\"Keypoint scores: \", keypoint_scores)\n",
    "        score_idx = get_best_keypoints(keypoint_scores)\n",
    "        #print(\"Best Keypoints IDX: \", score_idx)\n",
    "        #score_idx = 0\n",
    "        pred_keypoints = []\n",
    "        keypoints = output[prediction_number]['keypoints'][score_idx].detach().cpu().numpy()\n",
    "    \n",
    "        for kp in keypoints:\n",
    "            kp = list(map(int, kp)) #convert (x,y) coords in each keypoint to int\n",
    "            pred_keypoints.append(kp[:2])\n",
    "\n",
    "\n",
    "        if len(pred_keypoints) != 0:\n",
    "            pred_keypoints = [pred_keypoints] #convert to list of lists to match real_keypoints format\n",
    "            #print(\"Pred keypoints: \", pred_keypoints)\n",
    "        \n",
    "            distance_pred = calc_distance(pred_keypoints[0][0], pred_keypoints[0][1])\n",
    "            distance_error = abs(distance_pred - distance_real)\n",
    "        \n",
    "        #print(\"Real ETT to Carina:\", distance_real/50)\n",
    "        #print(\"Prediencted ETT to Carina:\", distance_pred/50)\n",
    "        #print(\"ETT to Carina Distance error:\" , distance_error/50)\n",
    "\n",
    "            #The magic number here, 50,3, and 7 will need to be changed to values in CM on the mimic dataset\n",
    "            if ((distance_real/50) <= 3) or ((distance_real/50) >= 7):\n",
    "                incorrect_real += 1\n",
    "            else:\n",
    "                correct_real += 1\n",
    "\n",
    "            if ((distance_pred/50) <= 3) or ((distance_pred/50) >= 7):\n",
    "                incorrect_predictions += 1\n",
    "            else:\n",
    "                correct_predictions += 1\n",
    "\n",
    "            if (((distance_real/50) <= 3) or ((distance_real/50) >= 7)) and (((distance_pred/50) <= 3) or ((distance_pred/50) >= 7)):\n",
    "                TN +=1\n",
    "                #print(\"TN\")\n",
    "            elif (((distance_real/50) <= 3) or ((distance_real/50) >= 7)) and (((distance_pred/50) > 3) or ((distance_pred/50) < 7)):\n",
    "                FN +=1\n",
    "                #print(\"FN\")\n",
    "            elif (((distance_real/50) > 3) or ((distance_real/50) < 7)) and (((distance_pred/50) <= 3) or ((distance_pred/50) >= 7)):\n",
    "                FP +=1\n",
    "                #print(\"FP\")\n",
    "            elif (((distance_real/50) > 3) or ((distance_real/50) < 7)) and (((distance_pred/50) > 3) or ((distance_pred/50) < 7)):\n",
    "                TP +=1\n",
    "                #print(\"TP\")\n",
    "\n",
    "            bboxes = []\n",
    "                \n",
    "            for bbox in  output[prediction_number]['boxes'][[score_idx]][[0]].detach().cpu().numpy():\n",
    "                bboxes.append(list(map(int, bbox.tolist())))\n",
    "            \n",
    "            \n",
    "            if (len(bboxes) == 0):\n",
    "                print(\"No bounding boxes found at image: \", prediction_number)\n",
    "                no_preds_count += 1 #count preds with no bounding box at given threshold  \n",
    "            else:\n",
    "                total_number_of_predictions += 1    \n",
    "                example_error, point_error = calc_mean_distance_error(real_keypoints, pred_keypoints)\n",
    "                total_mean_distance_error += example_error\n",
    "            \n",
    "        \n",
    "            example_point_error = [pt for pt in point_error]\n",
    "            total_point_error_list.append(example_point_error)\n",
    "            example_error_list.append(example_error)\n",
    "            ETT_distance_error_list.append(distance_error)\n",
    "            \n",
    "\n",
    "            save_img_path = f\"F:\\ET_Tube\\saved_img_preds\\prediction_{str(prediction_number)}_{str(batch_count)}.jpg\"\n",
    "            pred_image_file_list.append(save_img_path)\n",
    "            visualize(image, bboxes, pred_keypoints, image_original=image, keypoints_original=real_keypoints, bboxes_original=real_bboxes, save=False, save_path = save_img_path, display=False)\n",
    "            plt.savefig(save_img_path)\n",
    "\n",
    "        total_attempts += 1\n",
    "\n",
    "print(\"Placement Correct predictions: \", correct_predictions)\n",
    "print(\"Placement Incorrect predictions: \", incorrect_predictions)\n",
    "print(\"Placement Correct real: \", correct_real)\n",
    "print(\"Placement Incorrect real: \", incorrect_real)\n",
    "\n",
    "if total_number_of_predictions != 0:\n",
    "    total_mean_distance_error /= total_number_of_predictions\n",
    "print(\"Total mean Euclidean distance error: \", total_mean_distance_error)\n",
    "if len(ETT_distance_error_list) != 0:\n",
    "    total_mean_ETT_distance_error = sum(ETT_distance_error_list)/len(ETT_distance_error_list)\n",
    "print(\"Total ETT distance error: \", total_mean_ETT_distance_error)\n",
    "print(\"No predictions count: \", no_preds_count)\n",
    "print(\"Total attempts: \", total_attempts)\n",
    "\n",
    "pck_threshold_large = 75\n",
    "pck_threshold_small = 50\n",
    "pck_small = calculate_example_pck(total_point_error_list, threshold=pck_threshold_small)\n",
    "pck_large = calculate_example_pck(total_point_error_list, threshold=pck_threshold_large)\n",
    "print(f\"Fraction of Correct Keypoints: {pck_large:0.3f}, at a threshold of {pck_threshold_large} pixels.\")\n",
    "pck_large_list.append(pck_large)\n",
    "print(f\"Fraction of Correct Keypoints: {pck_small:0.3f}, at a threshold of {pck_threshold_small} pixels.\")\n",
    "pck_small_list.append(pck_small)\n",
    "\n",
    "print(\"TP: \", TP)\n",
    "print(\"FP: \", FP)\n",
    "print(\"TN: \", TN)\n",
    "print(\"FN: \", FN)\n",
    "\n",
    "epsilon = 1e-8 #to avoid division by 0 errors \n",
    "\n",
    "precision = TP / (TP + FP + epsilon)\n",
    "precision_list.append(precision)\n",
    "print(\"Precision/PPV: \", precision)\n",
    "recall = TP / (TP + FN + epsilon)\n",
    "recall_list.append(recall)\n",
    "print (\"Recall/Sensitivity: \", recall)\n",
    "NPV = TN / (TN + FN + epsilon)\n",
    "NPV_list.append(NPV)\n",
    "print(\"NPV: \", NPV)\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN + epsilon)\n",
    "accuracy_list.append(accuracy)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "specificity = TN / (TN + FP + epsilon)\n",
    "specificity_list.append(specificity)\n",
    "print(\"Specificity: \", specificity)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall + epsilon)\n",
    "f1_score_list.append(f1_score)\n",
    "print(\"F1 Score: \", f1_score)\n",
    "\n",
    "print(\"Completed at: \", datetime.datetime.now().strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old numbers I hand wrote in\n",
    "eucl_err = [29.192664806593392,30.417407798028098,54.13088229772116, 30.169575240273996]\n",
    "ETT_err = [16.936977627484584, 17.40249039641634, 17.95239513280611, 18.22531862299226]\n",
    "precision_list = [0.963414634028852,  0.9390243901293873, 0.926829268179655, 0.9878048779283165]\n",
    "recall_list = [0.963414634028852, 0.9624999998796876,  0.9743589742340566, 0.9204545453499484]\n",
    "NPV_list = [0.8888888885596707, 0.8888888885596707, 0.9259259255829904, 0.7407407404663923]\n",
    "accuracy_list = [0.944954128353674, 0.9266055045021463, 0.9266055045021463, 0.9266055045021463]\n",
    "specificity_list = [ 0.8888888885596707, 0.8275862066111771, 0.8064516126430801, 0.9523809519274377]\n",
    "f1_score_list = [ 0.963414629028852,0.9506172788340193, 0.949999994884375, 0.952941171364706 ]\n",
    "pck_list = [0.991,0.991,0.987, 0.989]\n",
    "\n",
    "stat_list = [precision_list, recall_list, NPV_list, accuracy_list, specificity_list, f1_score_list, pck_list, eucl_err , ETT_err]\n",
    "\n",
    "for list_ in stat_list:\n",
    "    std_dev = statistics.stdev(list_)\n",
    "    avg = sum(list_)/len(list_)\n",
    "    print(f\"The standard deviation of list is: {std_dev:.3f}\")\n",
    "    print(f\"The average of list is: {avg:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = [\n",
    "    [TP, FP],\n",
    "    [FN, TN]\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.set(font_scale=1.4)  \n",
    "ax = sns.heatmap(confusion_matrix, annot=True, cmap=\"YlGnBu\", fmt=\"d\", annot_kws={\"size\": 16})\n",
    "ax.set_ylim(sorted(ax.get_xlim(), reverse=True))\n",
    "\n",
    "ax.set_xlabel('Predicted labels')\n",
    "ax.set_ylabel('True labels')\n",
    "ax.set_title('Confusion Matrix for ETT Placement')\n",
    "\n",
    "ax.xaxis.set_ticklabels(['Correct ETT Placement', 'Incorrect ETT Placement'])\n",
    "ax.yaxis.set_ticklabels(['Correct ETT Placement', 'Incorrect ETT Placement'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_thumbnail(path):\n",
    "    i = Image.open(path)\n",
    "    i.thumbnail((500, 250), Image.Resampling.LANCZOS)\n",
    "    return i\n",
    "\n",
    "def image_base64(im):\n",
    "    if isinstance(im, str):\n",
    "        im = get_thumbnail(im)\n",
    "    with BytesIO() as buffer:\n",
    "        im.save(buffer, 'jpeg')\n",
    "        return base64.b64encode(buffer.getvalue()).decode()\n",
    "\n",
    "def image_formatter(im):\n",
    "    return f'<img src=\"data:image/jpeg;base64,{image_base64(im)}\">'\n",
    "\n",
    "df = pd.DataFrame(example_error_list, columns=['Example Error'])\n",
    "df['pck_small'] = pck_small_list * len(df)\n",
    "df['pck_large'] = pck_large_list * len(df)\n",
    "df['file'] = pred_image_file_list\n",
    "df['image'] = df.file.map(lambda f: get_thumbnail(f))\n",
    "df.head()\n",
    "\n",
    "HTML(df[['Example Error', 'image']].to_html(formatters={'image': image_formatter}, escape=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workbook = Workbook()\n",
    "worksheet = workbook.active\n",
    "\n",
    "# resize cells\n",
    "for row in range(2, len(df['file'])+2):\n",
    "    for col in range(1,2):\n",
    "        worksheet.row_dimensions[row].height = 400\n",
    "        col_letter = get_column_letter(col)\n",
    "        worksheet.column_dimensions[col_letter].width = 150\n",
    "        \n",
    "for col in range(2,50):\n",
    "    col_letter = get_column_letter(col)\n",
    "    worksheet.column_dimensions[col_letter].width = 30\n",
    "         \n",
    "# insert images\n",
    "for index, image in enumerate(df['file']):\n",
    "    if index == 0:\n",
    "        worksheet.cell(row=index+1, column=1, value=\"Image\")\n",
    "        worksheet.add_image(ExcelImage(image), anchor='A'+str(index+2))\n",
    "    else:\n",
    "        worksheet.add_image(ExcelImage(image), anchor='A'+str(index+2))\n",
    "\n",
    "excel_cell_values = []\n",
    "for excel_cell_value in df['Example Error']:\n",
    "    excel_cell_values.append(excel_cell_value)\n",
    "\n",
    "for index, excel_cell_value in enumerate(excel_cell_values):\n",
    "    if index == 0:\n",
    "        worksheet.cell(row=index+1, column=2, value=\"Example Error\")\n",
    "        worksheet.cell(row=index+2, column=2, value=excel_cell_value)\n",
    "    else:\n",
    "        worksheet.cell(row=index+2, column=2, value=excel_cell_value)\n",
    "\n",
    "# Find the last column\n",
    "last_column = worksheet.max_column\n",
    "\n",
    "# Add values to the last column\n",
    "worksheet.cell(row=1, column=last_column+1).value = f'Total Mean Euclidean Distance Error: {total_mean_distance_error}'\n",
    "worksheet.cell(row=2, column=last_column+1).value = f'No predictions count: {no_preds_count}'\n",
    "worksheet.cell(row=6, column=last_column+1).value = f'Fraction of Correct Keypoints at threshold of {pck_threshold_small} pixels: {pck_small}'\n",
    "worksheet.cell(row=7, column=last_column+1).value = f'Fraction of Correct Keypoints at threshold of {pck_threshold_large} pixels: {pck_large}'\n",
    "\n",
    "workbook.save(workbook_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_predict(image_file):\n",
    "    raw_img = cv2.imread(image_file)\n",
    "    aspect_ratio = raw_img.shape[0]/raw_img.shape[1]\n",
    "\n",
    "    if aspect_ratio < 1.5 and aspect_ratio > 0.5:\n",
    "        raw_img = cv2.resize(raw_img, (456, 456))\n",
    "        test_img = np.moveaxis(raw_img, -1, 0)\n",
    "        test_img= np.expand_dims(test_img, axis=0)\n",
    "        test_img = torch.from_numpy(test_img).float().to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.to(device)            \n",
    "            model.eval()\n",
    "            output = model(test_img)\n",
    "        \n",
    "        scores = output[0]['keypoints_scores']\n",
    "        score_idx = get_best_keypoints(scores)\n",
    "\n",
    "        keypoints = output[0]['keypoints'][score_idx].detach().cpu().numpy()\n",
    "        for idx, kp in enumerate(keypoints):\n",
    "            current_keypoint = kp[:2].astype(int)\n",
    "            raw_img = cv2.circle(raw_img, current_keypoint, 1, (255,255,0), 10)\n",
    "            image_original = cv2.putText(raw_img, \" \" + keypoints_classes_ids2names[idx], current_keypoint, cv2.FONT_HERSHEY_PLAIN, 2, (255,0,0), 2, cv2.LINE_AA)\n",
    "\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.imshow(raw_img)\n",
    "    else:\n",
    "        print(f\"Image aspect ratio is {aspect_ratio:0.2F} it but be between 0.5 - 1.5\")\n",
    "\n",
    "        \n",
    "#test_img = \"/mnt/c/Users/nprim/Downloads/F1.jpg\"\n",
    "#process_and_predict(test_img) # random picture from the internet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
